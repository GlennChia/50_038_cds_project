{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. References\n",
    "\n",
    "Title: Multi-Label Classification(Blog Tags Prediction)using NLP\n",
    "\n",
    "Link: https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc\n",
    "\n",
    "Naive approach is to do x -> y1, x -> y1, y2, x -> y1, y2, y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creation of a valid dataframe - one hot encoding style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/raw/\"\n",
    "INPUT_FILE_NAME = 'subset_raw.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         speaker                              headline  \\\n",
       "id                                                       \n",
       "1        Al Gore           Averting the climate crisis   \n",
       "2      Amy Smith         Simple designs to save a life   \n",
       "3   Ashraf Ghani         How to rebuild a broken state   \n",
       "4     Burt Rutan  The real future of space exploration   \n",
       "5   Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                          description duration  \\\n",
       "id                                                               \n",
       "1   With the same humor and humanity he exuded in ...  0:16:17   \n",
       "2   Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "3   Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "4   In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "5   American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                 tags  \\\n",
       "id                                                      \n",
       "1   cars,alternative energy,culture,politics,scien...   \n",
       "2   MacArthur grant,simplicity,industrial design,a...   \n",
       "3   corruption,poverty,economics,investment,milita...   \n",
       "4   aircraft,flight,industrial design,NASA,rocket ...   \n",
       "5   cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                           transcript      WC  \n",
       "id                                                             \n",
       "1   0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0  \n",
       "2   0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0  \n",
       "3   0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0  \n",
       "4   0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0  \n",
       "5   0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2475 entries, 1 to 2804\n",
      "Data columns (total 7 columns):\n",
      "speaker        2475 non-null object\n",
      "headline       2475 non-null object\n",
      "description    2475 non-null object\n",
      "duration       2475 non-null object\n",
      "tags           2475 non-null object\n",
      "transcript     2386 non-null object\n",
      "WC             2386 non-null float64\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 154.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:,:15].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Remove nan transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2386 entries, 0 to 2385\n",
      "Data columns (total 7 columns):\n",
      "speaker        2386 non-null object\n",
      "headline       2386 non-null object\n",
      "description    2386 non-null object\n",
      "duration       2386 non-null object\n",
      "tags           2386 non-null object\n",
      "transcript     2386 non-null object\n",
      "WC             2386 non-null float64\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 130.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['transcript'])\n",
    "df = df.reset_index(drop=True)\n",
    "df.iloc[:,:15].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Finding the unique tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cars', 'alternative energy', 'culture', 'politics', 'science', 'climate change', 'environment', 'sustainability', 'global issues', 'technology', 'macarthur grant', 'simplicity', 'industrial design', 'invention', 'engineering', 'design', 'corruption', 'poverty', 'economics', 'investment', 'military', 'policy', 'global development', 'entrepreneur', 'business', 'aircraft', 'flight', 'nasa', 'rocket science', 'transportation', 'art', 'biotech', 'oceans', 'genetics', 'dna', 'biology', 'biodiversity', 'ecology', 'computers', 'software', 'interface design', 'music', 'media', 'entertainment', 'performance', 'new york', 'memory', 'interview', 'death', 'architecture', 'disaster relief', 'cities', 'urban planning', 'collaboration', 'robots', 'education', 'innovation', 'social change', 'obesity', 'disease', 'health', 'health care', 'food', 'primates', 'africa', 'animals', 'nature', 'wunderkind', 'cancer', 'creativity', 'love', 'gender', 'relationships', 'cognitive science', 'psychology', 'evolution', 'biomimicry', 'fish', 'philosophy', 'choice', 'history', 'future', 'consumerism', 'marketing', 'storytelling', 'communication', 'community', 'faith', 'illusion', 'religion', 'ted brain trust', 'film', 'activism', 'open-source', 'library', 'poetry', 'product design', 'science and art', 'narcotics', 'race', 'statistics', 'parenting', 'brazil', 'animation', 'terrorism', 'war', 'peace', 'photography', 'wikipedia', 'aging', 'one laptop per child', 'philanthropy', 'children', 'cosmos', 'complexity', 'universe', 'astronomy', 'time', 'leadership', 'happiness', 'violin', 'youth', 'piano', 'physics', 'materials', 'typography', 'energy', 'green', 'pollution', 'inequality', 'ted prize', 'medicine', 'ebola', 'aids', 'sports', 'performance art', 'theater', 'united states', 'map', 'women', 'demo', 'dance', 'teaching', 'anthropology', 'language', 'success', 'work', 'christianity', 'god', 'motivation', 'work-life balance', 'personal growth', 'potential', 'apes', 'intelligence', 'online video', 'biomechanics', 'brain', 'telecom', 'microfinance', 'live music', 'singer', 'prosthetics', 'ants', 'insects', 'atheism', 'comedy', 'humor', 'travel', 'exploration', 'code', 'asia', 'math', 'google', 'visualizations', 'decision-making', 'consciousness', 'goal-setting', 'guitar', 'vocals', 'cello', 'self', 'china', 'web', 'spoken word', 'composing', 'natural disaster', 'meme', 'water', 'museums', 'ai', 'women in business', 'microsoft', 'virtual reality', 'buddhism', 'moon', 'mining', 'planets', 'space', 'adventure', 'bioethics', 'gaming', 'literature', 'books', 'sociology', 'violence', 'solar system', 'humanity', 'human origins', 'paleontology', 'drones', 'illness', 'law', 'suicide', 'depression', 'mental health', 'string theory', 'magic', 'empathy', 'compassion', 'writing', 'ted books', 'play', 'latin america', 'world cultures', 'infrastructure', 'bees', 'garden', 'plants', 'ancient world', 'toy', 'hack', 'news', 'heart health', 'public health', 'big bang', 'bacteria', 'microbiology', 'submarine', 'sex', 'society', 'archaeology', 'dinosaurs', 'evil', 'crime', 'prison', 'beauty', 'plastic', 'vaccines', 'conducting', 'family', 'trees', 'extraterrestrial life', 'personality', 'origami', 'dark matter', 'identity', 'nanoscale', 'morality', 'geology', 'life', 'presentation', 'democracy', 'solar', 'smell', 'social media', 'senses', 'fashion', 'mars', 'curiosity', 'programming', 'chemistry', 'shopping', 'body language', 'virus', 'solar energy', 'fear', 'birds', 'wind energy', 'extreme sports', 'prediction', 'productivity', 'mind', 'ted fellows', 'natural resources', 'agriculture', 'india', 'neuroscience', 'tedx', 'state-building', 'money', 'europe', 'data', 'sight', 'internet', 'government', 'men', 'advertising', 'sanitation', 'charter for compassion', 'weather', 'big problems', 'slavery', 'trafficking', 'egypt', 'yesallwomen', 'feminism', 'autism spectrum disorder', 'botany', 'discovery', 'mission blue', 'student', 'hiv', 'nuclear weapons', 'oil', 'novel', 'iraq', 'islam', 'monkeys', 'iran', 'middle east', 'sound', 'population', 'bullying', 'journalism', 'cyborg', 'foreign policy', 'surgery', 'medical research', 'protests', 'deextinction', 'exoskeleton', 'disability', 'nuclear energy', 'crowdsourcing', 'brand', 'speech', 'failure', 'security', 'pain', 'gender spectrum', 'glacier', 'mobility', 'public spaces', 'pharmaceuticals', 'molecular biology', 'behavioral economics', 'medical imaging', 'physiology', 'pregnancy', 'synthetic biology', 'hearing', 'jazz', 'nobel prize', 'finance', 'tedyouth', '3d printing', 'guns', 'algorithm', 'conservation', 'immigration', 'privacy', 'machine learning', 'lgbt', 'skateboarding', 'microbes', 'augmented reality', 'urban', 'forensics', 'painting', 'pandemic', 'mindfulness', 'meditation', 'transgender', 'testing', 'farming', 'debate', 'cloud', 'sleep', 'television', 'street art', 'addiction', 'south america', 'vulnerability', 'capitalism', 'tedmed', 'refugees', 'criminal justice', 'grammar', 'asteroid', 'biosphere', 'resources', 'development', 'manufacturing', 'friendship', 'funny', 'nonviolence', 'arts', 'ptsd', 'trust', 'driverless cars', 'surveillance', 'gender equality', 'blockchain', 'gay', 'crispr', 'sexual violence', 'anthropocene', 'syria', 'movies', 'ted residency', 'ted-ed', 'telescopes', 'ted en espanol', \"alzheimer's\", 'ted en español', 'epidemiology']\n",
      "417\n"
     ]
    }
   ],
   "source": [
    "joined_tags = df['tags'].str.cat(sep=',').split(',')\n",
    "all_tags = pd.Series(joined_tags).str.strip().str.lower()\n",
    "all_tags = list(dict.fromkeys(all_tags))\n",
    "try:\n",
    "    all_tags.remove('')\n",
    "except:\n",
    "    pass\n",
    "print(all_tags)\n",
    "print(len(all_tags))\n",
    "\n",
    "# tags = df['tags'].str.replace(', ', ',').str.lower().str.strip()\n",
    "# split_tags = tags.str.split(',')\n",
    "# tag_counts_per_talk = split_tags.apply(len)\n",
    "\n",
    "# joined_tags = tags.str.cat(sep=',').split(',')\n",
    "# all_tags_w_dup = pd.Series(joined_tags)\n",
    "\n",
    "# tag_counts = all_tags_w_dup.value_counts()\n",
    "# tag_cutoff = int(0.01*len(df.index))\n",
    "# print(tag_cutoff)\n",
    "# squashed_tags = pd.DataFrame(tag_counts)\n",
    "# # squashed_tags.columns = ['tag','count']\n",
    "# squashed_tags = squashed_tags[(squashed_tags[0]>tag_cutoff)]\n",
    "# squash_list = list(squashed_tags.index.values)\n",
    "# print(len(squash_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tags of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Creating a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encode(df=df):\n",
    "    complete_transcripts_tags = []\n",
    "    for rows, value in df.iterrows():\n",
    "        one_hot_encoding = [0] * len(all_tags)\n",
    "        transcript = [value['transcript']]\n",
    "        indiv_tags = value['tags'].split(',')\n",
    "        for tags in indiv_tags:\n",
    "            if tags == '':\n",
    "                continue\n",
    "            index = all_tags.index(tags.lower().lstrip(' '))\n",
    "            one_hot_encoding[index] = 1\n",
    "        indiv_transcript_tags = transcript + one_hot_encoding\n",
    "        complete_transcripts_tags.append(indiv_transcript_tags)\n",
    "    return pd.DataFrame(complete_transcripts_tags, columns=['transcript'] + all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>cars</th>\n",
       "      <th>alternative energy</th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>climate change</th>\n",
       "      <th>environment</th>\n",
       "      <th>sustainability</th>\n",
       "      <th>global issues</th>\n",
       "      <th>...</th>\n",
       "      <th>anthropocene</th>\n",
       "      <th>syria</th>\n",
       "      <th>movies</th>\n",
       "      <th>ted residency</th>\n",
       "      <th>ted-ed</th>\n",
       "      <th>telescopes</th>\n",
       "      <th>ted en espanol</th>\n",
       "      <th>alzheimer's</th>\n",
       "      <th>ted en español</th>\n",
       "      <th>epidemiology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>0:11\\r\\r\\rImagine that when you walked\\rin her...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>0:11\\r\\r\\rPaying close attention to something:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>0:11\\r\\r\\rSo, this happy pic of me\\rwas taken ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>0:12\\r\\r\\rMy seven-year-old grandson\\rsleeps j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>0:12\\r\\r\\rMichael Browning: engineer,\\rinnovat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2386 rows × 418 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             transcript  cars  \\\n",
       "0     0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...     1   \n",
       "1     0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...     0   \n",
       "2     0:12\\r\\r\\rA public, Dewey long ago observed,\\r...     0   \n",
       "3     0:11\\r\\r\\rI want to start off by saying, Houst...     0   \n",
       "4     0:12\\r\\r\\rWhat I want to talk about is, as bac...     1   \n",
       "...                                                 ...   ...   \n",
       "2381  0:11\\r\\r\\rImagine that when you walked\\rin her...     0   \n",
       "2382  0:11\\r\\r\\rPaying close attention to something:...     0   \n",
       "2383  0:11\\r\\r\\rSo, this happy pic of me\\rwas taken ...     0   \n",
       "2384  0:12\\r\\r\\rMy seven-year-old grandson\\rsleeps j...     0   \n",
       "2385  0:12\\r\\r\\rMichael Browning: engineer,\\rinnovat...     0   \n",
       "\n",
       "      alternative energy  culture  politics  science  climate change  \\\n",
       "0                      1        1         1        1               1   \n",
       "1                      1        0         0        0               0   \n",
       "2                      0        1         1        0               0   \n",
       "3                      0        0         0        0               0   \n",
       "4                      0        0         0        0               0   \n",
       "...                  ...      ...       ...      ...             ...   \n",
       "2381                   0        0         0        0               0   \n",
       "2382                   0        0         0        0               0   \n",
       "2383                   0        0         0        0               0   \n",
       "2384                   0        0         0        0               0   \n",
       "2385                   0        0         0        0               0   \n",
       "\n",
       "      environment  sustainability  global issues  ...  anthropocene  syria  \\\n",
       "0               1               1              1  ...             0      0   \n",
       "1               0               0              1  ...             0      0   \n",
       "2               0               0              1  ...             0      0   \n",
       "3               0               0              0  ...             0      0   \n",
       "4               0               0              0  ...             0      0   \n",
       "...           ...             ...            ...  ...           ...    ...   \n",
       "2381            0               0              0  ...             0      0   \n",
       "2382            0               0              0  ...             0      0   \n",
       "2383            0               0              0  ...             0      0   \n",
       "2384            0               0              0  ...             0      0   \n",
       "2385            0               0              0  ...             0      0   \n",
       "\n",
       "      movies  ted residency  ted-ed  telescopes  ted en espanol  alzheimer's  \\\n",
       "0          0              0       0           0               0            0   \n",
       "1          0              0       0           0               0            0   \n",
       "2          0              0       0           0               0            0   \n",
       "3          0              0       0           0               0            0   \n",
       "4          0              0       0           0               0            0   \n",
       "...      ...            ...     ...         ...             ...          ...   \n",
       "2381       0              0       0           0               0            0   \n",
       "2382       0              0       0           0               0            0   \n",
       "2383       0              0       0           0               0            0   \n",
       "2384       0              0       0           0               0            0   \n",
       "2385       0              0       0           0               0            0   \n",
       "\n",
       "      ted en español  epidemiology  \n",
       "0                  0             0  \n",
       "1                  0             0  \n",
       "2                  0             0  \n",
       "3                  0             0  \n",
       "4                  0             0  \n",
       "...              ...           ...  \n",
       "2381               0             0  \n",
       "2382               0             0  \n",
       "2383               0             0  \n",
       "2384               0             0  \n",
       "2385               0             0  \n",
       "\n",
       "[2386 rows x 418 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_tags = create_one_hot_encode()\n",
    "ted_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Numbers\n",
    "2. Apostrophe\n",
    "3. All punctuations\n",
    "4. Weird symbols\n",
    "5. Stop words\n",
    "6. lemmatization\n",
    "'''\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "sets=[stop_words.ENGLISH_STOP_WORDS]\n",
    "sklearnStopWords = [list(x) for x in sets][0]\n",
    "token=ToktokTokenizer()\n",
    "lemma=WordNetLemmatizer()\n",
    "stopWordList=stopwords.words('english')\n",
    "stopWords = stopWordList + sklearnStopWords\n",
    "stopWords = list(dict.fromkeys(stopWords))\n",
    "\n",
    "\n",
    "def stopWordsRemove(text):\n",
    "    wordList=[x.lower().strip() for x in token.tokenize(text)]\n",
    "    removedList=[x + ' ' for x in wordList if not x in stopWords]\n",
    "    text=''.join(removedList)\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemitizeWords(text):\n",
    "    words=token.tokenize(text)\n",
    "    listLemma=[]\n",
    "    for w in words:\n",
    "        x=lemma.lemmatize(w,'v')\n",
    "        listLemma.append(x)\n",
    "    return text\n",
    "\n",
    "\n",
    "# There is a mispelt word that needs to be replaced\n",
    "ted_tags['transcript'] = df['transcript'].str.replace('childrn','children')\n",
    "\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace('\\r',' ')\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"\\'s\",\" is\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"\\'m\",\" am\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"\\'ll\",\" will\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"Can\\'t\",\"cannot\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"Sha\\'t\",\"shall not\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"Won\\'t\",\"would not\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"n\\'t\",\" not\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"\\'ve\",\" have\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"\\'re\",\" are\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"\\'d\",\" would\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(r\"\\(([^)]+)\\)\",\"\")\n",
    "# Deal with Mr. and Dr.\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"mr. \",\"mr\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"Mr. \",\"mr\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"dr. \",\"dr\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"mrs. \",\"mrs\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"Mrs. \",\"mrs\")\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(\"Dr. \",\"dr\")\n",
    "\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(r'\\d+','')\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace(r'<.*?>','')\n",
    "for i in string.punctuation:\n",
    "    if i == \"'\":\n",
    "        ted_tags['transcript'] = ted_tags['transcript'].str.replace(i,'')\n",
    "    else:\n",
    "        ted_tags['transcript'] = ted_tags['transcript'].str.replace(i,' ')\n",
    "ted_tags['transcript'] = ted_tags['transcript'].map(lambda com : stopWordsRemove(com))\n",
    "ted_tags['transcript'] = ted_tags['transcript'].map(lambda com : lemitizeWords(com))\n",
    "ted_tags['transcript'] = ted_tags['transcript'].str.replace('\\s+',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>cars</th>\n",
       "      <th>alternative energy</th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>climate change</th>\n",
       "      <th>environment</th>\n",
       "      <th>sustainability</th>\n",
       "      <th>global issues</th>\n",
       "      <th>...</th>\n",
       "      <th>anthropocene</th>\n",
       "      <th>syria</th>\n",
       "      <th>movies</th>\n",
       "      <th>ted residency</th>\n",
       "      <th>ted-ed</th>\n",
       "      <th>telescopes</th>\n",
       "      <th>ted en espanol</th>\n",
       "      <th>alzheimer's</th>\n",
       "      <th>ted en español</th>\n",
       "      <th>epidemiology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terms invention like tell tale favorite projec...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public dewey long ago observed constituted dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want start saying houston problem entering sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want talk background idea cars art actually qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>imagine walked evening discovered everybody ro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>paying close attention easy attention pulled d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>happy pic taken senior college right dance pra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>seven year old grandson sleeps hall wakes lot ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>michael browning engineer innovator inventor r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2386 rows × 418 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             transcript  cars  \\\n",
       "0     thank chris truly great honor opportunity come...     1   \n",
       "1     terms invention like tell tale favorite projec...     0   \n",
       "2     public dewey long ago observed constituted dis...     0   \n",
       "3     want start saying houston problem entering sec...     0   \n",
       "4     want talk background idea cars art actually qu...     1   \n",
       "...                                                 ...   ...   \n",
       "2381  imagine walked evening discovered everybody ro...     0   \n",
       "2382  paying close attention easy attention pulled d...     0   \n",
       "2383  happy pic taken senior college right dance pra...     0   \n",
       "2384  seven year old grandson sleeps hall wakes lot ...     0   \n",
       "2385  michael browning engineer innovator inventor r...     0   \n",
       "\n",
       "      alternative energy  culture  politics  science  climate change  \\\n",
       "0                      1        1         1        1               1   \n",
       "1                      1        0         0        0               0   \n",
       "2                      0        1         1        0               0   \n",
       "3                      0        0         0        0               0   \n",
       "4                      0        0         0        0               0   \n",
       "...                  ...      ...       ...      ...             ...   \n",
       "2381                   0        0         0        0               0   \n",
       "2382                   0        0         0        0               0   \n",
       "2383                   0        0         0        0               0   \n",
       "2384                   0        0         0        0               0   \n",
       "2385                   0        0         0        0               0   \n",
       "\n",
       "      environment  sustainability  global issues  ...  anthropocene  syria  \\\n",
       "0               1               1              1  ...             0      0   \n",
       "1               0               0              1  ...             0      0   \n",
       "2               0               0              1  ...             0      0   \n",
       "3               0               0              0  ...             0      0   \n",
       "4               0               0              0  ...             0      0   \n",
       "...           ...             ...            ...  ...           ...    ...   \n",
       "2381            0               0              0  ...             0      0   \n",
       "2382            0               0              0  ...             0      0   \n",
       "2383            0               0              0  ...             0      0   \n",
       "2384            0               0              0  ...             0      0   \n",
       "2385            0               0              0  ...             0      0   \n",
       "\n",
       "      movies  ted residency  ted-ed  telescopes  ted en espanol  alzheimer's  \\\n",
       "0          0              0       0           0               0            0   \n",
       "1          0              0       0           0               0            0   \n",
       "2          0              0       0           0               0            0   \n",
       "3          0              0       0           0               0            0   \n",
       "4          0              0       0           0               0            0   \n",
       "...      ...            ...     ...         ...             ...          ...   \n",
       "2381       0              0       0           0               0            0   \n",
       "2382       0              0       0           0               0            0   \n",
       "2383       0              0       0           0               0            0   \n",
       "2384       0              0       0           0               0            0   \n",
       "2385       0              0       0           0               0            0   \n",
       "\n",
       "      ted en español  epidemiology  \n",
       "0                  0             0  \n",
       "1                  0             0  \n",
       "2                  0             0  \n",
       "3                  0             0  \n",
       "4                  0             0  \n",
       "...              ...           ...  \n",
       "2381               0             0  \n",
       "2382               0             0  \n",
       "2383               0             0  \n",
       "2384               0             0  \n",
       "2385               0             0  \n",
       "\n",
       "[2386 rows x 418 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "totalText=''\n",
    "for x in ted_tags['transcript']:\n",
    "    totalText=totalText+''+x\n",
    "wc=WordCloud(background_color='black',max_font_size=50).generate(totalText)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=ted_tags.iloc[:,0].values\n",
    "y=ted_tags.iloc[:,1:-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "body = ted_tags.transcript\n",
    "cv = CountVectorizer().fit(body)\n",
    "article = pd.DataFrame(cv.transform(body).todense(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfart=TfidfTransformer().fit(article)\n",
    "art=pd.DataFrame(tfidfart.transform(article).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(classifier=GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "                order=None, require_dense=[True, True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install scikit-multilearn\n",
    "\n",
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(art,y)\n",
    "# initialize classifier chains multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier = ClassifierChain(GaussianNB())\n",
    "\n",
    "# train\n",
    "classifier.fit(xtrain.astype(float), ytrain.astype(float))\n",
    "\n",
    "# predictions = classifier.predict(xtest.astype(float))\n",
    "#predictions = classifier.predict_proba(xtest.astype(float))\n",
    "#predictions.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = classifier.predict_proba(xtest.astype(float)) # This library has some issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(xtest.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(ytest.astype(float),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[ 5  6 17  4  9  9  5  7  6  4  9 19  6 12  4  4  8  6  6  9  3  4  2  8\n",
      "  8  9  6 16 12 15  5  8  4  8  8  9 21  6  2  9 10  9  8 22 13  5  5  3\n",
      " 17  5  7  6  5  9  9  4  3  8  5  6  5  5  5 12  3  6  5  7  4 12 15 17\n",
      "  6  5 20  3  4  6  6  7  6  6  4  5  5  6 15 10  9  6  9  4 23 22  9  9\n",
      " 20 20 12  4  2  6  5  5  5  8  6  3  7  3  2  7  4  2  5  4  8  5  8 14\n",
      "  5 12  4  5  8  5  4  6  6 11  8  9  6 15 24  6  5 15  5  4  6  8  5  5\n",
      "  5  6  8 20 14  3  9 18  5 18  3  4  8  5  4  8  7  9  4  7  6  9  7  8\n",
      "  3  5  7 18  8  7 14  7  9  8  5 14  3  2  5  7  7  5  3  5  6  6  4 14\n",
      " 26  6  4 12 14  9  7  9  8 12  6  8  9  4  4  7  6 12  8  5  6  8  9 13\n",
      " 12 11  6 14  5  6  5  4 24  5  3 15  4  4  9  5  5  7  6  3  5 14  7  9\n",
      "  8 14  9  6  4  4  5 21  5  5  3  5  7  5  3  5  4  8 10 11  7  7 10  3\n",
      " 13  5  5 13  5  7  6  5  5  5  7  5  9  4  3  4  5  7  4  5  6  3  3  7\n",
      "  7  4  6  9  5  8  4  7  5  9  5 14  9  5  9 11 17  3  8  4  6  4  4  6\n",
      " 11  3  7  8  7 13 13  9  4  8  7  3  4  6  5  6  6 24  4  9  8 11  7  5\n",
      "  5  4  9 12  4  5  7 12  5  3  6  3 13  5  4  4  3  2  7  4 10  7  4  4\n",
      " 16  5  7 11  8  3  5 17  6  5  5 13  3  5  6  3 14 15  5 23  4  5  6  7\n",
      "  7  5 13  8  4  9  3 10  7 11  5  6  7  6 11 15  4  7 17  4  6  7  8 14\n",
      "  5 18  3  6 10  5 14  4 10  5 10 10 18  3 15 11 15 12  5  2  4  5  7 10\n",
      " 10  4  6  4  7  8  6  5  6 11  9  8  5  4  6  7  6  8  4  8  4  8  9  5\n",
      "  4  7  4  5  3  7  3  6  6  6  5  5  5  4  3  3  6  6  6 11 13  8  2  9\n",
      "  5 11  4  7  4  9  6  8  6  2  6  9  4  6  5  6  7 11  5 10  6  7  8  2\n",
      "  3  5 12  7  3 13  8 11  5  8  7  3  8  4  6  9  8  5  6  5  7  7  5 21\n",
      "  6  8  1 16  5  5  5  3 15  2  8  3  9 12 16 15  8 14  7  7  8  7 10 10\n",
      "  4 19  7 20  7 15  4  6 13  5  3 12 18  5  4 10  4 20 17 16  9  7 14  6\n",
      "  3  8  4 13  4  6  4  6 20  4  5 10  5  3 10 10 19  5  4 12  7]\n",
      "[0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(ytest)\n",
    "print(ytest.sum(axis=1))\n",
    "print(ytest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "416\n"
     ]
    }
   ],
   "source": [
    "print(predictions.toarray())\n",
    "print(predictions.toarray()[0])\n",
    "print(len(predictions.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  5.  1.\n",
      "  0.  1.  0.  0.  0.  2.  1.  0.  0.  6.  0.  0.  0.  2.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  9.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0. 14.  2.  2.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  2.  1.  0.  0.  0.  2.\n",
      "  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0.  0.\n",
      "  0.  0.  4.  0.  0.  7.  0.  0.  3.  1.  0.  3.  0.  0.  0.  1.  0.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 13.  0.  1.  0.  0.\n",
      "  0.  1.  0.  1.  0.  6.  0.  0.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  2.  1.  0.  0.  0.  2.  0.  0.  0.  0.  1.  1.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  2.  0.  0.  2.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  2.  0.  7.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  1.  0.  0.  2.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "  0.  0.  0.  6.  1.  0.  0.  0.  0.  0.  1.  0.  2.  0.  0.  0.  0. 10.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.\n",
      "  1.  0.  0.  1.  0.  0.  1.  5.  1.  1.  0.  1.  2.  0.  0.  0.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  2.  1.  0.  1.  6.\n",
      "  0.  6.  0.  0.  1.  3.  0.  4.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  5.  0.  0.  1.  1.  0.  3.  4.  1.  0.  0.\n",
      "  0.  0.  0.  0.  2.  0.  0.  1.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  1.  2.  1.  0.  0.  0.  0.  0.  1.  9.  0.  0.  4.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  1.  0.  0.  0.  0.  0.  2.  1.  8.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  2.  0.  0.  0.  0.  0.\n",
      "  0.  2.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  1.\n",
      "  0.  0.  0.  2.  0.  3.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  1.\n",
      "  0.  0.  0.  0.  0.  0.  4.  0.  0.  0.  0.  0.  0.  0.  1.  3.  0.  0.\n",
      "  0.  2.  0.]\n",
      "<class 'scipy.sparse.csc.csc_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(predictions.toarray().sum(axis=1))\n",
    "print(type(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 0.37425149700598803\n",
      "The recall is 0.02755731922398589\n",
      "The accuracry (naive) is 0.9813973714727483\n",
      "The weighted harmonic mean/F1 score is 0.05133470225872689\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(test, predict):\n",
    "    true_positive_ls = []\n",
    "    true_negative_ls = []\n",
    "    false_positive_ls = []\n",
    "    false_negative_ls = []\n",
    "    labelled_tags = 0\n",
    "    # overall_correct_one = 0\n",
    "    for index_pred, value_pred in enumerate(predict):\n",
    "        true_positive = 0\n",
    "        true_negative = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        for index_pred_indiv, value_pred_indiv in enumerate(value_pred):\n",
    "            if test[index_pred][index_pred_indiv] == 1:\n",
    "                labelled_tags += 1\n",
    "            if value_pred_indiv == test[index_pred][index_pred_indiv]:\n",
    "                if value_pred_indiv == 1:\n",
    "                    # overall_correct_one += 1\n",
    "                    true_positive += 1\n",
    "                else:\n",
    "                    true_negative += 1\n",
    "            else:\n",
    "                if value_pred_indiv == 1:\n",
    "                    # Test is 0 but we predict 1\n",
    "                    false_positive += 1\n",
    "                else:\n",
    "                    false_negative += 1\n",
    "        true_positive_ls.append(true_positive)\n",
    "        true_negative_ls.append(true_negative)\n",
    "        false_positive_ls.append(false_positive)\n",
    "        false_negative_ls.append(false_negative)\n",
    "    return true_positive_ls, true_negative_ls, false_positive_ls, false_negative_ls\n",
    "#     print(correct_one_ls)\n",
    "#     print(wrong_ls)\n",
    "#     print(labelled_tags)\n",
    "#     print(overall_correct_one/labelled_tags)\n",
    "\n",
    "true_positive_ls, true_negative_ls, false_positive_ls, false_negative_ls = compute_accuracy(ytest, predictions.toarray())\n",
    "\n",
    "true_pos = sum(true_positive_ls)\n",
    "true_neg = sum(true_negative_ls)\n",
    "false_pos = sum(false_positive_ls)\n",
    "false_neg = sum(false_negative_ls)\n",
    "# print(true_pos)\n",
    "# print(true_neg)\n",
    "# print(false_pos)\n",
    "# print(false_neg)\n",
    "precision = true_pos/(true_pos + false_pos)\n",
    "recall = true_pos/(true_pos + false_neg)\n",
    "accuracy = (true_pos + true_neg) / (false_pos + false_neg + true_pos + true_neg)\n",
    "weighted_harmonic_mean = (2 * precision * recall) / (precision + recall)\n",
    "print('The precision is {}'.format(precision))\n",
    "print('The recall is {}'.format(recall))\n",
    "print('The accuracry (naive) is {}'.format(accuracy))\n",
    "print('The weighted harmonic mean/F1 score is {}'.format(weighted_harmonic_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# # For each class\n",
    "# precision = dict()\n",
    "# recall = dict()\n",
    "# average_precision = dict()\n",
    "# for i in range(234):\n",
    "#     precision[i], recall[i], _ = precision_recall_curve(ytest[:, i],\n",
    "#                                                         predictions.toarray()[:, i])\n",
    "#     average_precision[i] = average_precision_score(ytest[:, i], predictions.toarray()[:, i])\n",
    "\n",
    "# # A \"micro-average\": quantifying score on all classes jointly\n",
    "# precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(ytest.ravel(),\n",
    "#     predictions.toarray().ravel())\n",
    "# average_precision[\"micro\"] = average_precision_score(ytest, predictions.toarray(),\n",
    "#                                                      average=\"micro\")\n",
    "# print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
