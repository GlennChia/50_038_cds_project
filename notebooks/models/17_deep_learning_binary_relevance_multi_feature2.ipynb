{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links\n",
    "\n",
    "1. For the architecture https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada\n",
    "2. https://androidkt.com/multi-label-text-classification-in-tensorflow-keras/\n",
    "3. https://keras.io/preprocessing/sequence/\n",
    "4. https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/ ( Not really)\n",
    "5. For deep learning using word embeddings https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/processed/\"\n",
    "INPUT_FILE_NAME = 'final_squash15_with_pos_ner_tm.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>sim_tags</th>\n",
       "      <th>...</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>[thank, chris, truly, great, honor, opportunit...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>cars,solar system,energy,culture,politics,scie...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>[term, invention, like, tell, tale, favorite, ...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>macarthur grant,simplicity,design,solar system...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>[public, dewey, long, ago, observe, constitute...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>corruption,inequality,science,investment,war,c...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032121</td>\n",
       "      <td>0.075614</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>[want, start, say, houston, problem, enter, se...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>flight,design,nasa,science,invention,entrepren...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>[want, talk, background, idea, car, art, actua...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>cars,design,transportation,invention,technolog...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032952</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker                              headline  \\\n",
       "0       Al Gore           Averting the climate crisis   \n",
       "1     Amy Smith         Simple designs to save a life   \n",
       "2  Ashraf Ghani         How to rebuild a broken state   \n",
       "3    Burt Rutan  The real future of space exploration   \n",
       "4  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                         description duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4  American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                tags  \\\n",
       "0  cars,alternative energy,culture,politics,scien...   \n",
       "1  MacArthur grant,simplicity,industrial design,a...   \n",
       "2  corruption,poverty,economics,investment,milita...   \n",
       "3  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript      WC  \\\n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "\n",
       "                                    clean_transcript  \\\n",
       "0  [thank, chris, truly, great, honor, opportunit...   \n",
       "1  [term, invention, like, tell, tale, favorite, ...   \n",
       "2  [public, dewey, long, ago, observe, constitute...   \n",
       "3  [want, start, say, houston, problem, enter, se...   \n",
       "4  [want, talk, background, idea, car, art, actua...   \n",
       "\n",
       "                             clean_transcript_string  \\\n",
       "0  thank chris truly great honor opportunity come...   \n",
       "1  term invention like tell tale favorite project...   \n",
       "2  public dewey long ago observe constitute discu...   \n",
       "3  want start say houston problem enter second ge...   \n",
       "4  want talk background idea car art actually mea...   \n",
       "\n",
       "                                            sim_tags    ...    topic5  \\\n",
       "0  cars,solar system,energy,culture,politics,scie...    ...       0.0   \n",
       "1  macarthur grant,simplicity,design,solar system...    ...       0.0   \n",
       "2  corruption,inequality,science,investment,war,c...    ...       0.0   \n",
       "3  flight,design,nasa,science,invention,entrepren...    ...       0.0   \n",
       "4  cars,design,transportation,invention,technolog...    ...       0.0   \n",
       "\n",
       "     topic6    topic7    topic8    topic9  topic10  topic11   topic12  \\\n",
       "0  0.012230  0.000000  0.000000  0.001562      0.0      0.0  0.000000   \n",
       "1  0.001854  0.005125  0.002454  0.000472      0.0      0.0  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000      0.0      0.0  0.032121   \n",
       "3  0.000000  0.000000  0.000000  0.000000      0.0      0.0  0.016062   \n",
       "4  0.000000  0.114847  0.000000  0.000000      0.0      0.0  0.000000   \n",
       "\n",
       "    topic13   topic14  \n",
       "0  0.004836  0.000000  \n",
       "1  0.000000  0.021417  \n",
       "2  0.075614  0.000000  \n",
       "3  0.000000  0.034169  \n",
       "4  0.032952  0.000000  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2313 entries, 0 to 2312\n",
      "Data columns (total 14 columns):\n",
      "speaker                    2313 non-null object\n",
      "headline                   2313 non-null object\n",
      "description                2313 non-null object\n",
      "duration                   2313 non-null object\n",
      "tags                       2313 non-null object\n",
      "transcript                 2313 non-null object\n",
      "WC                         2313 non-null float64\n",
      "clean_transcript           2313 non-null object\n",
      "clean_transcript_string    2313 non-null object\n",
      "sim_tags                   2313 non-null object\n",
      "squash15_tags              2313 non-null object\n",
      "topic0                     15 non-null float64\n",
      "topic1                     15 non-null float64\n",
      "topic2                     15 non-null float64\n",
      "dtypes: float64(4), object(10)\n",
      "memory usage: 253.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:,:14].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    \n",
    "def compute_tag_ratio(target_column, df=df):\n",
    "    tags = df[target_column].str.replace(', ',',').str.lower().str.strip()\n",
    "    split_tags = tags.str.split(',')\n",
    "    tag_counts_per_talk = split_tags.apply(len)\n",
    "\n",
    "    joined_tags = tags.str.cat(sep=',').split(',')\n",
    "    all_tags = pd.Series(joined_tags)\n",
    "\n",
    "    tag_counts = all_tags.value_counts().rename_axis(target_column).reset_index(name='counts')\n",
    "    tag_counts['no_count'] = len(df)-tag_counts['counts']\n",
    "    tag_counts['ratio'] = tag_counts['counts']/tag_counts['no_count']\n",
    "    tag_counts['overall_ratio'] = tag_counts['counts']/(tag_counts['no_count'] + tag_counts['counts'])\n",
    "    return tag_counts\n",
    "\n",
    "#print(compute_tag_ratio('squash3_tags', df))\n",
    "squashed_tag_counts = compute_tag_ratio('squash15_tags', df)\n",
    "print_full_dataframe(squashed_tag_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction via Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# y = []\n",
    "# for index, row in df.iterrows():\n",
    "#     y.append(set(row['squash3_tags'].split(',')))\n",
    "    \n",
    "# mlb = MultiLabelBinarizer()\n",
    "# encoded_y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoded_y[0])\n",
    "# print(len(encoded_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tags = df['squash15_tags'].str.cat(sep=',').split(',')\n",
    "all_tags = pd.Series(joined_tags).str.strip().str.lower()\n",
    "all_tags = list(dict.fromkeys(all_tags))\n",
    "try:\n",
    "    all_tags.remove('')\n",
    "except:\n",
    "    pass\n",
    "print(all_tags)\n",
    "print(len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encode(df=df):\n",
    "    complete_transcripts_tags = []\n",
    "    for rows, value in df.iterrows():\n",
    "        one_hot_encoding = [0] * len(all_tags)\n",
    "        headline = [value['headline']]\n",
    "        transcript = [value['clean_transcript_string']]\n",
    "        pos_sequence = [value['pos_sequence']]\n",
    "        ner_sequence = [value['ner_sequence']]\n",
    "        indiv_tags = value['squash15_tags'].split(',')\n",
    "        for tags in indiv_tags:\n",
    "            if tags == '':\n",
    "                continue\n",
    "            index = all_tags.index(tags.lower().lstrip(' '))\n",
    "            one_hot_encoding[index] = 1\n",
    "        indiv_transcript_tags = headline + transcript + pos_sequence + ner_sequence + one_hot_encoding\n",
    "        complete_transcripts_tags.append(indiv_transcript_tags)\n",
    "    return pd.DataFrame(complete_transcripts_tags, columns=['headline', 'transcript', 'pos_sequence', 'ner_sequence'] + all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_one_hot_encode()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_column(target_tag, df=df):\n",
    "    return df[['headline', 'transcript','pos_sequence', 'ner_sequence', target_tag]]\n",
    "single_class = get_target_column('culture', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x = single_class[['transcript']]\n",
    "# df_y = df[['technology']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = single_class[['headline', 'transcript','pos_sequence', 'ner_sequence']]\n",
    "df_y = list(single_class['culture'])\n",
    "#print(df_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, train_y, valid_y = train_test_split(df_x, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D, concatenate\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import optimizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Use word embeddings for the main transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract train and test transcripts to list \n",
    "X_train_transcripts = X_train['transcript'].tolist()\n",
    "X_test_transcripts = X_test['transcript'].tolist()\n",
    "# Extract headline - we will use tfidf because headlines are short \n",
    "X_train_headline = X_train['headline'].tolist()\n",
    "X_test_headline = X_test['headline'].tolist()\n",
    "# Extract POS tags\n",
    "X_train_pos_seq= X_train['pos_sequence'].tolist()\n",
    "X_test_pos_seq = X_test['pos_sequence'].tolist()\n",
    "# Extract NER tags\n",
    "X_train_ner_seq = X_train['ner_sequence'].tolist()\n",
    "X_test_ner_seq = X_test['ner_sequence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train_transcripts)\n",
    "\n",
    "X_train_transcripts = tokenizer.texts_to_sequences(X_train_transcripts)\n",
    "X_test_transcripts = tokenizer.texts_to_sequences(X_test_transcripts)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 500 # since the average length is about there. Too long and the predicions are bad. we assume the intro has the most info\n",
    "\n",
    "X_train_transcripts = pad_sequences(X_train_transcripts, padding='post', maxlen=maxlen)\n",
    "X_test_transcripts = pad_sequences(X_test_transcripts, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_path = \"C:/Users/JSaw/Downloads/\"\n",
    "glove_file = open(glove_path+'glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_transcripts[1])\n",
    "print(X_train_transcripts.shape)\n",
    "print(type(X_train_transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 tfidf the headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vect_pos = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=50)\n",
    "# tfidf_vect_pos.fit(X_train_headline)\n",
    "\n",
    "# xtrain_tfidf_headline =  tfidf_vect_pos.transform(X_train_headline)\n",
    "# xtest_tfidf_headline =  tfidf_vect_pos.transform(X_test_headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xtrain_tfidf_headline.shape)\n",
    "# print(xtest_tfidf_headline.shape)\n",
    "# print(xtrain_tfidf_headline[0])\n",
    "# print(type(xtrain_tfidf_headline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vect_pos = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# tfidf_vect_pos.fit(df['pos_sequence'])\n",
    "# tfidf_vect_ner = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# tfidf_vect_ner.fit(df['ner_sequence'])\n",
    "\n",
    "# xtrain_tfidf_pos =  tfidf_vect_pos.transform(X_train['pos_sequence'])\n",
    "# xtest_tfidf_pos =  tfidf_vect_pos.transform(X_test['pos_sequence'])\n",
    "\n",
    "# xtrain_tfidf_ner =  tfidf_vect_ner.transform(X_train['ner_sequence'])\n",
    "# xtest_tfidf_ner =  tfidf_vect_ner.transform(X_test['ner_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try word embeddings on the vector \n",
    "tokenizer2 = Tokenizer(num_words=100)\n",
    "tokenizer2.fit_on_texts(X_train_headline)\n",
    "\n",
    "X_train_headline = tokenizer.texts_to_sequences(X_train_headline)\n",
    "X_test_headline = tokenizer.texts_to_sequences(X_test_headline)\n",
    "\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1\n",
    "\n",
    "maxlen2 = 100 # since the average length is about there. Too long and the predicions are bad. we assume the intro has the most info\n",
    "\n",
    "X_train_headline = pad_sequences(X_train_headline, padding='post', maxlen=maxlen2)\n",
    "X_test_headline = pad_sequences(X_test_headline, padding='post', maxlen=maxlen2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "# define two sets of inputs\n",
    "inputA = Input(shape=(maxlen2,))\n",
    "inputB = Input(shape=(maxlen,))\n",
    " \n",
    "# the first branch operates on the first input which is the headline\n",
    "embedding_layer_hedline = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputA) \n",
    "#model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "#model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "x = Conv1D(128, 5, activation='relu')(embedding_layer_hedline)\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(10, activation='relu')(x)\n",
    "X = Dropout(0.2)(x)\n",
    "x = Dense(4, activation=\"relu\")(x)\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# x = Dense(50, activation=\"relu\")(inputA)\n",
    "# x = Dense(4, activation=\"relu\")(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    " \n",
    "# the second branch opreates on the second input\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputB)\n",
    "y = LSTM(128)(embedding_layer)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(4, activation='relu')(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    " \n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output])\n",
    " \n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(2, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"sigmoid\")(z)\n",
    " \n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "print(model.summary())\n",
    "adam = optimizers.adam(lr=0.0001)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=0.001)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "history = model.fit([X_train_headline, X_train_transcripts], train_y, batch_size=32, epochs=4, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# # define two sets of inputs\n",
    "# inputA = Input(shape=(50,))\n",
    "# inputB = Input(shape=(maxlen,))\n",
    " \n",
    "# # the first branch operates on the first input\n",
    "# x = Dense(50, activation=\"relu\")(inputA)\n",
    "# x = Dense(4, activation=\"relu\")(x)\n",
    "# x = Model(inputs=inputA, outputs=x)\n",
    " \n",
    "# # the second branch opreates on the second input\n",
    "# embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputB)\n",
    "# y = LSTM(128)(embedding_layer)\n",
    "# y = Dense(4, activation='sigmoid')(y)\n",
    "# y = Model(inputs=inputB, outputs=y)\n",
    " \n",
    "# # combine the output of the two branches\n",
    "# combined = concatenate([x.output, y.output])\n",
    " \n",
    "# # apply a FC layer and then a regression prediction on the\n",
    "# # combined outputs\n",
    "# z = Dense(2, activation=\"relu\")(combined)\n",
    "# z = Dense(1, activation=\"linear\")(z)\n",
    " \n",
    "# # our model will accept the inputs of the two branches and\n",
    "# # then output a single value\n",
    "# model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "# print(model.summary())\n",
    "# adam = optimizers.adam(lr=0.0001)\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot_cnn.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = optimizers.adam(lr=0.0001)\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "# history = model.fit([xtrain_tfidf_headline, X_train_transcripts], train_y, batch_size=32, epochs=4, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_inputs = Input(shape=(maxlen,))\n",
    "# embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "# LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "# dense_layer_1 = Dense(1, activation='sigmoid')(LSTM_Layer_1)\n",
    "# model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# history = model.fit(X_train, train_y, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_glove = Sequential()\n",
    "# model_glove.add(Embedding(vocab_size, 100, input_length=3000, weights=[embedding_matrix], trainable=False))(Input(shape=(maxlen,)))\n",
    "# model_glove.add(Dropout(0.2))\n",
    "# model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "# model_glove.add(MaxPooling1D(pool_size=4))\n",
    "# model_glove.add(LSTM(100))\n",
    "# model_glove.add(Dense(1, activation='sigmoid'))\n",
    "# model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# ## Fit train data\n",
    "# model_glove.fit(X_train, np.array(train_y), validation_split=0.2, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test_headline, X_test_transcripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(threshold, predictions=predictions):\n",
    "    return [[1 if j > threshold else 0 for j in i.tolist()] for i in predictions]\n",
    "\n",
    "def get_tag_flat(threshold, predictions=predictions):\n",
    "    return [1 if j > threshold else 0 for i in predictions for j in i]\n",
    "predictions_flushed = get_tag(0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_tn_fp_fn(y_test, y_pred, classes):\n",
    "    '''\n",
    "    Return:\n",
    "    pre_score = {\n",
    "        'tag_1': {\n",
    "            'index': ,\n",
    "            'tp': ,\n",
    "            'tn': ,\n",
    "            'fp': ,\n",
    "            'fn': \n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # Create dictionary of tags \n",
    "    pre_score = {}\n",
    "    for index_tag, tag in enumerate(classes):\n",
    "        pre_score[tag] = {\n",
    "            'index':index_tag,\n",
    "            'tp': 0,\n",
    "            'tn': 0,\n",
    "            'fp': 0,\n",
    "            'fn': 0\n",
    "        }\n",
    "    for transcript_index, transcript_value in enumerate(y_test):\n",
    "        if transcript_value == y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['tp'] += 1\n",
    "        elif transcript_value == y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['tn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['fn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['fp'] += 1\n",
    "    return pre_score\n",
    "scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, ['culture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall_f1(preprocessed_scores):\n",
    "    for key, value in preprocessed_scores.items():\n",
    "        try:\n",
    "            precision = value['tp']/(value['tp']+value['fp'])\n",
    "        except:\n",
    "            print('precision issue: {}'.format(key))\n",
    "            precision = 0.0\n",
    "        try:\n",
    "            recall = value['tp']/(value['tp']+value['fn'])\n",
    "        except:\n",
    "            print('recall issue: {}'.format(key))\n",
    "            recall = 0.0\n",
    "        try:\n",
    "            f1 = (2 * precision * recall)/(precision + recall)\n",
    "        except:\n",
    "            print('f1 issue: {}'.format(key))\n",
    "            f1=0.0\n",
    "        preprocessed_scores[key]['precision'] = round(precision,2)\n",
    "        preprocessed_scores[key]['recall'] = round(recall,2)\n",
    "        preprocessed_scores[key]['f1'] = round(f1,2)\n",
    "    return preprocessed_scores\n",
    "final_scores = compute_precision_recall_f1(scores_preprocess)\n",
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scores_df(tag_classes, final_scores=final_scores):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    for index, value in enumerate(tag_classes):\n",
    "        precision.append(final_scores[value]['precision'])\n",
    "        recall.append(final_scores[value]['recall'])\n",
    "        f1.append(final_scores[value]['f1'])\n",
    "        accuracy.append((final_scores[value]['tp'] + final_scores[value]['tn'])/(final_scores[value]['tp'] + final_scores[value]['tn'] + final_scores[value]['fp'] + final_scores[value]['fn']))\n",
    "    df_result = pd.DataFrame(list(zip(tag_classes, precision, recall, f1, accuracy)), \n",
    "               columns =['class', 'precision', 'recall', 'f1', 'accuracy']) \n",
    "    return df_result\n",
    "df_results = format_scores_df(['culture'], final_scores)\n",
    "print_full_dataframe(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25, 45):\n",
    "    i = i/100\n",
    "    print(i)\n",
    "    predictions_flushed = get_tag(i)\n",
    "    scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, ['culture'])\n",
    "    final_scores = compute_precision_recall_f1(scores_preprocess)\n",
    "    print(final_scores)\n",
    "    df_results = format_scores_df(['culture'], final_scores)\n",
    "    print(df_results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluate_on_training_set(y_test, y_pred):\n",
    "  # Calculate AUC\n",
    "  print(\"AUC is: \", roc_auc_score(y_test, y_pred))\n",
    "  # print out recall and precision\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  # print out confusion matrix\n",
    "  print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "  # # calculate points for ROC curve\n",
    "  fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "  # Plot ROC curve\n",
    "  plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_score(y_test, y_pred)) \n",
    "  plt.plot([0, 1], [0, 1], 'k--') # random predictions curve\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.0])\n",
    "  plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "  plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "  plt.title('Receiver Operating Characteristic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_training_set(valid_y, get_tag_flat(0.4))\n",
    "#print(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
