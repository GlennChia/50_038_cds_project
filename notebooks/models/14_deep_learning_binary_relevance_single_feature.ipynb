{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links\n",
    "\n",
    "1. For the architecture https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada\n",
    "2. https://androidkt.com/multi-label-text-classification-in-tensorflow-keras/\n",
    "3. https://keras.io/preprocessing/sequence/\n",
    "4. https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/ ( Not really)\n",
    "5. For deep learning using word embeddings https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/processed/\"\n",
    "INPUT_FILE_NAME = 'cleaned_squash3_with_pos_ner.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>squash_tags</th>\n",
       "      <th>squash2_tags</th>\n",
       "      <th>squash3_tags</th>\n",
       "      <th>pos_sequence</th>\n",
       "      <th>ner_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>[thank, chris, truly, great, honor, opportunit...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>culture,politics,science,climate change,enviro...</td>\n",
       "      <td>culture,politics,science,global issues,environ...</td>\n",
       "      <td>culture,politics,science,global issues,environ...</td>\n",
       "      <td>VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...</td>\n",
       "      <td>PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>[term, invention, like, tell, tale, favorite, ...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>invention,engineering,design,global issues</td>\n",
       "      <td>invention,engineering,design,global issues</td>\n",
       "      <td>invention,design,global issues</td>\n",
       "      <td>NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...</td>\n",
       "      <td>GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>[public, dewey, long, ago, observe, constitute...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>poverty,economics,culture,politics,policy,glob...</td>\n",
       "      <td>inequality,economics,culture,politics,governme...</td>\n",
       "      <td>inequality,economics,culture,politics,global i...</td>\n",
       "      <td>ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...</td>\n",
       "      <td>DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>[want, start, say, houston, problem, enter, se...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>invention,engineering,entrepreneur,design,busi...</td>\n",
       "      <td>invention,engineering,entrepreneur,design,busi...</td>\n",
       "      <td>invention,design,business</td>\n",
       "      <td>VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>[want, talk, background, idea, car, art, actua...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>invention,design,technology,business,art</td>\n",
       "      <td>invention,design,technology,business,art</td>\n",
       "      <td>invention,design,technology,business,art</td>\n",
       "      <td>VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...</td>\n",
       "      <td>PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker                              headline  \\\n",
       "0       Al Gore           Averting the climate crisis   \n",
       "1     Amy Smith         Simple designs to save a life   \n",
       "2  Ashraf Ghani         How to rebuild a broken state   \n",
       "3    Burt Rutan  The real future of space exploration   \n",
       "4  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                         description duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4  American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                tags  \\\n",
       "0  cars,alternative energy,culture,politics,scien...   \n",
       "1  MacArthur grant,simplicity,industrial design,a...   \n",
       "2  corruption,poverty,economics,investment,milita...   \n",
       "3  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript      WC  \\\n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "\n",
       "                                    clean_transcript  \\\n",
       "0  [thank, chris, truly, great, honor, opportunit...   \n",
       "1  [term, invention, like, tell, tale, favorite, ...   \n",
       "2  [public, dewey, long, ago, observe, constitute...   \n",
       "3  [want, start, say, houston, problem, enter, se...   \n",
       "4  [want, talk, background, idea, car, art, actua...   \n",
       "\n",
       "                             clean_transcript_string  \\\n",
       "0  thank chris truly great honor opportunity come...   \n",
       "1  term invention like tell tale favorite project...   \n",
       "2  public dewey long ago observe constitute discu...   \n",
       "3  want start say houston problem enter second ge...   \n",
       "4  want talk background idea car art actually mea...   \n",
       "\n",
       "                                         squash_tags  \\\n",
       "0  culture,politics,science,climate change,enviro...   \n",
       "1         invention,engineering,design,global issues   \n",
       "2  poverty,economics,culture,politics,policy,glob...   \n",
       "3  invention,engineering,entrepreneur,design,busi...   \n",
       "4           invention,design,technology,business,art   \n",
       "\n",
       "                                        squash2_tags  \\\n",
       "0  culture,politics,science,global issues,environ...   \n",
       "1         invention,engineering,design,global issues   \n",
       "2  inequality,economics,culture,politics,governme...   \n",
       "3  invention,engineering,entrepreneur,design,busi...   \n",
       "4           invention,design,technology,business,art   \n",
       "\n",
       "                                        squash3_tags  \\\n",
       "0  culture,politics,science,global issues,environ...   \n",
       "1                     invention,design,global issues   \n",
       "2  inequality,economics,culture,politics,global i...   \n",
       "3                          invention,design,business   \n",
       "4           invention,design,technology,business,art   \n",
       "\n",
       "                                        pos_sequence  \\\n",
       "0  VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...   \n",
       "1  NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...   \n",
       "2  ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...   \n",
       "3  VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...   \n",
       "4  VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...   \n",
       "\n",
       "                                        ner_sequence  \n",
       "0  PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...  \n",
       "1  GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...  \n",
       "2  DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...  \n",
       "3  GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...  \n",
       "4  PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2328 entries, 0 to 2327\n",
      "Data columns (total 14 columns):\n",
      "speaker                    2328 non-null object\n",
      "headline                   2328 non-null object\n",
      "description                2328 non-null object\n",
      "duration                   2328 non-null object\n",
      "tags                       2328 non-null object\n",
      "transcript                 2328 non-null object\n",
      "WC                         2328 non-null float64\n",
      "clean_transcript           2328 non-null object\n",
      "clean_transcript_string    2328 non-null object\n",
      "squash_tags                2328 non-null object\n",
      "squash2_tags               2328 non-null object\n",
      "squash3_tags               2328 non-null object\n",
      "pos_sequence               2328 non-null object\n",
      "ner_sequence               2328 non-null object\n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 254.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:,:14].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     squash3_tags  counts  no_count     ratio  overall_ratio\n",
      "0         culture    1106      1222  0.905074       0.475086\n",
      "1         science     868      1460  0.594521       0.372852\n",
      "2      technology     787      1541  0.510707       0.338058\n",
      "3   global issues     679      1649  0.411765       0.291667\n",
      "4          design     400      1928  0.207469       0.171821\n",
      "5        business     329      1999  0.164582       0.141323\n",
      "6   entertainment     285      2043  0.139501       0.122423\n",
      "7             art     261      2067  0.126270       0.112113\n",
      "8          future     218      2110  0.103318       0.093643\n",
      "9    biodiversity     215      2113  0.101751       0.092354\n",
      "10      education     206      2122  0.097078       0.088488\n",
      "11  communication     185      2143  0.086328       0.079467\n",
      "12       politics     183      2145  0.085315       0.078608\n",
      "13       humanity     164      2164  0.075786       0.070447\n",
      "14  collaboration     163      2165  0.075289       0.070017\n",
      "15           life     156      2172  0.071823       0.067010\n",
      "16    environment     155      2173  0.071330       0.066581\n",
      "17      economics     154      2174  0.070837       0.066151\n",
      "18          brain     148      2180  0.067890       0.063574\n",
      "19       activism     147      2181  0.067400       0.063144\n",
      "20      invention     136      2192  0.062044       0.058419\n",
      "21      community     136      2192  0.062044       0.058419\n",
      "22        history     135      2193  0.061560       0.057990\n",
      "23       children     135      2193  0.061560       0.057990\n",
      "24     inequality     128      2200  0.058182       0.054983\n",
      "25          music     126      2202  0.057221       0.054124\n"
     ]
    }
   ],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    \n",
    "def compute_tag_ratio(target_column, df=df):\n",
    "    tags = df[target_column].str.replace(', ',',').str.lower().str.strip()\n",
    "    split_tags = tags.str.split(',')\n",
    "    tag_counts_per_talk = split_tags.apply(len)\n",
    "\n",
    "    joined_tags = tags.str.cat(sep=',').split(',')\n",
    "    all_tags = pd.Series(joined_tags)\n",
    "\n",
    "    tag_counts = all_tags.value_counts().rename_axis(target_column).reset_index(name='counts')\n",
    "    tag_counts['no_count'] = len(df)-tag_counts['counts']\n",
    "    tag_counts['ratio'] = tag_counts['counts']/tag_counts['no_count']\n",
    "    tag_counts['overall_ratio'] = tag_counts['counts']/(tag_counts['no_count'] + tag_counts['counts'])\n",
    "    return tag_counts\n",
    "\n",
    "#print(compute_tag_ratio('squash3_tags', df))\n",
    "squashed_tag_counts = compute_tag_ratio('squash3_tags', df)\n",
    "print_full_dataframe(squashed_tag_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction via Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# y = []\n",
    "# for index, row in df.iterrows():\n",
    "#     y.append(set(row['squash3_tags'].split(',')))\n",
    "    \n",
    "# mlb = MultiLabelBinarizer()\n",
    "# encoded_y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoded_y[0])\n",
    "# print(len(encoded_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['culture', 'politics', 'science', 'global issues', 'environment', 'technology', 'invention', 'design', 'inequality', 'economics', 'business', 'art', 'biodiversity', 'music', 'entertainment', 'collaboration', 'education', 'history', 'future', 'communication', 'community', 'activism', 'children', 'brain', 'humanity', 'life']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "joined_tags = df['squash3_tags'].str.cat(sep=',').split(',')\n",
    "all_tags = pd.Series(joined_tags).str.strip().str.lower()\n",
    "all_tags = list(dict.fromkeys(all_tags))\n",
    "try:\n",
    "    all_tags.remove('')\n",
    "except:\n",
    "    pass\n",
    "print(all_tags)\n",
    "print(len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encode(df=df):\n",
    "    complete_transcripts_tags = []\n",
    "    for rows, value in df.iterrows():\n",
    "        one_hot_encoding = [0] * 26\n",
    "        headline = [value['headline']]\n",
    "        transcript = [value['clean_transcript_string']]\n",
    "        pos_sequence = [value['pos_sequence']]\n",
    "        ner_sequence = [value['ner_sequence']]\n",
    "        indiv_tags = value['squash3_tags'].split(',')\n",
    "        for tags in indiv_tags:\n",
    "            if tags == '':\n",
    "                continue\n",
    "            index = all_tags.index(tags.lower().lstrip(' '))\n",
    "            one_hot_encoding[index] = 1\n",
    "        indiv_transcript_tags = headline + transcript + pos_sequence + ner_sequence + one_hot_encoding\n",
    "        complete_transcripts_tags.append(indiv_transcript_tags)\n",
    "    return pd.DataFrame(complete_transcripts_tags, columns=['headline', 'transcript', 'pos_sequence', 'ner_sequence'] + all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>transcript</th>\n",
       "      <th>pos_sequence</th>\n",
       "      <th>ner_sequence</th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>global issues</th>\n",
       "      <th>environment</th>\n",
       "      <th>technology</th>\n",
       "      <th>...</th>\n",
       "      <th>education</th>\n",
       "      <th>history</th>\n",
       "      <th>future</th>\n",
       "      <th>communication</th>\n",
       "      <th>community</th>\n",
       "      <th>activism</th>\n",
       "      <th>children</th>\n",
       "      <th>brain</th>\n",
       "      <th>humanity</th>\n",
       "      <th>life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...</td>\n",
       "      <td>PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...</td>\n",
       "      <td>GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...</td>\n",
       "      <td>DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...</td>\n",
       "      <td>PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>Why glass towers are bad for city life -- and ...</td>\n",
       "      <td>imagine walk even discover everybody room look...</td>\n",
       "      <td>VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...</td>\n",
       "      <td>ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>What happens in your brain when you pay attent...</td>\n",
       "      <td>pay close attention easy attention pull differ...</td>\n",
       "      <td>VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...</td>\n",
       "      <td>ORDINAL PERSON PRODUCT DATE EVENT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>Why you should define your fears instead of yo...</td>\n",
       "      <td>happy pic take senior college right dance prac...</td>\n",
       "      <td>ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...</td>\n",
       "      <td>DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>12 truths I learned from life and writing</td>\n",
       "      <td>sevenyearold grandson sleep hall wake lot morn...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...</td>\n",
       "      <td>PERSON PERSON PERSON PERSON PERSON DATE CARDIN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>How I built a jet suit</td>\n",
       "      <td>michael brown engineer innovator inventor insp...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...</td>\n",
       "      <td>PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2328 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headline  \\\n",
       "0                           Averting the climate crisis   \n",
       "1                         Simple designs to save a life   \n",
       "2                         How to rebuild a broken state   \n",
       "3                  The real future of space exploration   \n",
       "4                              Great cars are great art   \n",
       "...                                                 ...   \n",
       "2323  Why glass towers are bad for city life -- and ...   \n",
       "2324  What happens in your brain when you pay attent...   \n",
       "2325  Why you should define your fears instead of yo...   \n",
       "2326          12 truths I learned from life and writing   \n",
       "2327                             How I built a jet suit   \n",
       "\n",
       "                                             transcript  \\\n",
       "0     thank chris truly great honor opportunity come...   \n",
       "1     term invention like tell tale favorite project...   \n",
       "2     public dewey long ago observe constitute discu...   \n",
       "3     want start say houston problem enter second ge...   \n",
       "4     want talk background idea car art actually mea...   \n",
       "...                                                 ...   \n",
       "2323  imagine walk even discover everybody room look...   \n",
       "2324  pay close attention easy attention pull differ...   \n",
       "2325  happy pic take senior college right dance prac...   \n",
       "2326  sevenyearold grandson sleep hall wake lot morn...   \n",
       "2327  michael brown engineer innovator inventor insp...   \n",
       "\n",
       "                                           pos_sequence  \\\n",
       "0     VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...   \n",
       "1     NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...   \n",
       "2     ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...   \n",
       "3     VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...   \n",
       "4     VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...   \n",
       "...                                                 ...   \n",
       "2323  VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...   \n",
       "2324  VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...   \n",
       "2325  ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...   \n",
       "2326  PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...   \n",
       "2327  PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...   \n",
       "\n",
       "                                           ner_sequence  culture  politics  \\\n",
       "0     PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...        1         1   \n",
       "1     GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...        0         0   \n",
       "2     DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...        1         1   \n",
       "3     GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...        0         0   \n",
       "4     PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...        0         0   \n",
       "...                                                 ...      ...       ...   \n",
       "2323  ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...        1         0   \n",
       "2324                 ORDINAL PERSON PRODUCT DATE EVENT         0         0   \n",
       "2325  DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...        1         0   \n",
       "2326  PERSON PERSON PERSON PERSON PERSON DATE CARDIN...        1         0   \n",
       "2327  PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...        1         0   \n",
       "\n",
       "      science  global issues  environment  technology  ...  education  \\\n",
       "0           1              1            1           1  ...          0   \n",
       "1           0              1            0           0  ...          0   \n",
       "2           0              1            0           0  ...          0   \n",
       "3           0              0            0           0  ...          0   \n",
       "4           0              0            0           1  ...          0   \n",
       "...       ...            ...          ...         ...  ...        ...   \n",
       "2323        0              0            0           0  ...          0   \n",
       "2324        1              0            0           1  ...          0   \n",
       "2325        0              0            0           0  ...          0   \n",
       "2326        0              0            0           0  ...          0   \n",
       "2327        0              0            0           1  ...          0   \n",
       "\n",
       "      history  future  communication  community  activism  children  brain  \\\n",
       "0           0       0              0          0         0         0      0   \n",
       "1           0       0              0          0         0         0      0   \n",
       "2           0       0              0          0         0         0      0   \n",
       "3           0       0              0          0         0         0      0   \n",
       "4           0       0              0          0         0         0      0   \n",
       "...       ...     ...            ...        ...       ...       ...    ...   \n",
       "2323        0       0              0          1         0         0      0   \n",
       "2324        0       0              0          0         0         0      1   \n",
       "2325        0       0              0          0         0         0      0   \n",
       "2326        0       0              1          0         0         0      0   \n",
       "2327        0       1              0          0         0         0      0   \n",
       "\n",
       "      humanity  life  \n",
       "0            0     0  \n",
       "1            0     0  \n",
       "2            0     0  \n",
       "3            0     0  \n",
       "4            0     0  \n",
       "...        ...   ...  \n",
       "2323         0     0  \n",
       "2324         0     0  \n",
       "2325         1     1  \n",
       "2326         1     0  \n",
       "2327         0     0  \n",
       "\n",
       "[2328 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_one_hot_encode()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_column(target_tag, df=df):\n",
    "    return df[['headline', 'transcript','pos_sequence', 'ner_sequence', target_tag]]\n",
    "#single_class = get_target_column('technology', df)\n",
    "single_class = get_target_column('culture', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x = single_class[['transcript']]\n",
    "# df_y = df[['technology']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank chris truly great honor opportunity come stage twice extremely grateful blow away conference want thank nice comment night sincerely partly mock sob need position fly air force year shoe boot airplane tell quick story illustrate like true story bite true soon tipper leave mock sob white house drive home nashville little farm mile east nashville drive know sound like little thing look rearview mirror sudden hit motorcade hear phantom limb pain rent ford taurus dinnertime start look place eat get exit lebanon tennessee get exit shoneys restaurant lowcost family restaurant chain know go sit booth waitress come big commotion tipper take order go couple booth low voice strain hear say say yes vice president al gore wife tipper man say come long way kind series epiphany day continue totally true story get gv fly africa speech nigeria city lagos topic energy begin speech tell story happen day nashville tell pretty way share tipper drive shoneys lowcost family restaurant chain man say laugh give speech go airport fly home fell asleep plane middle night land azores island refuel wake open door go fresh air look man run runway wave piece paper yell washington washington think middle night middle atlantic world wrong washington remember bunch thing turn staff extremely upset wire service nigeria write story speech print city unite state america print monterey check story begin vice president al gore announce nigeria yesterday quote wife tipper open lowcost family restaurant name shoneys run yous soil david letterman jay leno start big white chef hat tipper say burger fry day late get nice long handwritten letter friend partner colleague clinton say congratulation new restaurant al like celebrate success life go talk information ecology think plan lifelong habit come ted maybe talk time chris anderson deal al gore want focus say like elaborate climate crisis want start couple go new image go recapitulate slide update slide time add new image learn time like beachcombing know time tide come shell day get new temperature record january unite state america historical average january degree month degree know want bad news environment kid recapitulation slide go new material want elaborate couple project yous contribution global warm business usual efficiency enduse electricity enduse energy lowhanging fruit efficiency conservation cost profit sign wrong negative positive investment pay effective deflect path car truck talk slideshow want perspective easy visible target concern global warm pollution come build car truck car truck significant low standard world address puzzle transportation efficiency important car truck renewables current level technological efficiency difference vinod john doerr lot people directly involve wedge go grow rapidly current projection show carbon capture sequestration cc stand likely killer app enable continue use fossil fuel way safe ok reduce emission home expenditure profitable insulation good design buy green electricity mention automobile buy hybrid use light rail figure option good important green consumer choice buy thing harsh effect harsh effect global climate crisis consider decision live carbonneutral life good brand love advice help way connect people easy think lot decision pretty easy mean reduce carbon dioxide emission range choice purchase acquire offset remainder completely reduce mean elaborate climatecrisisnet carbon calculator participant production convene active involvement lead software writer world arcane science carbon calculation construct consumerfriendly carbon calculator precisely calculate emission give option reduce time movie come update clickthrough purchase offset consider make business carbonneutral hard think integrate climate solution innovation technology entertainment design architecture community invest sustainably majora mention listen invest money manager compensate basis annual performance complain quarterly report ceo management time people pay judge go pay capital invest base shortterm return go shortterm decision lot say catalyst change teach learn talk movie movie version slideshow give night ago lot entertain come opportunity ensure lot people consider send somebody nashville pick personally go train people slideshow repurposed personal story obviously replace generic approach slide mean link go conduct course summer group people nominate different folk come en masse community country go update slideshow single week right cut edge work larry lessig process post tool limiteduse copyright young people remix way anybody idea ought stay arm length politics mean republican try convince democrat need republican bipartisan issue know group politically active democracy work way suppose work support idea cap carbon dioxide emission global warm pollution trade here long unite state world close close yous participation everybody board director people serve board director corporation close legal liability urge ceo maximum income reduce trade carbon emission avoid market work solve problem accomplish help mass persuasion campaign start spring change mind american people presently politician permission need modern country role logic reason long include mediate wealth power way repetition short hotbutton second second television ad buy lot ad let rebrand global warm suggest like climate crisis instead climate collapse good brand need help somebody say test face scientist tell combination opposable thumb neocortex viable combination true say night repeat political issue republican partisan influence democrat opportunity connect idea bring coherence thank appreciate\n"
     ]
    }
   ],
   "source": [
    "df_x = list(single_class['transcript'])\n",
    "# df_y = list(single_class['technology'])\n",
    "df_y = list(single_class['culture'])\n",
    "print(df_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, train_y, valid_y = train_test_split(df_x, df_y, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Use word embeddings for the main transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 800 # too many and the model cant tell the difference \n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('./glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 154  591   35 2075   35  260 1407 4414  597   31 1734 1346  464   73\n",
      "  154    2 3195  395 1382 3195  215  587 1451  531 1704 2316  205  101\n",
      "  350  587 4256  504   21 3271  330  233  272  315 4505   11  632  829\n",
      "   53  909    2  487  512 2229 1201  279  450  106   69 1825 2145  902\n",
      "  152  178 1017 1601    2 1734 1346 4415 1346  122 1734  196 1123 2944\n",
      " 1991  279  450   87 1734 1346 1066   35 1916 2903  279  142  183  673\n",
      "  198 2184  408  301  279  450  151  198  193  305  420    1 4865 1124\n",
      "  488  125 3049   21  594   73    1 1445 2719 4506  122   31  122   15\n",
      "   10  378 2833  165 2567 4506  165  179   58   48   98  599   16  131\n",
      "   36  254 4506 1337  177 2834 2591   83  271  356  533   91 1293   35\n",
      "  279  614  614  224  352  533    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "(1746, 800)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Glenn\\Anaconda3\\envs\\cds\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1396 samples, validate on 350 samples\n",
      "Epoch 1/4\n",
      "1396/1396 [==============================] - 75s 54ms/step - loss: 0.6946 - acc: 0.5315 - val_loss: 0.6954 - val_acc: 0.4914\n",
      "Epoch 2/4\n",
      "1396/1396 [==============================] - 72s 51ms/step - loss: 0.6869 - acc: 0.5387 - val_loss: 0.6926 - val_acc: 0.5486\n",
      "Epoch 3/4\n",
      "1396/1396 [==============================] - 80s 57ms/step - loss: 0.6847 - acc: 0.5423 - val_loss: 0.6921 - val_acc: 0.5457\n",
      "Epoch 4/4\n",
      "1396/1396 [==============================] - 78s 56ms/step - loss: 0.6772 - acc: 0.5559 - val_loss: 0.7076 - val_acc: 0.5086\n"
     ]
    }
   ],
   "source": [
    "deep_inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "dense_layer_1 = Dense(1, activation='sigmoid')(LSTM_Layer_1)\n",
    "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history = model.fit(X_train, train_y, batch_size=128, epochs=4, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_glove = Sequential()\n",
    "# model_glove.add(Embedding(vocab_size, 100, input_length=3000, weights=[embedding_matrix], trainable=False))(Input(shape=(maxlen,)))\n",
    "# model_glove.add(Dropout(0.2))\n",
    "# model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "# model_glove.add(MaxPooling1D(pool_size=4))\n",
    "# model_glove.add(LSTM(100))\n",
    "# model_glove.add(Dense(1, activation='sigmoid'))\n",
    "# model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# ## Fit train data\n",
    "# model_glove.fit(X_train, np.array(train_y), validation_split=0.2, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(threshold, predictions=predictions):\n",
    "    return [[1 if j > threshold else 0 for j in i.tolist()] for i in predictions]\n",
    "predictions_flushed = get_tag(0.45, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_tn_fp_fn(y_test, y_pred, classes):\n",
    "    '''\n",
    "    Return:\n",
    "    pre_score = {\n",
    "        'tag_1': {\n",
    "            'index': ,\n",
    "            'tp': ,\n",
    "            'tn': ,\n",
    "            'fp': ,\n",
    "            'fn': \n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # Create dictionary of tags \n",
    "    pre_score = {}\n",
    "    for index_tag, tag in enumerate(classes):\n",
    "        pre_score[tag] = {\n",
    "            'index':index_tag,\n",
    "            'tp': 0,\n",
    "            'tn': 0,\n",
    "            'fp': 0,\n",
    "            'fn': 0\n",
    "        }\n",
    "    for transcript_index, transcript_value in enumerate(y_test):\n",
    "        if transcript_value == y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['tp'] += 1\n",
    "        elif transcript_value == y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['tn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['fn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['fp'] += 1\n",
    "    return pre_score\n",
    "#scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, ['technology'])\n",
    "scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, ['culture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'culture': {'index': 0, 'tp': 273, 'tn': 13, 'fp': 288, 'fn': 8, 'precision': 0.49, 'recall': 0.97, 'f1': 0.65}}\n"
     ]
    }
   ],
   "source": [
    "def compute_precision_recall_f1(preprocessed_scores):\n",
    "    for key, value in preprocessed_scores.items():\n",
    "        try:\n",
    "            precision = value['tp']/(value['tp']+value['fp'])\n",
    "        except:\n",
    "            print('precision issue: {}'.format(key))\n",
    "            precision = 0.0\n",
    "        try:\n",
    "            recall = value['tp']/(value['tp']+value['fn'])\n",
    "        except:\n",
    "            print('recall issue: {}'.format(key))\n",
    "            recall = 0.0\n",
    "        try:\n",
    "            f1 = (2 * precision * recall)/(precision + recall)\n",
    "        except:\n",
    "            print('f1 issue: {}'.format(key))\n",
    "            f1=0.0\n",
    "        preprocessed_scores[key]['precision'] = round(precision,2)\n",
    "        preprocessed_scores[key]['recall'] = round(recall,2)\n",
    "        preprocessed_scores[key]['f1'] = round(f1,2)\n",
    "    return preprocessed_scores\n",
    "final_scores = compute_precision_recall_f1(scores_preprocess)\n",
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.49    0.97  0.65  0.491409\n"
     ]
    }
   ],
   "source": [
    "def format_scores_df(tag_classes, final_scores=final_scores):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    for index, value in enumerate(tag_classes):\n",
    "        precision.append(final_scores[value]['precision'])\n",
    "        recall.append(final_scores[value]['recall'])\n",
    "        f1.append(final_scores[value]['f1'])\n",
    "        accuracy.append((final_scores[value]['tp'] + final_scores[value]['tn'])/(final_scores[value]['tp'] + final_scores[value]['tn'] + final_scores[value]['fp'] + final_scores[value]['fn']))\n",
    "    df_result = pd.DataFrame(list(zip(tag_classes, precision, recall, f1, accuracy)), \n",
    "               columns =['class', 'precision', 'recall', 'f1', 'accuracy']) \n",
    "    return df_result\n",
    "# df_results = format_scores_df(['technology'], final_scores)\n",
    "df_results = format_scores_df(['culture'], final_scores)\n",
    "print_full_dataframe(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
