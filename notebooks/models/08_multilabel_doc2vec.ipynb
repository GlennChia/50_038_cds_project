{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. References\n",
    "\n",
    "Title: Multi-Label Text Classification Using Scikit-multilearn: a Case Study with StackOverflow Questions\n",
    "\n",
    "Link: https://medium.com/towards-artificial-intelligence/multi-label-text-classification-using-scikit-multilearn-case-study-with-stackoverflow-questions-768cb487ad12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/raw/\"\n",
    "# INPUT_FILE_NAME = 'cleaned.parquet'\n",
    "INPUT_FILE_NAME = 'cleaned_squashed.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>squash_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>[thank, chris, truly, great, honor, opportunit...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>alternative energy,culture,politics,science,cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>[term, invention, like, tell, tale, favorite, ...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>industrial design,alternative energy,invention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>[public, dewey, long, ago, observe, constitute...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>poverty,economics,investment,culture,politics,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>[want, start, say, houston, problem, enter, se...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>industrial design,invention,engineering,entrep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>[want, talk, background, idea, car, art, actua...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>industrial design,transportation,invention,des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker                              headline  \\\n",
       "0       Al Gore           Averting the climate crisis   \n",
       "1     Amy Smith         Simple designs to save a life   \n",
       "2  Ashraf Ghani         How to rebuild a broken state   \n",
       "3    Burt Rutan  The real future of space exploration   \n",
       "4  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                         description duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4  American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                tags  \\\n",
       "0  cars,alternative energy,culture,politics,scien...   \n",
       "1  MacArthur grant,simplicity,industrial design,a...   \n",
       "2  corruption,poverty,economics,investment,milita...   \n",
       "3  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript      WC  \\\n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "\n",
       "                                    clean_transcript  \\\n",
       "0  [thank, chris, truly, great, honor, opportunit...   \n",
       "1  [term, invention, like, tell, tale, favorite, ...   \n",
       "2  [public, dewey, long, ago, observe, constitute...   \n",
       "3  [want, start, say, houston, problem, enter, se...   \n",
       "4  [want, talk, background, idea, car, art, actua...   \n",
       "\n",
       "                             clean_transcript_string  \\\n",
       "0  thank chris truly great honor opportunity come...   \n",
       "1  term invention like tell tale favorite project...   \n",
       "2  public dewey long ago observe constitute discu...   \n",
       "3  want start say houston problem enter second ge...   \n",
       "4  want talk background idea car art actually mea...   \n",
       "\n",
       "                                         squash_tags  \n",
       "0  alternative energy,culture,politics,science,cl...  \n",
       "1  industrial design,alternative energy,invention...  \n",
       "2  poverty,economics,investment,culture,politics,...  \n",
       "3  industrial design,invention,engineering,entrep...  \n",
       "4  industrial design,transportation,invention,des...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "# df = df[:200]  # same as df.head(10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df[['headline', 'clean_transcript_string']]\n",
    "df_y = df[['squash_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "y = []\n",
    "for index, row in df_y.iterrows():\n",
    "    y.append(set(row['squash_tags'].split(',')))\n",
    "    \n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(encoded_y[0])\n",
    "print(len(encoded_y[0]))\n",
    "#print(mlb.inverse_transform(encoded_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, learning_rate=0.02, epochs=20, field=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "        self.field = field\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(clean_text(row[str(self.field)]).split(), [index]) for index, row in df_x.iterrows()]\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skl_utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(clean_text(row[str(self.field)]).split())\n",
    "                                     for index, row in df_x.iterrows()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_x, encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_x = train_x['clean_transcript_string']\n",
    "test_x = test_x['clean_transcript_string']\n",
    "\n",
    "#train_x = vectorizer.fit_transform(train_x['clean_transcript_string'])\n",
    "#test_x = vectorizer.transform(test_x['clean_transcript_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "train_x = vectorizer.fit_transform(train_x['clean_transcript_string'])\n",
    "test_x = vectorizer.transform(test_x['clean_transcript_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = FeatureUnion(transformer_list=[('title_doc2vec',Doc2VecTransformer(field='headline')),\n",
    "                                    ('body_doc2vec',Doc2VecTransformer(field='clean_transcript_string'))])\n",
    "binary_rel_model = BinaryRelevance(RandomForestClassifier(n_jobs=-1, n_estimators=10))\n",
    "\n",
    "multi_label_rf_br_model = Pipeline(steps=[\n",
    "                           ('feature_union', fu),\n",
    "                           ('binary_relevance', binary_rel_model)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def hamming_loss(multi_label_model_pipeline,train_x, train_y, test_x, test_y):\n",
    "    predictions_test_y = multi_label_model_pipeline.predict(test_x)\n",
    "    return metrics.hamming_loss(y_true=test_y, y_pred=predictions_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1789/1789 [00:00<00:00, 2273132.34it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2106572.11it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2928809.47it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 3546129.42it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2239883.54it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2045137.60it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2846589.47it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 3496556.32it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 1622402.13it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2999044.71it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2368563.72it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2121461.65it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2189556.42it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2150031.48it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 3355818.36it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2496210.86it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2920829.06it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 3663872.00it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2679860.66it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2793600.10it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 1795551.53it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2850915.60it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2632845.56it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2654265.96it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2408863.52it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 1778949.71it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2048487.54it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2976441.83it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2543596.56it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2409637.08it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2305964.92it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 1616112.40it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2137171.70it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2192115.06it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 1717859.40it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2199182.26it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2378323.25it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2413512.34it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2463430.68it/s]\n",
      "100%|██████████| 1789/1789 [00:00<00:00, 2517145.20it/s]\n"
     ]
    }
   ],
   "source": [
    "multi_label_rf_br_model.fit(train_x, train_y)\n",
    "print('Hamming loss for test data :', hamming_loss(multi_label_rf_br_model,train_x,train_y,test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_y = multi_label_rf_br_model.predict(test_x)\n",
    "print(predictions_test_y.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions_test_y.toarray()[-3])\n",
    "# print(encoded_y[154])\n",
    "# print(mlb.inverse_transform(predictions_test_y.toarray()))\n",
    "print(test_y[27])\n",
    "'''\n",
    "103  want year opportunity close conference incredi...  \n",
    "31   think podium bite scar chris ask tell structur...  \n",
    "41   music music end end hi sirena year old connect...  \n",
    "93   thank get story arrive plane long journey west...  \n",
    "152  go try view world problem opportunity face ask...  \n",
    "2    public dewey long ago observe constitute discu...  \n",
    "154                                              music  \n",
    "124  consider storyteller tell story usual way sens...  \n",
    "94 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = -1\n",
    "print(test_y[index])\n",
    "print(mlb.inverse_transform(test_y)[index])\n",
    "mlb.inverse_transform(predictions_test_y)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_transcript_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grid search params\n",
    "param_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "param_range_fl = [1.0, 0.5, 0.1]\n",
    "\n",
    "cv_grid_params = [] # Not implemened yet\n",
    "\n",
    "tfidf_grid_params = [] # Not implemented yet\n",
    "\n",
    "lr_grid_params = [{'clf__penalty': ['l1', 'l2'],\n",
    "                   'clf__C': param_range_fl,\n",
    "                   'clf__solver': ['liblinear']\n",
    "                  }] \n",
    "\n",
    "rf_grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "                   'clf__min_samples_leaf': param_range,\n",
    "                   'clf__max_depth': param_range,\n",
    "                   'clf__min_samples_split': param_range[1:],\n",
    "                   'clf__n_estimators': [10]\n",
    "                  }]\n",
    "\n",
    "svm_grid_params = [{'clf__kernel': ['linear', 'rbf'], \n",
    "                    'clf__C': param_range,\n",
    "                    'clf__gamma': ['auto'],\n",
    "                    'probability': [True]\n",
    "                   }]\n",
    "\n",
    "scoring = 'accuracy'\n",
    "njobs = -1\n",
    "\n",
    "# Instantiate vectorizer and desired models\n",
    "\n",
    "# OVR\n",
    "    ## logistic regression\n",
    "cv_lr_ovr = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "                     ]\n",
    "                    )\n",
    "gs_cv_lr_ovr = GridSearchCV(estimator=cv_lr_ovr,\n",
    "                            param_grid=lr_grid_params,\n",
    "                            scoring=scoring,\n",
    "                            cv=10) \n",
    "\n",
    "\n",
    "tfidf_lr_ovr = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "                       ]\n",
    "                      )\n",
    "gs_tfidf_lr_ovr = GridSearchCV(estimator=tfidf_lr_ovr,\n",
    "                               param_grid=lr_grid_params,\n",
    "                               scoring=scoring,\n",
    "                               cv=10) \n",
    "\n",
    "\n",
    "    ## random forest\n",
    "cv_rf_ovr = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('clf', OneVsRestClassifier(RandomForestClassifier()))\n",
    "                     ]\n",
    "                    )\n",
    "gs_cv_rf_ovr = GridSearchCV(estimator=cv_rf_ovr,\n",
    "                            param_grid=rf_grid_params,\n",
    "                            scoring=scoring,\n",
    "                            cv=10, \n",
    "                            n_jobs=njobs)\n",
    "\n",
    "\n",
    "tfidf_rf_ovr = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                        ('clf', OneVsRestClassifier(RandomForestClassifier()))\n",
    "                       ]\n",
    "                      )\n",
    "gs_tfidf_rf_ovr = GridSearchCV(estimator=tfidf_rf_ovr,\n",
    "                               param_grid=rf_grid_params,\n",
    "                               scoring=scoring,\n",
    "                               cv=10, \n",
    "                               n_jobs=njobs)\n",
    "\n",
    "\n",
    "    ## support vector classifier\n",
    "cv_svm_ovr = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                       ('clf', OneVsRestClassifier(SVC()))\n",
    "                      ]\n",
    "                     )\n",
    "gs_cv_svm_ovr = GridSearchCV(estimator=cv_svm_ovr,\n",
    "                             param_grid=svm_grid_params,\n",
    "                             scoring=scoring,\n",
    "                             cv=10,\n",
    "                             n_jobs=njobs)\n",
    "\n",
    "\n",
    "tfidf_svm_ovr = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                          ('clf', OneVsRestClassifier(SVC()))\n",
    "                         ]\n",
    "                        )\n",
    "gs_tfidf_svm_ovr = GridSearchCV(estimator=tfidf_svm_ovr,\n",
    "                                param_grid=svm_grid_params,\n",
    "                                scoring=scoring,\n",
    "                                cv=10,\n",
    "                                n_jobs=njobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [cv_lr_ovr, tfidf_lr_ovr, \n",
    "             cv_rf_ovr, tfidf_rf_ovr, \n",
    "             cv_svm_ovr, tfidf_svm_ovr]\n",
    "\n",
    "grids = [gs_cv_lr_ovr, gs_tfidf_lr_ovr,\n",
    "         gs_cv_rf_ovr, gs_tfidf_rf_ovr,\n",
    "         gs_cv_svm_ovr, gs_tfidf_svm_ovr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, clf in enumerate(pipelines):\n",
    "    y_pred_prob = clf.predict_proba(X_test)\n",
    "    t = 0.1 # threshold value\n",
    "    y_pred_new = (y_pred_prob >= t).astype(int)\n",
    "    scoring = f1_score(y_test, y_pred_new, average=\"micro\")\n",
    "    print('%s pipeline test accuracy: %.3f' % (idx, scoring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.inverse_transform(y_pred_new)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "for idx, clf in enumerate(pipelines):\n",
    "    if clf.score(X_test, y_test) > best_acc:\n",
    "        best_acc = clf.score(X_test, y_test)\n",
    "        best_pipe = clf\n",
    "print(f'Classifier with best accuracy: {best_pipe.named_steps} \\n with accuracy of {best_acc}')\n",
    "# joblib.dump(best_pipe, 'best_classifier.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performing model optimizations...')\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    y_pred = gs.predict(X_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set accuracy score for best params: %.3f ' % accuracy_score(y_test, y_pred))\n",
    "    # Track best (highest test accuracy) model\n",
    "    if accuracy_score(y_test, y_pred) > best_acc:\n",
    "        best_acc = accuracy_score(y_test, y_pred)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])\n",
    "\n",
    "# Save best grid search pipeline to file\n",
    "# dump_file = 'best_classifer_params.pkl'\n",
    "# joblib.dump(best_gs, dump_file, compress=1)\n",
    "# print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
