{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. References\n",
    "\n",
    "Title: Title Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK.\n",
    "\n",
    "Link: https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/processed/\"\n",
    "INPUT_FILE_NAME = 'cleaned_squashed15_final.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>sim_tags</th>\n",
       "      <th>squash15_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>[thank, chris, truly, great, honor, opportunit...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>cars,solar system,energy,culture,politics,scie...</td>\n",
       "      <td>culture,politics,science,global issues,technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>[term, invention, like, tell, tale, favorite, ...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>macarthur grant,simplicity,design,solar system...</td>\n",
       "      <td>design,global issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>[public, dewey, long, ago, observe, constitute...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>corruption,inequality,science,investment,war,c...</td>\n",
       "      <td>science,culture,politics,global issues,business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>[want, start, say, houston, problem, enter, se...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>flight,design,nasa,science,invention,entrepren...</td>\n",
       "      <td>design,science,business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>[want, talk, background, idea, car, art, actua...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>cars,design,transportation,invention,technolog...</td>\n",
       "      <td>design,technology,business,science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker                              headline  \\\n",
       "0       Al Gore           Averting the climate crisis   \n",
       "1     Amy Smith         Simple designs to save a life   \n",
       "2  Ashraf Ghani         How to rebuild a broken state   \n",
       "3    Burt Rutan  The real future of space exploration   \n",
       "4  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                         description duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4  American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                tags  \\\n",
       "0  cars,alternative energy,culture,politics,scien...   \n",
       "1  MacArthur grant,simplicity,industrial design,a...   \n",
       "2  corruption,poverty,economics,investment,milita...   \n",
       "3  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript      WC  \\\n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "\n",
       "                                    clean_transcript  \\\n",
       "0  [thank, chris, truly, great, honor, opportunit...   \n",
       "1  [term, invention, like, tell, tale, favorite, ...   \n",
       "2  [public, dewey, long, ago, observe, constitute...   \n",
       "3  [want, start, say, houston, problem, enter, se...   \n",
       "4  [want, talk, background, idea, car, art, actua...   \n",
       "\n",
       "                             clean_transcript_string  \\\n",
       "0  thank chris truly great honor opportunity come...   \n",
       "1  term invention like tell tale favorite project...   \n",
       "2  public dewey long ago observe constitute discu...   \n",
       "3  want start say houston problem enter second ge...   \n",
       "4  want talk background idea car art actually mea...   \n",
       "\n",
       "                                            sim_tags  \\\n",
       "0  cars,solar system,energy,culture,politics,scie...   \n",
       "1  macarthur grant,simplicity,design,solar system...   \n",
       "2  corruption,inequality,science,investment,war,c...   \n",
       "3  flight,design,nasa,science,invention,entrepren...   \n",
       "4  cars,design,transportation,invention,technolog...   \n",
       "\n",
       "                                       squash15_tags  \n",
       "0  culture,politics,science,global issues,technology  \n",
       "1                               design,global issues  \n",
       "2    science,culture,politics,global issues,business  \n",
       "3                            design,science,business  \n",
       "4                 design,technology,business,science  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2313 entries, 0 to 2312\n",
      "Data columns (total 11 columns):\n",
      "speaker                    2313 non-null object\n",
      "headline                   2313 non-null object\n",
      "description                2313 non-null object\n",
      "duration                   2313 non-null object\n",
      "tags                       2313 non-null object\n",
      "transcript                 2313 non-null object\n",
      "WC                         2313 non-null float64\n",
      "clean_transcript           2313 non-null object\n",
      "clean_transcript_string    2313 non-null object\n",
      "sim_tags                   2313 non-null object\n",
      "squash15_tags              2313 non-null object\n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 198.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:,:15].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2313 entries, 0 to 2312\n",
      "Data columns (total 11 columns):\n",
      "speaker                    2313 non-null object\n",
      "headline                   2313 non-null object\n",
      "description                2313 non-null object\n",
      "duration                   2313 non-null object\n",
      "tags                       2313 non-null object\n",
      "transcript                 2313 non-null object\n",
      "WC                         2313 non-null float64\n",
      "clean_transcript           2313 non-null object\n",
      "clean_transcript_string    2313 non-null object\n",
      "sim_tags                   2313 non-null object\n",
      "squash15_tags              2313 non-null object\n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 198.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['clean_transcript_string'])\n",
    "df = df.reset_index(drop=True)\n",
    "df.iloc[:,:15].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank chris truly great honor opportunity come stage twice extremely grateful blow away conference want thank nice comment night sincerely partly sob need position fly air force year shoe boot airplane tell quick story illustrate like true story bite true soon tipper leave sob white house drive home nashville little farm mile east nashville drive know sound like little thing look rearview mirror sudden hit motorcade hear phantom limb pain rent ford taurus dinnertime start look place eat get exit lebanon tennessee get exit shoneys restaurant lowcost family restaurant chain know go sit booth waitress come big commotion tipper take order go couple booth low voice strain hear say say yes vice president al gore wife tipper man say come long way kind series epiphany day continue totally true story get gv fly africa speech nigeria city lagos topic energy begin speech tell story happen day nashville tell pretty way share tipper drive shoneys lowcost family restaurant chain man say laugh give speech go airport fly home fell asleep plane middle night land azores island refuel wake open door go fresh air look man run runway wave piece paper yell washington washington think middle night middle atlantic world wrong washington remember bunch thing turn staff extremely upset wire service nigeria write story speech print city unite state america print monterey check story begin vice president al gore announce nigeria yesterday quote wife tipper open lowcost family restaurant name shoneys run yous soil david letterman jay leno start big white chef hat tipper say burger fry day late get nice long handwritten letter friend partner colleague clinton say congratulation new restaurant al like celebrate success life go talk information ecology think plan lifelong habit come ted maybe talk time chris anderson deal al gore want focus say like elaborate climate crisis want start couple go new image go recapitulate slide update slide time add new image learn time like beachcombing know time tide come shell day get new temperature record january unite state america historical average january degree month degree know want bad news environment kid recapitulation slide go new material want elaborate couple project yous contribution global warm business usual efficiency enduse electricity enduse energy lowhanging fruit efficiency conservation cost profit sign wrong negative positive investment pay effective deflect path car truck talk slideshow want perspective easy visible target concern global warm pollution come build car truck car truck significant low standard world address puzzle transportation efficiency important car truck renewables current level technological efficiency difference vinod john doerr lot people directly involve wedge go grow rapidly current projection show carbon capture sequestration cc stand likely killer app enable continue use fossil fuel way safe ok reduce emission home expenditure profitable insulation good design buy green electricity mention automobile buy hybrid use light rail figure option good important green consumer choice buy thing harsh effect harsh effect global climate crisis consider decision live carbonneutral life good brand love advice help way connect people easy think lot decision pretty easy mean reduce carbon dioxide emission range choice purchase acquire offset remainder completely reduce mean elaborate climatecrisisnet carbon calculator participant production convene active involvement lead software writer world arcane science carbon calculation construct consumerfriendly carbon calculator precisely calculate emission give option reduce time movie come update clickthrough purchase offset consider make business carbonneutral hard think integrate climate solution innovation technology entertainment design architecture community invest sustainably majora mention listen invest money manager compensate basis annual performance complain quarterly report ceo management time people pay judge go pay capital invest base shortterm return go shortterm decision lot say catalyst change teach learn talk movie movie version slideshow give night ago lot entertain come opportunity ensure lot people consider send somebody nashville pick personally go train people slideshow repurposed personal story obviously replace generic approach slide mean link go conduct course summer group people nominate different folk come en masse community country go update slideshow single week right cut edge work larry lessig process post tool limiteduse copyright young people remix way anybody idea ought stay arm length politics mean republican try convince democrat need republican bipartisan issue know group politically active democracy work way suppose work support idea cap carbon dioxide emission global warm pollution trade here long unite state world close close yous participation everybody board director people serve board director corporation close legal liability urge ceo maximum income reduce trade carbon emission avoid market work solve problem accomplish help mass persuasion campaign start spring change mind american people presently politician permission need modern country role logic reason long include mediate wealth power way repetition short hotbutton second second television ad buy lot ad let rebrand global warm suggest like climate crisis instead climate collapse good brand need help somebody say test face scientist tell combination opposable thumb neocortex viable combination true say night repeat political issue republican partisan influence democrat opportunity connect idea bring coherence thank appreciate'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_transcript_string'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 1. Numbers\n",
    "# 2. Apostrophe\n",
    "# 3. All punctuations\n",
    "# 4. Weird symbols\n",
    "# 5. Stop words\n",
    "# 6. lemmatization\n",
    "# '''\n",
    "\n",
    "# import string\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import ToktokTokenizer\n",
    "# from sklearn.feature_extraction import stop_words\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# sets=[stop_words.ENGLISH_STOP_WORDS]\n",
    "# sklearnStopWords = [list(x) for x in sets][0]\n",
    "# token=ToktokTokenizer()\n",
    "# lemma=WordNetLemmatizer()\n",
    "# stopWordList=stopwords.words('english')\n",
    "# stopWords = stopWordList + sklearnStopWords\n",
    "# stopWords = list(dict.fromkeys(stopWords))\n",
    "\n",
    "\n",
    "# def stopWordsRemove(text):\n",
    "#     wordList=[x.lower().strip() for x in token.tokenize(text)]\n",
    "#     removedList=[x + ' ' for x in wordList if not x in stopWords]\n",
    "#     text=''.join(removedList)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# def lemitizeWords(text):\n",
    "#     words=token.tokenize(text)\n",
    "#     listLemma=[]\n",
    "#     for w in words:\n",
    "#         x=lemma.lemmatize(w,'v')\n",
    "#         listLemma.append(x)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # There is a mispelt word that needs to be replaced\n",
    "# df['transcript'] = df['transcript'].str.replace('childrn','children')\n",
    "\n",
    "# df['transcript'] = df['transcript'].str.replace('\\r',' ')\n",
    "# df['transcript'] = df['transcript'].str.replace(\"\\'s\",\" is\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"\\'m\",\" am\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"\\'ll\",\" will\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"Can\\'t\",\"cannot\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"Sha\\'t\",\"shall not\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"Won\\'t\",\"would not\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"n\\'t\",\" not\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"\\'ve\",\" have\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"\\'re\",\" are\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"\\'d\",\" would\")\n",
    "# df['transcript'] = df['transcript'].str.replace(r\"\\(([^)]+)\\)\",\"\")\n",
    "# # Deal with Mr. and Dr.\n",
    "# df['transcript'] = df['transcript'].str.replace(\"mr. \",\"mr\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"Mr. \",\"mr\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"dr. \",\"dr\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"mrs. \",\"mrs\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"Mrs. \",\"mrs\")\n",
    "# df['transcript'] = df['transcript'].str.replace(\"Dr. \",\"dr\")\n",
    "\n",
    "# df['transcript'] = df['transcript'].str.replace(r'\\d+','')\n",
    "# df['transcript'] = df['transcript'].str.replace(r'<.*?>','')\n",
    "# for i in string.punctuation:\n",
    "#     if i == \"'\":\n",
    "#         df['transcript'] = df['transcript'].str.replace(i,'')\n",
    "#     else:\n",
    "#         df['transcript'] = df['transcript'].str.replace(i,' ')\n",
    "# df['transcript'] = df['transcript'].map(lambda com : stopWordsRemove(com))\n",
    "# df['transcript'] = df['transcript'].map(lambda com : lemitizeWords(com))\n",
    "# df['transcript'] = df['transcript'].str.replace('\\s+',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['transcript'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Compare Tags to get a single tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(tag_column):\n",
    "    tags = tag_column.str.replace(', ', ',').str.lower().str.strip()\n",
    "    joined_tags = tags.str.cat(sep=',').split(',')\n",
    "    all_tags_w_dup = pd.Series(joined_tags)\n",
    "\n",
    "    tag_counts = all_tags_w_dup.value_counts()\n",
    "    tag_list = list(tag_counts.index)\n",
    "    return tag_counts, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "science          1467\n",
       "culture          1155\n",
       "technology        787\n",
       "global issues     679\n",
       "design            477\n",
       "history           385\n",
       "business          349\n",
       "entertainment     285\n",
       "media             279\n",
       "biomechanics      220\n",
       "future            218\n",
       "biodiversity      218\n",
       "humanity          217\n",
       "politics          199\n",
       "communication     185\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_counts, tag_list = count_tags(df['squash15_tags'])\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>squash15_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>global issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>break ask people comment age debate comment un...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>music sound silence simon garfunkel hello voic...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kurt andersen like architect david hog limelig...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>point time come learn morning world expert gue...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>legitimate concern aid avian flu hear brillian...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>good morning fantastic past day secondly feel ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thank honor privilege spend day teenager today...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frank gehry listen scientist morning dr mullis...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>imagine spend seven year mit research laborato...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>like talk today big social trend come century ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>thrill conference devote inspire nature imagin...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>know figure exactly technology mean life spend...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>think suppose talk new book call blink snap ju...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>past couple day prepare speech nervous go stag...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>michael shermer director skeptic society publi...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>love tree lucky live near wonderful arboretum ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rich baraniuk like talk little bite today idea...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>write poem hear pretty know actress tell know ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lovegrove know lovegroves parent cousin know h...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>go specific example go cover end company call ...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>happy know talk tragedy people tragedy lot eas...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>time dread disease afflict child fact disease ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>know audience member howard howard thom mayne ...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ask come speak creation minute count minute th...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ask lot difference work typical pentagon longr...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>gayle king seat serena williams seat mom cheer...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>people like live year old yeah think hopeful e...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>let despise western democracy democracy trap f...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>chris anderson elon hey welcome ted great elon...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>get college education year investment grow poo...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>documentary jacques cousteau palme dor oscar a...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>go tell story song madrid night teacher friend...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>day husband paul diagnose stage iv lung cancer...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>chris anderson robert spend year think weird h...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>namaskar movie star year age use botox clean b...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>lee sedol lee sedol world great player friend ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>think important ocean daily life ocean cover t...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>clock morning pitch black outside yearold son ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>twoyearold daughter name naya mistake impressi...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>like start simple question poor poor decision ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>confession affair year old wish talk butterfly...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>story begin age world chess champion beat anat...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>vanessa garrison vanessa daughter annette daug...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>want share father teach condition permanent le...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>silicon valley obsess disruption day big disru...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>domino fall toy car ball roll music shall pas ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>zika fever new dread disease come adult relati...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>write famous tell know hero name marlon peters...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>sing water boy guitar strum hidin guitar strum...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>imagine smartphone miniaturize hook directly b...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>imagine walk even discover everybody room look...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>pay close attention easy attention pull differ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>happy pic take senior college right dance prac...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>sevenyearold grandson sleep hall wake lot morn...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>michael brown engineer innovator inventor insp...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2313 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                clean_transcript_string  squash15_tags\n",
       "0     thank chris truly great honor opportunity come...        science\n",
       "1     term invention like tell tale favorite project...  global issues\n",
       "2     public dewey long ago observe constitute discu...        science\n",
       "3     want start say houston problem enter second ge...        science\n",
       "4     want talk background idea car art actually mea...        science\n",
       "5     break ask people comment age debate comment un...        science\n",
       "6     music sound silence simon garfunkel hello voic...        science\n",
       "7     kurt andersen like architect david hog limelig...        culture\n",
       "8     point time come learn morning world expert gue...        science\n",
       "9     legitimate concern aid avian flu hear brillian...        science\n",
       "10    good morning fantastic past day secondly feel ...        science\n",
       "11    thank honor privilege spend day teenager today...        science\n",
       "12    frank gehry listen scientist morning dr mullis...        culture\n",
       "13    imagine spend seven year mit research laborato...        science\n",
       "14    like talk today big social trend come century ...        science\n",
       "15    thrill conference devote inspire nature imagin...        science\n",
       "16    know figure exactly technology mean life spend...        science\n",
       "17    think suppose talk new book call blink snap ju...        science\n",
       "18    past couple day prepare speech nervous go stag...        culture\n",
       "19    michael shermer director skeptic society publi...        science\n",
       "20    love tree lucky live near wonderful arboretum ...        science\n",
       "21    rich baraniuk like talk little bite today idea...        science\n",
       "22    write poem hear pretty know actress tell know ...        science\n",
       "23    lovegrove know lovegroves parent cousin know h...        science\n",
       "24    go specific example go cover end company call ...        culture\n",
       "25    happy know talk tragedy people tragedy lot eas...        science\n",
       "26    time dread disease afflict child fact disease ...        science\n",
       "27    know audience member howard howard thom mayne ...        culture\n",
       "28    ask come speak creation minute count minute th...        science\n",
       "29    ask lot difference work typical pentagon longr...        culture\n",
       "...                                                 ...            ...\n",
       "2283  gayle king seat serena williams seat mom cheer...        culture\n",
       "2284  people like live year old yeah think hopeful e...        science\n",
       "2285  let despise western democracy democracy trap f...        culture\n",
       "2286  chris anderson elon hey welcome ted great elon...        science\n",
       "2287  get college education year investment grow poo...        science\n",
       "2288  documentary jacques cousteau palme dor oscar a...        science\n",
       "2289  go tell story song madrid night teacher friend...        science\n",
       "2290  day husband paul diagnose stage iv lung cancer...        science\n",
       "2291  chris anderson robert spend year think weird h...        science\n",
       "2292  namaskar movie star year age use botox clean b...        science\n",
       "2293  lee sedol lee sedol world great player friend ...        science\n",
       "2294  think important ocean daily life ocean cover t...        science\n",
       "2295  clock morning pitch black outside yearold son ...        science\n",
       "2296  twoyearold daughter name naya mistake impressi...        science\n",
       "2297  like start simple question poor poor decision ...        science\n",
       "2298  confession affair year old wish talk butterfly...        science\n",
       "2299  story begin age world chess champion beat anat...        culture\n",
       "2300  vanessa garrison vanessa daughter annette daug...        science\n",
       "2301  want share father teach condition permanent le...        science\n",
       "2302  silicon valley obsess disruption day big disru...        culture\n",
       "2303  domino fall toy car ball roll music shall pas ...        science\n",
       "2304  zika fever new dread disease come adult relati...        science\n",
       "2305  write famous tell know hero name marlon peters...        culture\n",
       "2306  sing water boy guitar strum hidin guitar strum...        history\n",
       "2307  imagine smartphone miniaturize hook directly b...        science\n",
       "2308  imagine walk even discover everybody room look...        science\n",
       "2309  pay close attention easy attention pull differ...        science\n",
       "2310  happy pic take senior college right dance prac...        culture\n",
       "2311  sevenyearold grandson sleep hall wake lot morn...        science\n",
       "2312  michael brown engineer innovator inventor insp...        culture\n",
       "\n",
       "[2313 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tag_selection(df=df):\n",
    "    complete_transcripts_tags = []\n",
    "    for rows, value in df.iterrows():\n",
    "        indiv_tags = value['squash15_tags'].split(',')\n",
    "#         print(indiv_tags)\n",
    "        best_tag = ''\n",
    "        best_tag_count = 0\n",
    "        for i in range(len(indiv_tags)):\n",
    "            tag = indiv_tags[i]\n",
    "            tag_count = tag_counts[tag]\n",
    "            if tag_count > best_tag_count:\n",
    "                best_tag = tag\n",
    "                best_tag_count = tag_count\n",
    "#         longest_tag = max(indiv_tags, key=len)\n",
    "        indiv_transcript_tags = [value['clean_transcript_string'], best_tag]\n",
    "        complete_transcripts_tags.append(indiv_transcript_tags)\n",
    "    return pd.DataFrame(complete_transcripts_tags, columns=['clean_transcript_string', 'squash15_tags'])\n",
    "tag_cleaned = tag_selection()\n",
    "tag_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique tags\n",
    "tags_cleaned_up = tag_cleaned['tags'].unique()\n",
    "print(len(tags_cleaned_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ML part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the train test split \n",
    "transcript = tag_cleaned['transcript'].to_numpy()\n",
    "tags = tag_cleaned['tags'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     transcript, tags, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "# print(y_test)\n",
    "# print(predicted)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ls = ['sample text']\n",
    "sample_ls = np.array(sample_ls)\n",
    "predicted_new = text_clf.predict(sample_ls)\n",
    "print(predicted_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), \n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                                   alpha=1e-3, n_iter_no_change=5, random_state=42)),\n",
    "                        ])\n",
    "text_clf_svm = text_clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_train)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                      'tfidf__use_idf': (True, False), \n",
    "                      'clf-svm__alpha': (1e-2, 1e-3),}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "#dump(gs_clf_svm, 'gs_clf_svm.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ls = [\"As New Yorkers, we're often busy looking up at the development going on around us. We rarely stop to consider what lies beneath the city streets. And it's really hard to imagine that this small island village would one day become a forest of skyscrapers. Yet, as an urban archaeologist, that's exactly what I do. I consider landscapes, artifacts to tell the stories of the people who walked these streets before us. Because history is so much more than facts and figures. When people think of archaeology, they usually think of dusty old maps, far off lands, ancient civilizations. You don't think New York City and construction sites. Yet, that's where all the action happens and we're never sure exactly what we're going to find beneath the city streets. Like this wooden well ring which was the base for the construction of a water well. It provided us an opportunity to take a sample of the wood for tree-ring dating, and get a date to confirm the fact that we had indeed found a series of 18th-century structures beneath Fulton Street. Archaeology is about everyday people using everyday objects, like the child who may have played with this small toy, or the person who consumed the contents of this bottle. This bottle contained water imported from Germany and dates to 1790. Now okay, we know New Yorkers always had to go to great lengths to get fresh drinking water. Small island, you really couldn't drink the well water, it was to brackish. But the notion that New Yorkers were importing bottled water from Europe, more then two hundred years ago, is truly a testament to the fact that New York City is a cosmopolitan city, always has been, where you could get practically anything from anywhere. If you and I were to walk through City Hall Park, you might see an urban park and government offices. I see New York City's largest and most complex archaeological site. And it's significant not because it's City Hall, but because of the thousands of poor prisoners and British soldiers who lived and died here. Before it was City Hall Park, the area was known as The Common, and it was pretty far outside the city limits. In the 17th century, it was a place for public protests and execution. \"]\n",
    "sample_ls = np.array(sample_ls)\n",
    "predicted_new = gs_clf_svm.predict(sample_ls)\n",
    "print(predicted_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
