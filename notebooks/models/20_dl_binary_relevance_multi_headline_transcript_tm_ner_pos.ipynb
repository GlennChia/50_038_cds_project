{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links\n",
    "\n",
    "1. For the architecture https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada\n",
    "2. https://androidkt.com/multi-label-text-classification-in-tensorflow-keras/\n",
    "3. https://keras.io/preprocessing/sequence/\n",
    "4. https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/ ( Not really)\n",
    "5. For deep learning using word embeddings https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/processed/\"\n",
    "INPUT_FILE_NAME = 'final_squash15_with_pos_ner_tm.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>sim_tags</th>\n",
       "      <th>squash15_tags</th>\n",
       "      <th>pos_sequence</th>\n",
       "      <th>ner_sequence</th>\n",
       "      <th>tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>b'[\"thank\", \"chris\", \"truly\", \"great\", \"honor\"...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>cars,solar system,energy,culture,politics,scie...</td>\n",
       "      <td>culture,politics,science,global issues,technology</td>\n",
       "      <td>VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...</td>\n",
       "      <td>PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...</td>\n",
       "      <td>[0.04325945698517057, 0.0, 0.00142482934694180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>b'[\"term\", \"invention\", \"like\", \"tell\", \"tale\"...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>macarthur grant,simplicity,design,solar system...</td>\n",
       "      <td>design,global issues</td>\n",
       "      <td>NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...</td>\n",
       "      <td>GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...</td>\n",
       "      <td>[0.013287880838036227, 0.0, 0.0, 0.00511725094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>b'[\"public\", \"dewey\", \"long\", \"ago\", \"observe\"...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>corruption,inequality,science,investment,war,c...</td>\n",
       "      <td>science,culture,politics,global issues,business</td>\n",
       "      <td>ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...</td>\n",
       "      <td>DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...</td>\n",
       "      <td>[0.0, 0.006699599134802422, 0.0, 0.00564851883...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>b'[\"want\", \"start\", \"say\", \"houston\", \"problem...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>flight,design,nasa,science,invention,entrepren...</td>\n",
       "      <td>design,science,business</td>\n",
       "      <td>VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...</td>\n",
       "      <td>[0.040282108339079505, 0.03732895646484358, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>b'[\"want\", \"talk\", \"background\", \"idea\", \"car\"...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>cars,design,transportation,invention,technolog...</td>\n",
       "      <td>design,technology,business,science</td>\n",
       "      <td>VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...</td>\n",
       "      <td>PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...</td>\n",
       "      <td>[0.08049208168957463, 0.0, 0.0, 0.008031187136...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker                              headline  \\\n",
       "0       Al Gore           Averting the climate crisis   \n",
       "1     Amy Smith         Simple designs to save a life   \n",
       "2  Ashraf Ghani         How to rebuild a broken state   \n",
       "3    Burt Rutan  The real future of space exploration   \n",
       "4  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                         description duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4  American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                tags  \\\n",
       "0  cars,alternative energy,culture,politics,scien...   \n",
       "1  MacArthur grant,simplicity,industrial design,a...   \n",
       "2  corruption,poverty,economics,investment,milita...   \n",
       "3  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript      WC  \\\n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "\n",
       "                                    clean_transcript  \\\n",
       "0  b'[\"thank\", \"chris\", \"truly\", \"great\", \"honor\"...   \n",
       "1  b'[\"term\", \"invention\", \"like\", \"tell\", \"tale\"...   \n",
       "2  b'[\"public\", \"dewey\", \"long\", \"ago\", \"observe\"...   \n",
       "3  b'[\"want\", \"start\", \"say\", \"houston\", \"problem...   \n",
       "4  b'[\"want\", \"talk\", \"background\", \"idea\", \"car\"...   \n",
       "\n",
       "                             clean_transcript_string  \\\n",
       "0  thank chris truly great honor opportunity come...   \n",
       "1  term invention like tell tale favorite project...   \n",
       "2  public dewey long ago observe constitute discu...   \n",
       "3  want start say houston problem enter second ge...   \n",
       "4  want talk background idea car art actually mea...   \n",
       "\n",
       "                                            sim_tags  \\\n",
       "0  cars,solar system,energy,culture,politics,scie...   \n",
       "1  macarthur grant,simplicity,design,solar system...   \n",
       "2  corruption,inequality,science,investment,war,c...   \n",
       "3  flight,design,nasa,science,invention,entrepren...   \n",
       "4  cars,design,transportation,invention,technolog...   \n",
       "\n",
       "                                       squash15_tags  \\\n",
       "0  culture,politics,science,global issues,technology   \n",
       "1                               design,global issues   \n",
       "2    science,culture,politics,global issues,business   \n",
       "3                            design,science,business   \n",
       "4                 design,technology,business,science   \n",
       "\n",
       "                                        pos_sequence  \\\n",
       "0  VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...   \n",
       "1  NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...   \n",
       "2  ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...   \n",
       "3  VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...   \n",
       "4  VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...   \n",
       "\n",
       "                                        ner_sequence  \\\n",
       "0  PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...   \n",
       "1  GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...   \n",
       "2  DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...   \n",
       "3  GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...   \n",
       "4  PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...   \n",
       "\n",
       "                                                  tm  \n",
       "0  [0.04325945698517057, 0.0, 0.00142482934694180...  \n",
       "1  [0.013287880838036227, 0.0, 0.0, 0.00511725094...  \n",
       "2  [0.0, 0.006699599134802422, 0.0, 0.00564851883...  \n",
       "3  [0.040282108339079505, 0.03732895646484358, 0....  \n",
       "4  [0.08049208168957463, 0.0, 0.0, 0.008031187136...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2313 entries, 0 to 2312\n",
      "Data columns (total 14 columns):\n",
      "speaker                    2313 non-null object\n",
      "headline                   2313 non-null object\n",
      "description                2313 non-null object\n",
      "duration                   2313 non-null object\n",
      "tags                       2313 non-null object\n",
      "transcript                 2313 non-null object\n",
      "WC                         2313 non-null float64\n",
      "clean_transcript           2313 non-null object\n",
      "clean_transcript_string    2313 non-null object\n",
      "sim_tags                   2313 non-null object\n",
      "squash15_tags              2313 non-null object\n",
      "pos_sequence               2313 non-null object\n",
      "ner_sequence               2313 non-null object\n",
      "tm                         2313 non-null object\n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 253.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:,:14].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    squash15_tags  counts  no_count     ratio  overall_ratio\n",
      "0         science    1467       846  1.734043       0.634241\n",
      "1         culture    1155      1158  0.997409       0.499351\n",
      "2      technology     787      1526  0.515727       0.340251\n",
      "3   global issues     679      1634  0.415545       0.293558\n",
      "4          design     477      1836  0.259804       0.206226\n",
      "5         history     385      1928  0.199689       0.166450\n",
      "6        business     349      1964  0.177699       0.150886\n",
      "7   entertainment     285      2028  0.140533       0.123217\n",
      "8           media     279      2034  0.137168       0.120623\n",
      "9    biomechanics     220      2093  0.105112       0.095115\n",
      "10         future     218      2095  0.104057       0.094250\n",
      "11   biodiversity     218      2095  0.104057       0.094250\n",
      "12       humanity     217      2096  0.103531       0.093818\n",
      "13       politics     199      2114  0.094134       0.086035\n",
      "14  communication     185      2128  0.086936       0.079983\n"
     ]
    }
   ],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    \n",
    "def compute_tag_ratio(target_column, df=df):\n",
    "    tags = df[target_column].str.replace(', ',',').str.lower().str.strip()\n",
    "    split_tags = tags.str.split(',')\n",
    "    tag_counts_per_talk = split_tags.apply(len)\n",
    "\n",
    "    joined_tags = tags.str.cat(sep=',').split(',')\n",
    "    all_tags = pd.Series(joined_tags)\n",
    "\n",
    "    tag_counts = all_tags.value_counts().rename_axis(target_column).reset_index(name='counts')\n",
    "    tag_counts['no_count'] = len(df)-tag_counts['counts']\n",
    "    tag_counts['ratio'] = tag_counts['counts']/tag_counts['no_count']\n",
    "    tag_counts['overall_ratio'] = tag_counts['counts']/(tag_counts['no_count'] + tag_counts['counts'])\n",
    "    return tag_counts\n",
    "\n",
    "#print(compute_tag_ratio('squash3_tags', df))\n",
    "squashed_tag_counts = compute_tag_ratio('squash15_tags', df)\n",
    "print_full_dataframe(squashed_tag_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction via Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# y = []\n",
    "# for index, row in df.iterrows():\n",
    "#     y.append(set(row['squash3_tags'].split(',')))\n",
    "    \n",
    "# mlb = MultiLabelBinarizer()\n",
    "# encoded_y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoded_y[0])\n",
    "# print(len(encoded_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['culture', 'politics', 'science', 'global issues', 'technology', 'design', 'business', 'biomechanics', 'biodiversity', 'media', 'entertainment', 'history', 'future', 'communication', 'humanity']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "joined_tags = df['squash15_tags'].str.cat(sep=',').split(',')\n",
    "all_tags = pd.Series(joined_tags).str.strip().str.lower()\n",
    "all_tags = list(dict.fromkeys(all_tags))\n",
    "try:\n",
    "    all_tags.remove('')\n",
    "except:\n",
    "    pass\n",
    "print(all_tags)\n",
    "print(len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encode(df=df):\n",
    "    complete_transcripts_tags = []\n",
    "    for rows, value in df.iterrows():\n",
    "        one_hot_encoding = [0] * len(all_tags)\n",
    "        headline = [value['headline']]\n",
    "        transcript = [value['clean_transcript_string']]\n",
    "        pos_sequence = [value['pos_sequence']]\n",
    "        ner_sequence = [value['ner_sequence']]\n",
    "        tm = [value['tm']]\n",
    "        indiv_tags = value['squash15_tags'].split(',')\n",
    "        for tags in indiv_tags:\n",
    "            if tags == '':\n",
    "                continue\n",
    "            index = all_tags.index(tags.lower().lstrip(' '))\n",
    "            one_hot_encoding[index] = 1\n",
    "        indiv_transcript_tags = headline + transcript + pos_sequence + ner_sequence + tm +one_hot_encoding\n",
    "        complete_transcripts_tags.append(indiv_transcript_tags)\n",
    "    return pd.DataFrame(complete_transcripts_tags, columns=['headline', 'transcript', 'pos_sequence', 'ner_sequence','tm'] + all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>transcript</th>\n",
       "      <th>pos_sequence</th>\n",
       "      <th>ner_sequence</th>\n",
       "      <th>tm</th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>global issues</th>\n",
       "      <th>technology</th>\n",
       "      <th>design</th>\n",
       "      <th>business</th>\n",
       "      <th>biomechanics</th>\n",
       "      <th>biodiversity</th>\n",
       "      <th>media</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>history</th>\n",
       "      <th>future</th>\n",
       "      <th>communication</th>\n",
       "      <th>humanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...</td>\n",
       "      <td>PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...</td>\n",
       "      <td>[0.04325945698517057, 0.0, 0.00142482934694180...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...</td>\n",
       "      <td>GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...</td>\n",
       "      <td>[0.013287880838036227, 0.0, 0.0, 0.00511725094...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...</td>\n",
       "      <td>DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...</td>\n",
       "      <td>[0.0, 0.006699599134802422, 0.0, 0.00564851883...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...</td>\n",
       "      <td>[0.040282108339079505, 0.03732895646484358, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...</td>\n",
       "      <td>PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...</td>\n",
       "      <td>[0.08049208168957463, 0.0, 0.0, 0.008031187136...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sampling the ocean's DNA</td>\n",
       "      <td>break ask people comment age debate comment un...</td>\n",
       "      <td>VERB VERB NOUN VERB NOUN NOUN NOUN VERB NOUN A...</td>\n",
       "      <td>DATE DATE ORG DATE DATE PERSON ORG CARDINAL CA...</td>\n",
       "      <td>[0.0, 0.01122282724927712, 0.0, 0.163765591818...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>music sound silence simon garfunkel hello voic...</td>\n",
       "      <td>NOUN PROPN PROPN PROPN PROPN INTJ NOUN NOUN AD...</td>\n",
       "      <td>PERSON TIME TIME ORG PERSON FAC DATE DATE ORG ...</td>\n",
       "      <td>[0.062272408748748564, 0.0, 0.0243049615007748...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A memorial at Ground Zero</td>\n",
       "      <td>kurt andersen like architect david hog limelig...</td>\n",
       "      <td>PROPN PROPN SCONJ PROPN PROPN PROPN PROPN ADV ...</td>\n",
       "      <td>PERSON PERSON ORG PERSON DATE GPE PERSON PERSO...</td>\n",
       "      <td>[0.045631610155157765, 0.0, 0.0, 0.0, 0.004847...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>To invent is to give</td>\n",
       "      <td>point time come learn morning world expert gue...</td>\n",
       "      <td>NOUN NOUN VERB VERB NOUN NOUN NOUN VERB ADJ NO...</td>\n",
       "      <td>DATE DATE TIME PERSON DATE ORG CARDINAL GPE CA...</td>\n",
       "      <td>[0.041662618917564086, 0.0, 0.0, 0.0, 0.004215...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The killer American diet that's sweeping the p...</td>\n",
       "      <td>legitimate concern aid avian flu hear brillian...</td>\n",
       "      <td>ADJ NOUN NOUN ADJ NOUN VERB ADJ PROPN ADJ ADV ...</td>\n",
       "      <td>NORP TIME LOC LOC CARDINAL DATE DATE DATE NORP...</td>\n",
       "      <td>[0.003366184031329983, 0.0, 0.0007976442417315...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What separates us from chimpanzees?</td>\n",
       "      <td>good morning fantastic past day secondly feel ...</td>\n",
       "      <td>ADJ NOUN ADJ ADJ NOUN ADV VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>TIME GPE EVENT LOC GPE TIME DATE NORP LANGUAGE...</td>\n",
       "      <td>[0.050577383112491395, 0.0, 0.0, 0.00540960362...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Meet the future of cancer research</td>\n",
       "      <td>thank honor privilege spend day teenager today...</td>\n",
       "      <td>VERB PROPN NOUN VERB NOUN NOUN NOUN VERB NOUN ...</td>\n",
       "      <td>DATE DATE PERSON GPE ORG DATE EVENT GPE GPE GP...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.11278535017156091, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A master architect asks, Now what?</td>\n",
       "      <td>frank gehry listen scientist morning dr mullis...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROP...</td>\n",
       "      <td>PERSON TIME PERSON PERSON PERSON PERSON ORG OR...</td>\n",
       "      <td>[0.05140618959723995, 0.0, 0.0, 0.0, 0.0009926...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Software (as) art</td>\n",
       "      <td>imagine spend seven year mit research laborato...</td>\n",
       "      <td>VERB VERB NUM NOUN PROPN PROPN PROPN NOUN NOUN...</td>\n",
       "      <td>DATE ORG ORG PERSON TIME DATE GPE ORG ORDINAL ...</td>\n",
       "      <td>[0.027425263762819323, 0.0, 0.0103312675628116...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Why we love, why we cheat</td>\n",
       "      <td>like talk today big social trend come century ...</td>\n",
       "      <td>SCONJ NOUN NOUN ADJ ADJ NOUN VERB NOUN NOUN VE...</td>\n",
       "      <td>DATE DATE PERSON GPE PERSON PERSON PERSON PERS...</td>\n",
       "      <td>[0.04767230505808768, 0.0, 0.0, 0.0, 0.0151999...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Biomimicry's surprising lessons from nature's ...</td>\n",
       "      <td>thrill conference devote inspire nature imagin...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN NOUN PROPN NOUN PROPN ...</td>\n",
       "      <td>ORG FAC LOC ORG ORG ORG TIME DATE PERSON NORP ...</td>\n",
       "      <td>[0.027884164799976387, 0.006465436955228223, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How technology evolves</td>\n",
       "      <td>know figure exactly technology mean life spend...</td>\n",
       "      <td>VERB NOUN ADV NOUN ADJ NOUN VERB ADJ NOUN VERB...</td>\n",
       "      <td>DATE DATE DATE PERSON PERSON TIME PERSON PERSO...</td>\n",
       "      <td>[0.03079442054798949, 0.033779253241753265, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Choice, happiness and spaghetti sauce</td>\n",
       "      <td>think suppose talk new book call blink snap ju...</td>\n",
       "      <td>VERB VERB NOUN ADJ NOUN NOUN PROPN VERB NOUN N...</td>\n",
       "      <td>DATE NORP DATE PERSON PERSON PERSON ORG GPE DA...</td>\n",
       "      <td>[0.037593597923793624, 0.0029150189212423312, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Meet the founder of the blog revolution</td>\n",
       "      <td>past couple day prepare speech nervous go stag...</td>\n",
       "      <td>ADJ ADJ NOUN VERB NOUN ADJ VERB NOUN ADJ NOUN ...</td>\n",
       "      <td>PERSON PERSON PERSON PERSON DATE CARDINAL ORG ...</td>\n",
       "      <td>[0.08771567308527171, 0.0, 0.00844979525258797...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Why people believe weird things</td>\n",
       "      <td>michael shermer director skeptic society publi...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN NOUN ADJ NOUN PR...</td>\n",
       "      <td>PERSON ORG ORG ORG GPE PERSON PERSON ORG PERSO...</td>\n",
       "      <td>[0.04277426034635319, 0.04816268255472439, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fight injustice with raw video</td>\n",
       "      <td>love tree lucky live near wonderful arboretum ...</td>\n",
       "      <td>NOUN NOUN ADV ADV SCONJ ADJ PROPN PROPN ADV NO...</td>\n",
       "      <td>DATE ORDINAL PERSON PERSON PERSON PERSON ORG P...</td>\n",
       "      <td>[0.04036151740308654, 0.0, 0.03179935823326201...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The birth of the open-source learning revolution</td>\n",
       "      <td>rich baraniuk like talk little bite today idea...</td>\n",
       "      <td>ADJ PROPN SCONJ NOUN ADJ NOUN NOUN NOUN VERB A...</td>\n",
       "      <td>PERSON DATE DATE DATE PERSON DATE PERSON DATE ...</td>\n",
       "      <td>[0.02597837193451334, 0.0, 0.08252494437881452...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>If I controlled the Internet</td>\n",
       "      <td>write poem hear pretty know actress tell know ...</td>\n",
       "      <td>VERB NOUN VERB ADV VERB NOUN VERB VERB NOUN NO...</td>\n",
       "      <td>ORG ORG PERSON PERSON PERSON PERSON ORG DATE P...</td>\n",
       "      <td>[0.02287873745464833, 0.0, 0.01603027422617334...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Organic design, inspired by nature</td>\n",
       "      <td>lovegrove know lovegroves parent cousin know h...</td>\n",
       "      <td>VERB VERB NOUN NOUN NOUN VERB VERB VERB ADV AD...</td>\n",
       "      <td>DATE TIME ORG PERSON DATE PERSON PERSON ORG DA...</td>\n",
       "      <td>[0.05046730159068269, 0.008620141121501114, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How to get your ideas to spread</td>\n",
       "      <td>go specific example go cover end company call ...</td>\n",
       "      <td>VERB ADJ NOUN VERB VERB NOUN NOUN VERB NOUN AD...</td>\n",
       "      <td>PERSON PERSON DATE DATE DATE MONEY MONEY DATE ...</td>\n",
       "      <td>[0.06367106794151096, 0.0, 0.00889361872405093...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The freakonomics of crack dealing</td>\n",
       "      <td>happy know talk tragedy people tragedy lot eas...</td>\n",
       "      <td>ADJ VERB NOUN NOUN NOUN NOUN NOUN ADJ ADJ NOUN...</td>\n",
       "      <td>GPE DATE GPE GPE PERSON PERSON ORG PERSON PERS...</td>\n",
       "      <td>[0.039759217679600674, 0.0, 0.0, 0.00048959509...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Surprising stats about child carseats</td>\n",
       "      <td>time dread disease afflict child fact disease ...</td>\n",
       "      <td>NOUN PROPN NOUN ADJ NOUN NOUN NOUN VERB NOUN A...</td>\n",
       "      <td>DATE ORDINAL ORDINAL GPE DATE PERSON MONEY DAT...</td>\n",
       "      <td>[0.030542894542230838, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How architecture can connect us</td>\n",
       "      <td>know audience member howard howard thom mayne ...</td>\n",
       "      <td>VERB NOUN PROPN PROPN PROPN PROPN PROPN PROPN ...</td>\n",
       "      <td>PERSON PERSON PERSON CARDINAL CARDINAL TIME PE...</td>\n",
       "      <td>[0.00260903544742166, 0.018802235238984867, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Art with wire, sugar, chocolate and string</td>\n",
       "      <td>ask come speak creation minute count minute th...</td>\n",
       "      <td>VERB VERB VERB NOUN NOUN NOUN NOUN VERB VERB P...</td>\n",
       "      <td>GPE PERSON GPE GPE DATE PERSON GPE GPE PERSON ...</td>\n",
       "      <td>[0.07840706808120378, 0.012627683498918756, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Let's rethink America's military strategy</td>\n",
       "      <td>ask lot difference work typical pentagon longr...</td>\n",
       "      <td>VERB NOUN NOUN NOUN ADJ PROPN VERB ADJ NOUN NO...</td>\n",
       "      <td>ORG DATE DATE ORG ORG ORG GPE GPE ORG ORDINAL ...</td>\n",
       "      <td>[0.030947785944380562, 0.008915481859539746, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>On tennis, love and motherhood</td>\n",
       "      <td>gayle king seat serena williams seat mom cheer...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN NOUN NOUN ...</td>\n",
       "      <td>ORG PERSON PERSON ORG TIME DATE DATE NORP NORP...</td>\n",
       "      <td>[0.05508546301919964, 0.0, 0.00227033438246327...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>What you can do to prevent Alzheimer's</td>\n",
       "      <td>people like live year old yeah think hopeful e...</td>\n",
       "      <td>NOUN SCONJ ADJ NOUN ADJ INTJ VERB ADJ NOUN VER...</td>\n",
       "      <td>DATE ORG PERSON PERSON PERSON NORP DATE DATE T...</td>\n",
       "      <td>[0.015113367818834416, 0.0, 0.0, 0.02859407621...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>How to exploit democracy</td>\n",
       "      <td>let despise western democracy democracy trap f...</td>\n",
       "      <td>VERB VERB ADJ NOUN NOUN NOUN ADJ NOUN NOUN PRO...</td>\n",
       "      <td>NORP PERSON NORP NORP DATE GPE PERSON NORP NOR...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>The future we're building -- and boring</td>\n",
       "      <td>chris anderson elon hey welcome ted great elon...</td>\n",
       "      <td>PROPN PROPN PROPN INTJ VERB PROPN PROPN PROPN ...</td>\n",
       "      <td>PERSON TIME TIME TIME TIME MONEY MONEY CARDINA...</td>\n",
       "      <td>[0.023778624544851402, 0.007829825788402496, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>A summer school kids actually want to attend</td>\n",
       "      <td>get college education year investment grow poo...</td>\n",
       "      <td>AUX PROPN PROPN NOUN NOUN VERB ADJ NOUN VERB A...</td>\n",
       "      <td>ORG DATE DATE CARDINAL GPE GPE ORDINAL PERSON ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.009131007449666096, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>How human noise affects ocean habitats</td>\n",
       "      <td>documentary jacques cousteau palme dor oscar a...</td>\n",
       "      <td>ADJ PROPN PROPN PROPN PROPN PROPN PROPN NOUN P...</td>\n",
       "      <td>PERSON LOC DATE LOC PERSON NORP DATE LOC LOC D...</td>\n",
       "      <td>[0.0, 0.0, 0.012520814310613023, 0.0, 0.0, 0.1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>Poetry, music and identity</td>\n",
       "      <td>go tell story song madrid night teacher friend...</td>\n",
       "      <td>VERB VERB NOUN PROPN PROPN NOUN NOUN PROPN PRO...</td>\n",
       "      <td>GPE TIME PERSON PERSON NORP NORP PERSON GPE OR...</td>\n",
       "      <td>[0.03468020816383595, 0.004695919146035526, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>What makes life worth living in the face of death</td>\n",
       "      <td>day husband paul diagnose stage iv lung cancer...</td>\n",
       "      <td>NOUN NOUN PROPN PROPN PROPN PROPN NOUN NOUN PR...</td>\n",
       "      <td>PERSON PERSON PERSON ORG PERSON NORP PERSON GP...</td>\n",
       "      <td>[0.05339526199505615, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>The biology of our best and worst selves</td>\n",
       "      <td>chris anderson robert spend year think weird h...</td>\n",
       "      <td>PROPN PROPN PROPN VERB NOUN VERB ADJ ADJ NOUN ...</td>\n",
       "      <td>PERSON DATE PERSON PERSON ORDINAL PERSON PERSO...</td>\n",
       "      <td>[0.017161374254025186, 0.0, 0.0015174001197554...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>Thoughts on humanity, fame and love</td>\n",
       "      <td>namaskar movie star year age use botox clean b...</td>\n",
       "      <td>PROPN PROPN PROPN NOUN NOUN NOUN NOUN ADJ NOUN...</td>\n",
       "      <td>PERSON DATE PERSON CARDINAL GPE PERSON GPE GPE...</td>\n",
       "      <td>[0.06401594840887878, 0.016237498143283166, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>3 principles for creating safer AI</td>\n",
       "      <td>lee sedol lee sedol world great player friend ...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROP...</td>\n",
       "      <td>PERSON LOC ORG ORG PERSON PERSON PERSON PERSON...</td>\n",
       "      <td>[0.03266345974648634, 0.0, 0.0, 0.0, 0.0028504...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>How pollution is changing the ocean's chemistry</td>\n",
       "      <td>think important ocean daily life ocean cover t...</td>\n",
       "      <td>VERB ADJ PROPN PROPN PROPN PROPN NOUN DET NOUN...</td>\n",
       "      <td>CARDINAL DATE DATE DATE GPE ORDINAL EVENT DATE...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.15660827815235764,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>Why school should start later for teens</td>\n",
       "      <td>clock morning pitch black outside yearold son ...</td>\n",
       "      <td>NOUN NOUN NOUN ADJ ADP PROPN NOUN VERB ADJ NOU...</td>\n",
       "      <td>TIME PERSON PERSON PERSON PERSON NORP TIME GPE...</td>\n",
       "      <td>[0.015927306932493678, 0.0, 0.0005219988142567...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>A climate solution where all sides can win</td>\n",
       "      <td>twoyearold daughter name naya mistake impressi...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...</td>\n",
       "      <td>GPE PERSON NORP GPE GPE GPE ORG NORP PERSON NO...</td>\n",
       "      <td>[0.0, 0.0, 0.0011856008341072713, 0.0, 0.04285...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>Poverty isn't a lack of character; it's a lack...</td>\n",
       "      <td>like start simple question poor poor decision ...</td>\n",
       "      <td>SCONJ VERB ADJ NOUN ADJ ADJ NOUN VERB ADJ NOUN...</td>\n",
       "      <td>NORP PERSON PERSON DATE NORP GPE DATE DATE DAT...</td>\n",
       "      <td>[0.035301185398910626, 0.004142997863142574, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>Why I speak up about living with epilepsy</td>\n",
       "      <td>confession affair year old wish talk butterfly...</td>\n",
       "      <td>PROPN PROPN NOUN ADJ ADJ NOUN NOUN NOUN NOUN N...</td>\n",
       "      <td>ORG DATE DATE GPE PERSON NORP NORP NORP GPE OR...</td>\n",
       "      <td>[0.03825146812357643, 0.0002520530721952876, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>Don't fear intelligent machines. Work with them</td>\n",
       "      <td>story begin age world chess champion beat anat...</td>\n",
       "      <td>NOUN VERB NOUN NOUN NOUN NOUN PROPN PROPN PROP...</td>\n",
       "      <td>PERSON DATE GPE PERSON NORP PERSON PERSON PERS...</td>\n",
       "      <td>[0.0, 0.0, 0.0010252769472308598, 0.0, 0.0, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>Walking as a revolutionary act of self-care</td>\n",
       "      <td>vanessa garrison vanessa daughter annette daug...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROP...</td>\n",
       "      <td>PERSON PERSON PERSON PERSON PERSON ORG PERSON ...</td>\n",
       "      <td>[0.01606418830777042, 0.005388137902250922, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>No one should die because they live too far fr...</td>\n",
       "      <td>want share father teach condition permanent le...</td>\n",
       "      <td>VERB NOUN NOUN VERB NOUN ADJ NOUN NOUN VERB AD...</td>\n",
       "      <td>GPE ORG DATE EVENT ORG DATE ORG PERSON CARDINA...</td>\n",
       "      <td>[0.0023661319331237784, 0.0, 0.000636940282449...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>This is what democracy looks like</td>\n",
       "      <td>silicon valley obsess disruption day big disru...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN ADJ NOUN VERB PR...</td>\n",
       "      <td>LOC DATE LOC GPE GPE GPE CARDINAL NORP TIME GP...</td>\n",
       "      <td>[0.020778226494857336, 0.002293181170735768, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>How to find a wonderful idea</td>\n",
       "      <td>domino fall toy car ball roll music shall pas ...</td>\n",
       "      <td>DET PROPN PROPN NOUN NOUN NOUN NOUN VERB NOUN ...</td>\n",
       "      <td>TIME TIME TIME TIME TIME TIME TIME TIME TIME T...</td>\n",
       "      <td>[0.07339320400975863, 0.004287185808484255, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>A secret weapon against Zika and other mosquit...</td>\n",
       "      <td>zika fever new dread disease come adult relati...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN VERB NOUN ADV AD...</td>\n",
       "      <td>GPE PERSON GPE GPE DATE PERSON LOC GPE ORG PER...</td>\n",
       "      <td>[0.00022790152811686269, 0.0, 4.81912535361483...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>Am I not human? A call for criminal justice re...</td>\n",
       "      <td>write famous tell know hero name marlon peters...</td>\n",
       "      <td>VERB ADJ NOUN NOUN NOUN PROPN PROPN PROPN PROP...</td>\n",
       "      <td>PERSON GPE NORP GPE ORG PERSON ORG GPE PERSON ...</td>\n",
       "      <td>[0.030062572867008263, 0.0002639586848992885, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>Songs that bring history to life</td>\n",
       "      <td>sing water boy guitar strum hidin guitar strum...</td>\n",
       "      <td>VERB NOUN NOUN PROPN PROPN PROPN PROPN PROPN V...</td>\n",
       "      <td>ORG GPE PERSON PERSON PERSON FAC ORG FAC FAC L...</td>\n",
       "      <td>[0.03400419777173537, 0.0, 0.14868577353664802...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>How to see past your own perspective and find ...</td>\n",
       "      <td>imagine smartphone miniaturize hook directly b...</td>\n",
       "      <td>VERB NOUN NOUN NOUN ADV NOUN NOUN NOUN NOUN AD...</td>\n",
       "      <td>DATE NORP NORP PERSON PERSON NORP PERSON PERSO...</td>\n",
       "      <td>[0.06112071611956173, 0.002545963587318732, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>Why glass towers are bad for city life -- and ...</td>\n",
       "      <td>imagine walk even discover everybody room look...</td>\n",
       "      <td>VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...</td>\n",
       "      <td>ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...</td>\n",
       "      <td>[0.0, 0.005742817940048278, 0.0013400504020271...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>What happens in your brain when you pay attent...</td>\n",
       "      <td>pay close attention easy attention pull differ...</td>\n",
       "      <td>VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...</td>\n",
       "      <td>ORDINAL PERSON PRODUCT DATE DATE</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>Why you should define your fears instead of yo...</td>\n",
       "      <td>happy pic take senior college right dance prac...</td>\n",
       "      <td>ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...</td>\n",
       "      <td>DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...</td>\n",
       "      <td>[0.05075222279006517, 0.0012024359432974202, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>12 truths I learned from life and writing</td>\n",
       "      <td>sevenyearold grandson sleep hall wake lot morn...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...</td>\n",
       "      <td>PERSON PERSON PERSON PERSON PERSON DATE CARDIN...</td>\n",
       "      <td>[0.11474111662254229, 0.0031870816685010414, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>How I built a jet suit</td>\n",
       "      <td>michael brown engineer innovator inventor insp...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...</td>\n",
       "      <td>PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...</td>\n",
       "      <td>[0.05883474744248605, 0.002542825028156852, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2313 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headline  \\\n",
       "0                           Averting the climate crisis   \n",
       "1                         Simple designs to save a life   \n",
       "2                         How to rebuild a broken state   \n",
       "3                  The real future of space exploration   \n",
       "4                              Great cars are great art   \n",
       "5                              Sampling the ocean's DNA   \n",
       "6                                      Simplicity sells   \n",
       "7                             A memorial at Ground Zero   \n",
       "8                                  To invent is to give   \n",
       "9     The killer American diet that's sweeping the p...   \n",
       "10                  What separates us from chimpanzees?   \n",
       "11                   Meet the future of cancer research   \n",
       "12                   A master architect asks, Now what?   \n",
       "13                                    Software (as) art   \n",
       "14                            Why we love, why we cheat   \n",
       "15    Biomimicry's surprising lessons from nature's ...   \n",
       "16                               How technology evolves   \n",
       "17                Choice, happiness and spaghetti sauce   \n",
       "18              Meet the founder of the blog revolution   \n",
       "19                      Why people believe weird things   \n",
       "20                       Fight injustice with raw video   \n",
       "21     The birth of the open-source learning revolution   \n",
       "22                         If I controlled the Internet   \n",
       "23                   Organic design, inspired by nature   \n",
       "24                      How to get your ideas to spread   \n",
       "25                    The freakonomics of crack dealing   \n",
       "26                Surprising stats about child carseats   \n",
       "27                      How architecture can connect us   \n",
       "28           Art with wire, sugar, chocolate and string   \n",
       "29            Let's rethink America's military strategy   \n",
       "...                                                 ...   \n",
       "2283                     On tennis, love and motherhood   \n",
       "2284             What you can do to prevent Alzheimer's   \n",
       "2285                           How to exploit democracy   \n",
       "2286            The future we're building -- and boring   \n",
       "2287       A summer school kids actually want to attend   \n",
       "2288             How human noise affects ocean habitats   \n",
       "2289                         Poetry, music and identity   \n",
       "2290  What makes life worth living in the face of death   \n",
       "2291           The biology of our best and worst selves   \n",
       "2292                Thoughts on humanity, fame and love   \n",
       "2293                 3 principles for creating safer AI   \n",
       "2294    How pollution is changing the ocean's chemistry   \n",
       "2295            Why school should start later for teens   \n",
       "2296         A climate solution where all sides can win   \n",
       "2297  Poverty isn't a lack of character; it's a lack...   \n",
       "2298          Why I speak up about living with epilepsy   \n",
       "2299    Don't fear intelligent machines. Work with them   \n",
       "2300        Walking as a revolutionary act of self-care   \n",
       "2301  No one should die because they live too far fr...   \n",
       "2302                  This is what democracy looks like   \n",
       "2303                       How to find a wonderful idea   \n",
       "2304  A secret weapon against Zika and other mosquit...   \n",
       "2305  Am I not human? A call for criminal justice re...   \n",
       "2306                   Songs that bring history to life   \n",
       "2307  How to see past your own perspective and find ...   \n",
       "2308  Why glass towers are bad for city life -- and ...   \n",
       "2309  What happens in your brain when you pay attent...   \n",
       "2310  Why you should define your fears instead of yo...   \n",
       "2311          12 truths I learned from life and writing   \n",
       "2312                             How I built a jet suit   \n",
       "\n",
       "                                             transcript  \\\n",
       "0     thank chris truly great honor opportunity come...   \n",
       "1     term invention like tell tale favorite project...   \n",
       "2     public dewey long ago observe constitute discu...   \n",
       "3     want start say houston problem enter second ge...   \n",
       "4     want talk background idea car art actually mea...   \n",
       "5     break ask people comment age debate comment un...   \n",
       "6     music sound silence simon garfunkel hello voic...   \n",
       "7     kurt andersen like architect david hog limelig...   \n",
       "8     point time come learn morning world expert gue...   \n",
       "9     legitimate concern aid avian flu hear brillian...   \n",
       "10    good morning fantastic past day secondly feel ...   \n",
       "11    thank honor privilege spend day teenager today...   \n",
       "12    frank gehry listen scientist morning dr mullis...   \n",
       "13    imagine spend seven year mit research laborato...   \n",
       "14    like talk today big social trend come century ...   \n",
       "15    thrill conference devote inspire nature imagin...   \n",
       "16    know figure exactly technology mean life spend...   \n",
       "17    think suppose talk new book call blink snap ju...   \n",
       "18    past couple day prepare speech nervous go stag...   \n",
       "19    michael shermer director skeptic society publi...   \n",
       "20    love tree lucky live near wonderful arboretum ...   \n",
       "21    rich baraniuk like talk little bite today idea...   \n",
       "22    write poem hear pretty know actress tell know ...   \n",
       "23    lovegrove know lovegroves parent cousin know h...   \n",
       "24    go specific example go cover end company call ...   \n",
       "25    happy know talk tragedy people tragedy lot eas...   \n",
       "26    time dread disease afflict child fact disease ...   \n",
       "27    know audience member howard howard thom mayne ...   \n",
       "28    ask come speak creation minute count minute th...   \n",
       "29    ask lot difference work typical pentagon longr...   \n",
       "...                                                 ...   \n",
       "2283  gayle king seat serena williams seat mom cheer...   \n",
       "2284  people like live year old yeah think hopeful e...   \n",
       "2285  let despise western democracy democracy trap f...   \n",
       "2286  chris anderson elon hey welcome ted great elon...   \n",
       "2287  get college education year investment grow poo...   \n",
       "2288  documentary jacques cousteau palme dor oscar a...   \n",
       "2289  go tell story song madrid night teacher friend...   \n",
       "2290  day husband paul diagnose stage iv lung cancer...   \n",
       "2291  chris anderson robert spend year think weird h...   \n",
       "2292  namaskar movie star year age use botox clean b...   \n",
       "2293  lee sedol lee sedol world great player friend ...   \n",
       "2294  think important ocean daily life ocean cover t...   \n",
       "2295  clock morning pitch black outside yearold son ...   \n",
       "2296  twoyearold daughter name naya mistake impressi...   \n",
       "2297  like start simple question poor poor decision ...   \n",
       "2298  confession affair year old wish talk butterfly...   \n",
       "2299  story begin age world chess champion beat anat...   \n",
       "2300  vanessa garrison vanessa daughter annette daug...   \n",
       "2301  want share father teach condition permanent le...   \n",
       "2302  silicon valley obsess disruption day big disru...   \n",
       "2303  domino fall toy car ball roll music shall pas ...   \n",
       "2304  zika fever new dread disease come adult relati...   \n",
       "2305  write famous tell know hero name marlon peters...   \n",
       "2306  sing water boy guitar strum hidin guitar strum...   \n",
       "2307  imagine smartphone miniaturize hook directly b...   \n",
       "2308  imagine walk even discover everybody room look...   \n",
       "2309  pay close attention easy attention pull differ...   \n",
       "2310  happy pic take senior college right dance prac...   \n",
       "2311  sevenyearold grandson sleep hall wake lot morn...   \n",
       "2312  michael brown engineer innovator inventor insp...   \n",
       "\n",
       "                                           pos_sequence  \\\n",
       "0     VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...   \n",
       "1     NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...   \n",
       "2     ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...   \n",
       "3     VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...   \n",
       "4     VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...   \n",
       "5     VERB VERB NOUN VERB NOUN NOUN NOUN VERB NOUN A...   \n",
       "6     NOUN PROPN PROPN PROPN PROPN INTJ NOUN NOUN AD...   \n",
       "7     PROPN PROPN SCONJ PROPN PROPN PROPN PROPN ADV ...   \n",
       "8     NOUN NOUN VERB VERB NOUN NOUN NOUN VERB ADJ NO...   \n",
       "9     ADJ NOUN NOUN ADJ NOUN VERB ADJ PROPN ADJ ADV ...   \n",
       "10    ADJ NOUN ADJ ADJ NOUN ADV VERB ADJ NOUN NOUN N...   \n",
       "11    VERB PROPN NOUN VERB NOUN NOUN NOUN VERB NOUN ...   \n",
       "12    PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROP...   \n",
       "13    VERB VERB NUM NOUN PROPN PROPN PROPN NOUN NOUN...   \n",
       "14    SCONJ NOUN NOUN ADJ ADJ NOUN VERB NOUN NOUN VE...   \n",
       "15    PROPN PROPN PROPN PROPN NOUN PROPN NOUN PROPN ...   \n",
       "16    VERB NOUN ADV NOUN ADJ NOUN VERB ADJ NOUN VERB...   \n",
       "17    VERB VERB NOUN ADJ NOUN NOUN PROPN VERB NOUN N...   \n",
       "18    ADJ ADJ NOUN VERB NOUN ADJ VERB NOUN ADJ NOUN ...   \n",
       "19    PROPN PROPN PROPN PROPN PROPN NOUN ADJ NOUN PR...   \n",
       "20    NOUN NOUN ADV ADV SCONJ ADJ PROPN PROPN ADV NO...   \n",
       "21    ADJ PROPN SCONJ NOUN ADJ NOUN NOUN NOUN VERB A...   \n",
       "22    VERB NOUN VERB ADV VERB NOUN VERB VERB NOUN NO...   \n",
       "23    VERB VERB NOUN NOUN NOUN VERB VERB VERB ADV AD...   \n",
       "24    VERB ADJ NOUN VERB VERB NOUN NOUN VERB NOUN AD...   \n",
       "25    ADJ VERB NOUN NOUN NOUN NOUN NOUN ADJ ADJ NOUN...   \n",
       "26    NOUN PROPN NOUN ADJ NOUN NOUN NOUN VERB NOUN A...   \n",
       "27    VERB NOUN PROPN PROPN PROPN PROPN PROPN PROPN ...   \n",
       "28    VERB VERB VERB NOUN NOUN NOUN NOUN VERB VERB P...   \n",
       "29    VERB NOUN NOUN NOUN ADJ PROPN VERB ADJ NOUN NO...   \n",
       "...                                                 ...   \n",
       "2283  PROPN PROPN PROPN PROPN PROPN PROPN NOUN NOUN ...   \n",
       "2284  NOUN SCONJ ADJ NOUN ADJ INTJ VERB ADJ NOUN VER...   \n",
       "2285  VERB VERB ADJ NOUN NOUN NOUN ADJ NOUN NOUN PRO...   \n",
       "2286  PROPN PROPN PROPN INTJ VERB PROPN PROPN PROPN ...   \n",
       "2287  AUX PROPN PROPN NOUN NOUN VERB ADJ NOUN VERB A...   \n",
       "2288  ADJ PROPN PROPN PROPN PROPN PROPN PROPN NOUN P...   \n",
       "2289  VERB VERB NOUN PROPN PROPN NOUN NOUN PROPN PRO...   \n",
       "2290  NOUN NOUN PROPN PROPN PROPN PROPN NOUN NOUN PR...   \n",
       "2291  PROPN PROPN PROPN VERB NOUN VERB ADJ ADJ NOUN ...   \n",
       "2292  PROPN PROPN PROPN NOUN NOUN NOUN NOUN ADJ NOUN...   \n",
       "2293  PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROP...   \n",
       "2294  VERB ADJ PROPN PROPN PROPN PROPN NOUN DET NOUN...   \n",
       "2295  NOUN NOUN NOUN ADJ ADP PROPN NOUN VERB ADJ NOU...   \n",
       "2296  PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...   \n",
       "2297  SCONJ VERB ADJ NOUN ADJ ADJ NOUN VERB ADJ NOUN...   \n",
       "2298  PROPN PROPN NOUN ADJ ADJ NOUN NOUN NOUN NOUN N...   \n",
       "2299  NOUN VERB NOUN NOUN NOUN NOUN PROPN PROPN PROP...   \n",
       "2300  PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROP...   \n",
       "2301  VERB NOUN NOUN VERB NOUN ADJ NOUN NOUN VERB AD...   \n",
       "2302  PROPN PROPN PROPN PROPN PROPN ADJ NOUN VERB PR...   \n",
       "2303  DET PROPN PROPN NOUN NOUN NOUN NOUN VERB NOUN ...   \n",
       "2304  PROPN PROPN PROPN PROPN PROPN VERB NOUN ADV AD...   \n",
       "2305  VERB ADJ NOUN NOUN NOUN PROPN PROPN PROPN PROP...   \n",
       "2306  VERB NOUN NOUN PROPN PROPN PROPN PROPN PROPN V...   \n",
       "2307  VERB NOUN NOUN NOUN ADV NOUN NOUN NOUN NOUN AD...   \n",
       "2308  VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...   \n",
       "2309  VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...   \n",
       "2310  ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...   \n",
       "2311  PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...   \n",
       "2312  PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...   \n",
       "\n",
       "                                           ner_sequence  \\\n",
       "0     PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...   \n",
       "1     GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...   \n",
       "2     DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...   \n",
       "3     GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...   \n",
       "4     PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...   \n",
       "5     DATE DATE ORG DATE DATE PERSON ORG CARDINAL CA...   \n",
       "6     PERSON TIME TIME ORG PERSON FAC DATE DATE ORG ...   \n",
       "7     PERSON PERSON ORG PERSON DATE GPE PERSON PERSO...   \n",
       "8     DATE DATE TIME PERSON DATE ORG CARDINAL GPE CA...   \n",
       "9     NORP TIME LOC LOC CARDINAL DATE DATE DATE NORP...   \n",
       "10    TIME GPE EVENT LOC GPE TIME DATE NORP LANGUAGE...   \n",
       "11    DATE DATE PERSON GPE ORG DATE EVENT GPE GPE GP...   \n",
       "12    PERSON TIME PERSON PERSON PERSON PERSON ORG OR...   \n",
       "13    DATE ORG ORG PERSON TIME DATE GPE ORG ORDINAL ...   \n",
       "14    DATE DATE PERSON GPE PERSON PERSON PERSON PERS...   \n",
       "15    ORG FAC LOC ORG ORG ORG TIME DATE PERSON NORP ...   \n",
       "16    DATE DATE DATE PERSON PERSON TIME PERSON PERSO...   \n",
       "17    DATE NORP DATE PERSON PERSON PERSON ORG GPE DA...   \n",
       "18    PERSON PERSON PERSON PERSON DATE CARDINAL ORG ...   \n",
       "19    PERSON ORG ORG ORG GPE PERSON PERSON ORG PERSO...   \n",
       "20    DATE ORDINAL PERSON PERSON PERSON PERSON ORG P...   \n",
       "21    PERSON DATE DATE DATE PERSON DATE PERSON DATE ...   \n",
       "22    ORG ORG PERSON PERSON PERSON PERSON ORG DATE P...   \n",
       "23    DATE TIME ORG PERSON DATE PERSON PERSON ORG DA...   \n",
       "24    PERSON PERSON DATE DATE DATE MONEY MONEY DATE ...   \n",
       "25    GPE DATE GPE GPE PERSON PERSON ORG PERSON PERS...   \n",
       "26    DATE ORDINAL ORDINAL GPE DATE PERSON MONEY DAT...   \n",
       "27    PERSON PERSON PERSON CARDINAL CARDINAL TIME PE...   \n",
       "28    GPE PERSON GPE GPE DATE PERSON GPE GPE PERSON ...   \n",
       "29    ORG DATE DATE ORG ORG ORG GPE GPE ORG ORDINAL ...   \n",
       "...                                                 ...   \n",
       "2283  ORG PERSON PERSON ORG TIME DATE DATE NORP NORP...   \n",
       "2284  DATE ORG PERSON PERSON PERSON NORP DATE DATE T...   \n",
       "2285  NORP PERSON NORP NORP DATE GPE PERSON NORP NOR...   \n",
       "2286  PERSON TIME TIME TIME TIME MONEY MONEY CARDINA...   \n",
       "2287  ORG DATE DATE CARDINAL GPE GPE ORDINAL PERSON ...   \n",
       "2288  PERSON LOC DATE LOC PERSON NORP DATE LOC LOC D...   \n",
       "2289  GPE TIME PERSON PERSON NORP NORP PERSON GPE OR...   \n",
       "2290  PERSON PERSON PERSON ORG PERSON NORP PERSON GP...   \n",
       "2291  PERSON DATE PERSON PERSON ORDINAL PERSON PERSO...   \n",
       "2292  PERSON DATE PERSON CARDINAL GPE PERSON GPE GPE...   \n",
       "2293  PERSON LOC ORG ORG PERSON PERSON PERSON PERSON...   \n",
       "2294  CARDINAL DATE DATE DATE GPE ORDINAL EVENT DATE...   \n",
       "2295  TIME PERSON PERSON PERSON PERSON NORP TIME GPE...   \n",
       "2296  GPE PERSON NORP GPE GPE GPE ORG NORP PERSON NO...   \n",
       "2297  NORP PERSON PERSON DATE NORP GPE DATE DATE DAT...   \n",
       "2298  ORG DATE DATE GPE PERSON NORP NORP NORP GPE OR...   \n",
       "2299  PERSON DATE GPE PERSON NORP PERSON PERSON PERS...   \n",
       "2300  PERSON PERSON PERSON PERSON PERSON ORG PERSON ...   \n",
       "2301  GPE ORG DATE EVENT ORG DATE ORG PERSON CARDINA...   \n",
       "2302  LOC DATE LOC GPE GPE GPE CARDINAL NORP TIME GP...   \n",
       "2303  TIME TIME TIME TIME TIME TIME TIME TIME TIME T...   \n",
       "2304  GPE PERSON GPE GPE DATE PERSON LOC GPE ORG PER...   \n",
       "2305  PERSON GPE NORP GPE ORG PERSON ORG GPE PERSON ...   \n",
       "2306  ORG GPE PERSON PERSON PERSON FAC ORG FAC FAC L...   \n",
       "2307  DATE NORP NORP PERSON PERSON NORP PERSON PERSO...   \n",
       "2308  ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...   \n",
       "2309                   ORDINAL PERSON PRODUCT DATE DATE   \n",
       "2310  DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...   \n",
       "2311  PERSON PERSON PERSON PERSON PERSON DATE CARDIN...   \n",
       "2312  PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...   \n",
       "\n",
       "                                                     tm  culture  politics  \\\n",
       "0     [0.04325945698517057, 0.0, 0.00142482934694180...        1         1   \n",
       "1     [0.013287880838036227, 0.0, 0.0, 0.00511725094...        0         0   \n",
       "2     [0.0, 0.006699599134802422, 0.0, 0.00564851883...        1         1   \n",
       "3     [0.040282108339079505, 0.03732895646484358, 0....        0         0   \n",
       "4     [0.08049208168957463, 0.0, 0.0, 0.008031187136...        0         0   \n",
       "5     [0.0, 0.01122282724927712, 0.0, 0.163765591818...        0         0   \n",
       "6     [0.062272408748748564, 0.0, 0.0243049615007748...        0         0   \n",
       "7     [0.045631610155157765, 0.0, 0.0, 0.0, 0.004847...        1         0   \n",
       "8     [0.041662618917564086, 0.0, 0.0, 0.0, 0.004215...        1         0   \n",
       "9     [0.003366184031329983, 0.0, 0.0007976442417315...        1         0   \n",
       "10    [0.050577383112491395, 0.0, 0.0, 0.00540960362...        1         0   \n",
       "11    [0.0, 0.0, 0.0, 0.11278535017156091, 0.0, 0.0,...        0         0   \n",
       "12    [0.05140618959723995, 0.0, 0.0, 0.0, 0.0009926...        1         0   \n",
       "13    [0.027425263762819323, 0.0, 0.0103312675628116...        0         0   \n",
       "14    [0.04767230505808768, 0.0, 0.0, 0.0, 0.0151999...        1         0   \n",
       "15    [0.027884164799976387, 0.006465436955228223, 0...        0         0   \n",
       "16    [0.03079442054798949, 0.033779253241753265, 0....        1         0   \n",
       "17    [0.037593597923793624, 0.0029150189212423312, ...        1         0   \n",
       "18    [0.08771567308527171, 0.0, 0.00844979525258797...        1         0   \n",
       "19    [0.04277426034635319, 0.04816268255472439, 0.0...        1         0   \n",
       "20    [0.04036151740308654, 0.0, 0.03179935823326201...        1         0   \n",
       "21    [0.02597837193451334, 0.0, 0.08252494437881452...        1         0   \n",
       "22    [0.02287873745464833, 0.0, 0.01603027422617334...        1         0   \n",
       "23    [0.05046730159068269, 0.008620141121501114, 0....        1         0   \n",
       "24    [0.06367106794151096, 0.0, 0.00889361872405093...        1         0   \n",
       "25    [0.039759217679600674, 0.0, 0.0, 0.00048959509...        1         0   \n",
       "26    [0.030542894542230838, 0.0, 0.0, 0.0, 0.0, 0.0...        1         0   \n",
       "27    [0.00260903544742166, 0.018802235238984867, 0....        1         0   \n",
       "28    [0.07840706808120378, 0.012627683498918756, 0....        1         0   \n",
       "29    [0.030947785944380562, 0.008915481859539746, 0...        1         0   \n",
       "...                                                 ...      ...       ...   \n",
       "2283  [0.05508546301919964, 0.0, 0.00227033438246327...        1         0   \n",
       "2284  [0.015113367818834416, 0.0, 0.0, 0.02859407621...        0         0   \n",
       "2285  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1         1   \n",
       "2286  [0.023778624544851402, 0.007829825788402496, 0...        1         0   \n",
       "2287  [0.0, 0.0, 0.0, 0.0, 0.009131007449666096, 0.0...        1         0   \n",
       "2288  [0.0, 0.0, 0.012520814310613023, 0.0, 0.0, 0.1...        1         0   \n",
       "2289  [0.03468020816383595, 0.004695919146035526, 0....        1         0   \n",
       "2290  [0.05339526199505615, 0.0, 0.0, 0.0, 0.0, 0.0,...        1         0   \n",
       "2291  [0.017161374254025186, 0.0, 0.0015174001197554...        1         0   \n",
       "2292  [0.06401594840887878, 0.016237498143283166, 0....        1         0   \n",
       "2293  [0.03266345974648634, 0.0, 0.0, 0.0, 0.0028504...        1         0   \n",
       "2294  [0.0, 0.0, 0.0, 0.0, 0.0, 0.15660827815235764,...        1         0   \n",
       "2295  [0.015927306932493678, 0.0, 0.0005219988142567...        1         0   \n",
       "2296  [0.0, 0.0, 0.0011856008341072713, 0.0, 0.04285...        1         1   \n",
       "2297  [0.035301185398910626, 0.004142997863142574, 0...        1         1   \n",
       "2298  [0.03825146812357643, 0.0002520530721952876, 0...        1         0   \n",
       "2299  [0.0, 0.0, 0.0010252769472308598, 0.0, 0.0, 0....        1         0   \n",
       "2300  [0.01606418830777042, 0.005388137902250922, 0....        1         0   \n",
       "2301  [0.0023661319331237784, 0.0, 0.000636940282449...        1         0   \n",
       "2302  [0.020778226494857336, 0.002293181170735768, 0...        1         1   \n",
       "2303  [0.07339320400975863, 0.004287185808484255, 0....        0         0   \n",
       "2304  [0.00022790152811686269, 0.0, 4.81912535361483...        1         0   \n",
       "2305  [0.030062572867008263, 0.0002639586848992885, ...        1         0   \n",
       "2306  [0.03400419777173537, 0.0, 0.14868577353664802...        0         0   \n",
       "2307  [0.06112071611956173, 0.002545963587318732, 0....        1         1   \n",
       "2308  [0.0, 0.005742817940048278, 0.0013400504020271...        1         0   \n",
       "2309  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0         0   \n",
       "2310  [0.05075222279006517, 0.0012024359432974202, 0...        1         0   \n",
       "2311  [0.11474111662254229, 0.0031870816685010414, 0...        1         0   \n",
       "2312  [0.05883474744248605, 0.002542825028156852, 0....        1         0   \n",
       "\n",
       "      science  global issues  technology  design  business  biomechanics  \\\n",
       "0           1              1           1       0         0             0   \n",
       "1           0              1           0       1         0             0   \n",
       "2           1              1           0       0         1             0   \n",
       "3           1              0           0       1         1             0   \n",
       "4           1              0           1       1         1             0   \n",
       "5           1              0           1       0         0             1   \n",
       "6           1              0           1       1         0             0   \n",
       "7           0              0           0       1         0             0   \n",
       "8           1              1           1       1         1             0   \n",
       "9           1              1           0       0         0             0   \n",
       "10          1              1           0       0         0             0   \n",
       "11          1              0           1       0         0             0   \n",
       "12          0              0           0       1         1             0   \n",
       "13          1              0           1       0         0             0   \n",
       "14          1              0           0       0         0             0   \n",
       "15          1              0           1       1         0             1   \n",
       "16          1              0           1       0         0             0   \n",
       "17          1              0           0       0         1             0   \n",
       "18          0              0           0       1         1             0   \n",
       "19          1              0           0       0         0             0   \n",
       "20          1              1           0       0         0             0   \n",
       "21          1              1           1       0         1             0   \n",
       "22          1              0           0       0         0             0   \n",
       "23          1              0           0       1         0             1   \n",
       "24          0              0           0       0         1             0   \n",
       "25          1              0           0       0         1             0   \n",
       "26          1              0           0       0         1             0   \n",
       "27          0              0           0       1         0             0   \n",
       "28          1              0           0       1         0             0   \n",
       "29          0              1           1       0         0             0   \n",
       "...       ...            ...         ...     ...       ...           ...   \n",
       "2283        0              0           0       0         0             0   \n",
       "2284        1              0           0       0         0             1   \n",
       "2285        0              1           1       0         0             0   \n",
       "2286        1              1           1       1         1             0   \n",
       "2287        1              1           0       0         0             0   \n",
       "2288        1              1           0       0         0             1   \n",
       "2289        1              0           0       0         0             0   \n",
       "2290        1              0           0       0         0             0   \n",
       "2291        1              0           0       0         0             1   \n",
       "2292        1              0           1       0         0             0   \n",
       "2293        1              0           1       0         0             0   \n",
       "2294        1              1           0       0         0             1   \n",
       "2295        1              1           0       0         0             0   \n",
       "2296        1              1           0       0         1             0   \n",
       "2297        1              1           0       0         0             0   \n",
       "2298        1              1           0       0         0             0   \n",
       "2299        0              1           1       0         0             0   \n",
       "2300        1              1           0       0         0             0   \n",
       "2301        1              1           1       0         0             0   \n",
       "2302        0              1           0       0         0             0   \n",
       "2303        1              0           0       0         0             0   \n",
       "2304        1              1           1       0         0             1   \n",
       "2305        0              1           0       0         0             0   \n",
       "2306        0              0           0       0         0             0   \n",
       "2307        1              1           0       0         0             0   \n",
       "2308        1              0           0       0         0             0   \n",
       "2309        1              0           1       0         0             0   \n",
       "2310        0              0           0       0         0             0   \n",
       "2311        1              0           0       0         0             0   \n",
       "2312        0              0           1       1         0             0   \n",
       "\n",
       "      biodiversity  media  entertainment  history  future  communication  \\\n",
       "0                0      0              0        0       0              0   \n",
       "1                0      0              0        0       0              0   \n",
       "2                0      0              0        0       0              0   \n",
       "3                0      0              0        0       0              0   \n",
       "4                0      0              0        0       0              0   \n",
       "5                1      0              0        0       0              0   \n",
       "6                0      1              1        0       0              0   \n",
       "7                0      0              0        0       0              0   \n",
       "8                0      1              0        0       0              0   \n",
       "9                0      0              0        0       0              0   \n",
       "10               0      0              0        0       0              0   \n",
       "11               0      0              0        0       0              0   \n",
       "12               0      0              0        0       0              0   \n",
       "13               0      0              1        0       0              0   \n",
       "14               0      0              0        1       0              0   \n",
       "15               1      0              0        1       0              0   \n",
       "16               0      0              0        1       1              0   \n",
       "17               0      1              0        0       0              0   \n",
       "18               0      0              1        0       0              1   \n",
       "19               0      0              1        0       0              0   \n",
       "20               0      1              0        0       0              0   \n",
       "21               0      0              0        0       0              0   \n",
       "22               0      0              1        0       0              0   \n",
       "23               1      0              0        0       0              0   \n",
       "24               0      0              0        0       0              0   \n",
       "25               0      0              0        0       0              0   \n",
       "26               0      0              0        0       0              0   \n",
       "27               0      0              0        0       0              0   \n",
       "28               0      0              0        0       0              0   \n",
       "29               0      0              0        0       0              0   \n",
       "...            ...    ...            ...      ...     ...            ...   \n",
       "2283             0      1              0        0       0              0   \n",
       "2284             0      0              0        0       1              0   \n",
       "2285             0      0              0        1       1              1   \n",
       "2286             0      1              0        1       1              0   \n",
       "2287             0      1              0        1       0              0   \n",
       "2288             1      0              0        0       0              1   \n",
       "2289             0      0              1        1       0              0   \n",
       "2290             0      0              0        0       0              1   \n",
       "2291             1      0              0        1       0              0   \n",
       "2292             0      0              1        0       1              1   \n",
       "2293             0      0              0        0       1              1   \n",
       "2294             1      0              0        0       1              0   \n",
       "2295             0      1              0        1       0              0   \n",
       "2296             0      0              0        1       0              0   \n",
       "2297             0      1              0        1       0              0   \n",
       "2298             0      1              0        1       0              1   \n",
       "2299             0      1              0        0       1              0   \n",
       "2300             0      1              0        1       0              0   \n",
       "2301             0      0              0        1       0              0   \n",
       "2302             0      1              0        1       1              1   \n",
       "2303             0      0              0        0       0              1   \n",
       "2304             1      0              0        1       0              0   \n",
       "2305             0      1              0        1       0              0   \n",
       "2306             0      0              0        1       0              0   \n",
       "2307             0      1              0        1       0              1   \n",
       "2308             0      0              0        1       0              0   \n",
       "2309             0      0              0        0       0              0   \n",
       "2310             0      0              0        0       0              0   \n",
       "2311             0      0              0        1       0              1   \n",
       "2312             0      0              0        0       1              0   \n",
       "\n",
       "      humanity  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  \n",
       "11           0  \n",
       "12           0  \n",
       "13           0  \n",
       "14           0  \n",
       "15           0  \n",
       "16           0  \n",
       "17           0  \n",
       "18           0  \n",
       "19           0  \n",
       "20           0  \n",
       "21           0  \n",
       "22           0  \n",
       "23           0  \n",
       "24           0  \n",
       "25           0  \n",
       "26           0  \n",
       "27           0  \n",
       "28           0  \n",
       "29           0  \n",
       "...        ...  \n",
       "2283         0  \n",
       "2284         0  \n",
       "2285         0  \n",
       "2286         1  \n",
       "2287         0  \n",
       "2288         0  \n",
       "2289         1  \n",
       "2290         1  \n",
       "2291         0  \n",
       "2292         1  \n",
       "2293         1  \n",
       "2294         0  \n",
       "2295         0  \n",
       "2296         0  \n",
       "2297         0  \n",
       "2298         1  \n",
       "2299         0  \n",
       "2300         1  \n",
       "2301         1  \n",
       "2302         0  \n",
       "2303         0  \n",
       "2304         0  \n",
       "2305         1  \n",
       "2306         0  \n",
       "2307         0  \n",
       "2308         0  \n",
       "2309         0  \n",
       "2310         1  \n",
       "2311         1  \n",
       "2312         0  \n",
       "\n",
       "[2313 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_one_hot_encode()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_column(target, df):\n",
    "    return df[['headline', 'transcript','pos_sequence', 'ner_sequence','tm', target_tag]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df[['headline', 'transcript','pos_sequence', 'ner_sequence','tm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>global issues</th>\n",
       "      <th>technology</th>\n",
       "      <th>design</th>\n",
       "      <th>business</th>\n",
       "      <th>biomechanics</th>\n",
       "      <th>biodiversity</th>\n",
       "      <th>media</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>history</th>\n",
       "      <th>future</th>\n",
       "      <th>communication</th>\n",
       "      <th>humanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2313 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      culture  politics  science  global issues  technology  design  business  \\\n",
       "0           1         1        1              1           1       0         0   \n",
       "1           0         0        0              1           0       1         0   \n",
       "2           1         1        1              1           0       0         1   \n",
       "3           0         0        1              0           0       1         1   \n",
       "4           0         0        1              0           1       1         1   \n",
       "5           0         0        1              0           1       0         0   \n",
       "6           0         0        1              0           1       1         0   \n",
       "7           1         0        0              0           0       1         0   \n",
       "8           1         0        1              1           1       1         1   \n",
       "9           1         0        1              1           0       0         0   \n",
       "10          1         0        1              1           0       0         0   \n",
       "11          0         0        1              0           1       0         0   \n",
       "12          1         0        0              0           0       1         1   \n",
       "13          0         0        1              0           1       0         0   \n",
       "14          1         0        1              0           0       0         0   \n",
       "15          0         0        1              0           1       1         0   \n",
       "16          1         0        1              0           1       0         0   \n",
       "17          1         0        1              0           0       0         1   \n",
       "18          1         0        0              0           0       1         1   \n",
       "19          1         0        1              0           0       0         0   \n",
       "20          1         0        1              1           0       0         0   \n",
       "21          1         0        1              1           1       0         1   \n",
       "22          1         0        1              0           0       0         0   \n",
       "23          1         0        1              0           0       1         0   \n",
       "24          1         0        0              0           0       0         1   \n",
       "25          1         0        1              0           0       0         1   \n",
       "26          1         0        1              0           0       0         1   \n",
       "27          1         0        0              0           0       1         0   \n",
       "28          1         0        1              0           0       1         0   \n",
       "29          1         0        0              1           1       0         0   \n",
       "...       ...       ...      ...            ...         ...     ...       ...   \n",
       "2283        1         0        0              0           0       0         0   \n",
       "2284        0         0        1              0           0       0         0   \n",
       "2285        1         1        0              1           1       0         0   \n",
       "2286        1         0        1              1           1       1         1   \n",
       "2287        1         0        1              1           0       0         0   \n",
       "2288        1         0        1              1           0       0         0   \n",
       "2289        1         0        1              0           0       0         0   \n",
       "2290        1         0        1              0           0       0         0   \n",
       "2291        1         0        1              0           0       0         0   \n",
       "2292        1         0        1              0           1       0         0   \n",
       "2293        1         0        1              0           1       0         0   \n",
       "2294        1         0        1              1           0       0         0   \n",
       "2295        1         0        1              1           0       0         0   \n",
       "2296        1         1        1              1           0       0         1   \n",
       "2297        1         1        1              1           0       0         0   \n",
       "2298        1         0        1              1           0       0         0   \n",
       "2299        1         0        0              1           1       0         0   \n",
       "2300        1         0        1              1           0       0         0   \n",
       "2301        1         0        1              1           1       0         0   \n",
       "2302        1         1        0              1           0       0         0   \n",
       "2303        0         0        1              0           0       0         0   \n",
       "2304        1         0        1              1           1       0         0   \n",
       "2305        1         0        0              1           0       0         0   \n",
       "2306        0         0        0              0           0       0         0   \n",
       "2307        1         1        1              1           0       0         0   \n",
       "2308        1         0        1              0           0       0         0   \n",
       "2309        0         0        1              0           1       0         0   \n",
       "2310        1         0        0              0           0       0         0   \n",
       "2311        1         0        1              0           0       0         0   \n",
       "2312        1         0        0              0           1       1         0   \n",
       "\n",
       "      biomechanics  biodiversity  media  entertainment  history  future  \\\n",
       "0                0             0      0              0        0       0   \n",
       "1                0             0      0              0        0       0   \n",
       "2                0             0      0              0        0       0   \n",
       "3                0             0      0              0        0       0   \n",
       "4                0             0      0              0        0       0   \n",
       "5                1             1      0              0        0       0   \n",
       "6                0             0      1              1        0       0   \n",
       "7                0             0      0              0        0       0   \n",
       "8                0             0      1              0        0       0   \n",
       "9                0             0      0              0        0       0   \n",
       "10               0             0      0              0        0       0   \n",
       "11               0             0      0              0        0       0   \n",
       "12               0             0      0              0        0       0   \n",
       "13               0             0      0              1        0       0   \n",
       "14               0             0      0              0        1       0   \n",
       "15               1             1      0              0        1       0   \n",
       "16               0             0      0              0        1       1   \n",
       "17               0             0      1              0        0       0   \n",
       "18               0             0      0              1        0       0   \n",
       "19               0             0      0              1        0       0   \n",
       "20               0             0      1              0        0       0   \n",
       "21               0             0      0              0        0       0   \n",
       "22               0             0      0              1        0       0   \n",
       "23               1             1      0              0        0       0   \n",
       "24               0             0      0              0        0       0   \n",
       "25               0             0      0              0        0       0   \n",
       "26               0             0      0              0        0       0   \n",
       "27               0             0      0              0        0       0   \n",
       "28               0             0      0              0        0       0   \n",
       "29               0             0      0              0        0       0   \n",
       "...            ...           ...    ...            ...      ...     ...   \n",
       "2283             0             0      1              0        0       0   \n",
       "2284             1             0      0              0        0       1   \n",
       "2285             0             0      0              0        1       1   \n",
       "2286             0             0      1              0        1       1   \n",
       "2287             0             0      1              0        1       0   \n",
       "2288             1             1      0              0        0       0   \n",
       "2289             0             0      0              1        1       0   \n",
       "2290             0             0      0              0        0       0   \n",
       "2291             1             1      0              0        1       0   \n",
       "2292             0             0      0              1        0       1   \n",
       "2293             0             0      0              0        0       1   \n",
       "2294             1             1      0              0        0       1   \n",
       "2295             0             0      1              0        1       0   \n",
       "2296             0             0      0              0        1       0   \n",
       "2297             0             0      1              0        1       0   \n",
       "2298             0             0      1              0        1       0   \n",
       "2299             0             0      1              0        0       1   \n",
       "2300             0             0      1              0        1       0   \n",
       "2301             0             0      0              0        1       0   \n",
       "2302             0             0      1              0        1       1   \n",
       "2303             0             0      0              0        0       0   \n",
       "2304             1             1      0              0        1       0   \n",
       "2305             0             0      1              0        1       0   \n",
       "2306             0             0      0              0        1       0   \n",
       "2307             0             0      1              0        1       0   \n",
       "2308             0             0      0              0        1       0   \n",
       "2309             0             0      0              0        0       0   \n",
       "2310             0             0      0              0        0       0   \n",
       "2311             0             0      0              0        1       0   \n",
       "2312             0             0      0              0        0       1   \n",
       "\n",
       "      communication  humanity  \n",
       "0                 0         0  \n",
       "1                 0         0  \n",
       "2                 0         0  \n",
       "3                 0         0  \n",
       "4                 0         0  \n",
       "5                 0         0  \n",
       "6                 0         0  \n",
       "7                 0         0  \n",
       "8                 0         0  \n",
       "9                 0         0  \n",
       "10                0         0  \n",
       "11                0         0  \n",
       "12                0         0  \n",
       "13                0         0  \n",
       "14                0         0  \n",
       "15                0         0  \n",
       "16                0         0  \n",
       "17                0         0  \n",
       "18                1         0  \n",
       "19                0         0  \n",
       "20                0         0  \n",
       "21                0         0  \n",
       "22                0         0  \n",
       "23                0         0  \n",
       "24                0         0  \n",
       "25                0         0  \n",
       "26                0         0  \n",
       "27                0         0  \n",
       "28                0         0  \n",
       "29                0         0  \n",
       "...             ...       ...  \n",
       "2283              0         0  \n",
       "2284              0         0  \n",
       "2285              1         0  \n",
       "2286              0         1  \n",
       "2287              0         0  \n",
       "2288              1         0  \n",
       "2289              0         1  \n",
       "2290              1         1  \n",
       "2291              0         0  \n",
       "2292              1         1  \n",
       "2293              1         1  \n",
       "2294              0         0  \n",
       "2295              0         0  \n",
       "2296              0         0  \n",
       "2297              0         0  \n",
       "2298              1         1  \n",
       "2299              0         0  \n",
       "2300              0         1  \n",
       "2301              0         1  \n",
       "2302              1         0  \n",
       "2303              1         0  \n",
       "2304              0         0  \n",
       "2305              0         1  \n",
       "2306              0         0  \n",
       "2307              1         0  \n",
       "2308              0         0  \n",
       "2309              0         0  \n",
       "2310              0         1  \n",
       "2311              1         1  \n",
       "2312              0         0  \n",
       "\n",
       "[2313 rows x 15 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y = df[all_tags]\n",
    "# dff = list(get_target_column('culture', df))\n",
    "# dff\n",
    "# print(len(df_y))\n",
    "df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, train_y, valid_y = train_test_split(df_x, df_y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D, concatenate\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import optimizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Use word embeddings for the main transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract train and test transcripts to list \n",
    "X_train_transcripts = X_train['transcript'].tolist()\n",
    "X_test_transcripts = X_test['transcript'].tolist()\n",
    "# Extract headline - we will use tfidf because headlines are short \n",
    "X_train_headline = X_train['headline'].tolist()\n",
    "X_test_headline = X_test['headline'].tolist()\n",
    "# Extract POS tags\n",
    "X_train_pos_seq= X_train['pos_sequence'].tolist()\n",
    "X_test_pos_seq = X_test['pos_sequence'].tolist()\n",
    "# Extract NER tags\n",
    "X_train_ner_seq = X_train['ner_sequence'].tolist()\n",
    "X_test_ner_seq = X_test['ner_sequence'].tolist()\n",
    "# Extract topic modelling arrays\n",
    "X_train_tm = X_train['tm']\n",
    "X_test_tm = X_test['tm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train_transcripts)\n",
    "\n",
    "X_train_transcripts = tokenizer.texts_to_sequences(X_train_transcripts)\n",
    "X_test_transcripts = tokenizer.texts_to_sequences(X_test_transcripts)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 500 # since the average length is about there. Too long and the predicions are bad. we assume the intro has the most info\n",
    "\n",
    "X_train_transcripts = pad_sequences(X_train_transcripts, padding='post', maxlen=maxlen)\n",
    "X_test_transcripts = pad_sequences(X_test_transcripts, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_path = \"C:/Users/JSaw/Downloads/\"\n",
    "glove_file = open(glove_path+'glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4469 4298 2877  570  399  847 1837 1132 2904 1242  663 2904 1242 3117\n",
      " 2877  848   67  449  583  301 2904 1242 1447 1132  261   21  570    8\n",
      "   17   68 2420 1035  169   21  382 3410  399  598  101  444  101   12\n",
      " 1783  715 2211  471 1378  368 2511  715  433 4299  570  153 2843  273\n",
      "  438  169   76  368  438 1423 1483 1026 1285 1035  528    6   19 2453\n",
      " 2683  598 1274   78  149  598 3460  598   11  813 1721 4157   11   38\n",
      "   24  153 2843  678 1437 2878 1036 2033  469    5 1514 1688   17  802\n",
      "  292   25 2878 1437  814 1026 1026 2454  197  387  141  814 1326  117\n",
      " 1226   14  588 1233 1462  149  241 2878    8  500  102 1548  387  141\n",
      "  964   30 4300   34  241 2878   51   34 3461   51 3709  448  387  141\n",
      "  544  206  241 2878  625   30   14    7 1530 1186  387  141  402   69\n",
      "  241 2878   21   34   47   56  136   47  241 2878   21   77  663   43\n",
      "  663 1394 4552 1437 1676   10  241 2878    7  459 2810  389  888  176\n",
      " 4931 4301  626  264  285  264  285   14  567  399  241  709  389  814\n",
      " 2878    2 2878  399  399  402  354  163 4654  214  197   30 1463 1538\n",
      "    8  793  394  526   20 2405   36  307  230  470  229   63  268  758\n",
      " 3777  230 1753  900  402   10 2878  681 1816   36  108 2990  124   10\n",
      " 4654  163  155   13 1863 1158  735  289  930  814 2878  387  399  133\n",
      "  579  814  354 4401  297    2   19  122  361  173 2212 1928  872 2624\n",
      " 3412 1700  169  708   17  241 2878  276  297  360 2878  814   89  297\n",
      "   13 1863  133  729  144   28  263  580  617  483  526  709  938 2304\n",
      "  814 2878  883  264 3831  693 1462  149  241 2878    8 1151  368 1549\n",
      "  515 3049  264 1569  678   38 2938  642   50   28  399  166 2878  951\n",
      " 1437  180  389 1098 1437  268  137 1387 1530  101 3710 1437  396  368\n",
      "   42 2454 2129  663  133 1967 1530 1437 1430 3778  114  128   80   89\n",
      "  702  241  693  238 2392   63   14 1424   48   69 1838  190  128 3118\n",
      "   23 1482   30 3410  126  747  857 3889 3554  139 1159   77  522  181\n",
      " 3554    1   33  747  857 3554  104    5  747  857   46  943 1744  336\n",
      "  197   38 2421   30 2235  181   32  157   82 1298 2878 1298  709   32\n",
      "  205 4015   34   34   70  286  541 4932 1753  177  814 2878   10  241\n",
      "   17   19  122  361 1151  697  122  361  257   95  579 1753  351  133\n",
      "  133  416  579   28 1130  169    1   17  259   34 1721  103  112    1\n",
      "   17   11  259   10 2769   34  579  307   43  297  580  480    7  133\n",
      "  708  600  149 2878  709  900   17  197   13 4015   17  276   17   13\n",
      "  110 3555 4470  197    2 4015   17   10  160  355  488  239   82  702\n",
      " 2213  506  235 1286   32  298   46  148   38  355]\n",
      "(1734, 500)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_transcripts[1])\n",
    "print(X_train_transcripts.shape)\n",
    "print(type(X_train_transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 tfidf the headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vect_pos = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=50)\n",
    "# tfidf_vect_pos.fit(X_train_headline)\n",
    "\n",
    "# xtrain_tfidf_headline =  tfidf_vect_pos.transform(X_train_headline)\n",
    "# xtest_tfidf_headline =  tfidf_vect_pos.transform(X_test_headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xtrain_tfidf_headline.shape)\n",
    "# print(xtest_tfidf_headline.shape)\n",
    "# print(xtrain_tfidf_headline[0])\n",
    "# print(type(xtrain_tfidf_headline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vect_pos = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# tfidf_vect_pos.fit(df['pos_sequence'])\n",
    "# tfidf_vect_ner = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# tfidf_vect_ner.fit(df['ner_sequence'])\n",
    "\n",
    "# xtrain_tfidf_pos =  tfidf_vect_pos.transform(X_train['pos_sequence'])\n",
    "# xtest_tfidf_pos =  tfidf_vect_pos.transform(X_test['pos_sequence'])\n",
    "\n",
    "# xtrain_tfidf_ner =  tfidf_vect_ner.transform(X_train['ner_sequence'])\n",
    "# xtest_tfidf_ner =  tfidf_vect_ner.transform(X_test['ner_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try word embeddings on the vector \n",
    "tokenizer2 = Tokenizer(num_words=100)\n",
    "tokenizer2.fit_on_texts(X_train_headline)\n",
    "\n",
    "X_train_headline = tokenizer2.texts_to_sequences(X_train_headline)\n",
    "X_test_headline = tokenizer2.texts_to_sequences(X_test_headline)\n",
    "\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1\n",
    "\n",
    "maxlen2 = 100 # since the average length is about there. Too long and the predicions are bad. we assume the intro has the most info\n",
    "\n",
    "X_train_headline = pad_sequences(X_train_headline, padding='post', maxlen=maxlen2)\n",
    "X_test_headline = pad_sequences(X_test_headline, padding='post', maxlen=maxlen2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Place all tm vectors into big array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_vectors(series,num):\n",
    "    big = np.zeros((len(series),num))\n",
    "    for i in range(len(series)):\n",
    "        array = series.iloc[i]\n",
    "        big[i] = array\n",
    "        return big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_tm))\n",
    "print(type(X_test_tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tm = compile_vectors(X_train_tm,15)\n",
    "X_test_tm = compile_vectors(X_test_tm,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1734, 15)\n",
      "(579, 15)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_tm))\n",
    "print(type(X_test_tm))\n",
    "print(X_train_tm.shape)\n",
    "print(X_test_tm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 POS NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "5135 3703\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_pos_seq))\n",
    "print(type(X_test_pos_seq))\n",
    "\n",
    "print(type(X_train_ner_seq))\n",
    "print(type(X_test_ner_seq))\n",
    "print(len(X_train_pos_seq[0]),len(X_train_pos_seq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4031\n"
     ]
    }
   ],
   "source": [
    "ave_pos = 0\n",
    "for i in range(len(X_train_pos_seq)):\n",
    "    small = len(X_train_pos_seq[i])\n",
    "    ave_pos += small\n",
    "for j in range(len(X_test_pos_seq)):\n",
    "    small = len(X_test_pos_seq[j])\n",
    "    ave_pos += small\n",
    "ave_pos = int(ave_pos / (len(X_train_pos_seq)+len(X_test_pos_seq)))\n",
    "print(ave_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VERB', 'PROPN', 'ADV', 'ADJ', 'NOUN', 'ADP', 'SCONJ', 'INTJ', 'AUX', 'PRON', 'X', 'NUM', 'DET', 'PART', 'PUNCT', 'SYM', 'CCONJ']\n"
     ]
    }
   ],
   "source": [
    "unique_pos = list(df['pos_sequence'].str.split(' ', expand=True).stack().unique())\n",
    "print(unique_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "ave_ner = 0\n",
    "for i in range(len(X_train_ner_seq)):\n",
    "    small = len(X_train_ner_seq[i])\n",
    "    ave_ner += small\n",
    "for j in range(len(X_test_ner_seq)):\n",
    "    small = len(X_test_ner_seq[j])\n",
    "    ave_ner += small\n",
    "ave_ner = int(ave_ner / (len(X_train_ner_seq)+len(X_test_ner_seq)))\n",
    "print(ave_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERSON', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'DATE', 'EVENT', 'NORP', 'TIME', 'CARDINAL', 'MONEY', 'ORDINAL', 'PERCENT', 'FAC', 'QUANTITY', 'LANGUAGE', 'LAW', 'WORK_OF_ART', '']\n"
     ]
    }
   ],
   "source": [
    "unique_ner = list(df['ner_sequence'].str.split(' ', expand=True).stack().unique())\n",
    "print(unique_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try word embeddings on the vector \n",
    "tokenizer3 = Tokenizer(num_words=len(unique_pos))\n",
    "tokenizer3.fit_on_texts(X_train_pos_seq)\n",
    "\n",
    "X_train_pos_seq = tokenizer3.texts_to_sequences(X_train_pos_seq)\n",
    "X_test_pos_seq = tokenizer3.texts_to_sequences(X_test_pos_seq)\n",
    "\n",
    "vocab_size3 = len(tokenizer3.word_index) + 1\n",
    "\n",
    "maxlen3 = ave_pos # since the average length is about there. Too long and the predicions are bad. we assume the intro has the most info\n",
    "\n",
    "X_train_pos_seq = pad_sequences(X_train_pos_seq, padding='post', maxlen=maxlen3)\n",
    "X_test_pos_seq = pad_sequences(X_test_pos_seq, padding='post', maxlen=maxlen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try word embeddings on the vector \n",
    "tokenizer4 = Tokenizer(num_words=len(unique_ner))\n",
    "tokenizer4.fit_on_texts(X_train_ner_seq)\n",
    "\n",
    "X_train_ner_seq = tokenizer4.texts_to_sequences(X_train_ner_seq)\n",
    "X_test_ner_seq = tokenizer4.texts_to_sequences(X_test_ner_seq)\n",
    "\n",
    "vocab_size4 = len(tokenizer4.word_index) + 1\n",
    "\n",
    "maxlen4 = ave_ner # since the average length is about there. Too long and the predicions are bad. we assume the intro has the most info\n",
    "\n",
    "X_train_ner_seq = pad_sequences(X_train_ner_seq, padding='post', maxlen=maxlen4)\n",
    "X_test_ner_seq = pad_sequences(X_test_ner_seq, padding='post', maxlen=maxlen4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 215)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 4031)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 100, 100)     4267400     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 215, 20)      420         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 4031, 20)     360         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 96, 128)      64128       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 211, 128)     12928       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 4027, 128)    12928       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 500, 100)     4267400     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 10)           1290        global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 128)          117248      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 10)           1290        global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 10)           1290        global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 10)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 64)           1024        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 10)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 10)           0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 4)            44          dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 4)            516         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 4)            260         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 4)            44          dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 4)            44          dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 20)           0           dense_24[0][0]                   \n",
      "                                                                 dense_25[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 2)            42          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1)            3           dense_32[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,748,659\n",
      "Trainable params: 213,859\n",
      "Non-trainable params: 8,534,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "# define two sets of inputs\n",
    "inputA = Input(shape=(maxlen2,))\n",
    "inputB = Input(shape=(maxlen,))\n",
    "inputTM = Input(shape=(15,))\n",
    "inputNER = Input(shape=(maxlen4,))\n",
    "inputPOS = Input(shape=(maxlen3,))\n",
    " \n",
    "# the first branch operates on the first input which is the headline\n",
    "embedding_layer_headline = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputA) \n",
    "#model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "#model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "x = Conv1D(128, 5, activation='relu')(embedding_layer_headline)\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(10, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(4, activation=\"relu\")(x)\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# x = Dense(50, activation=\"relu\")(inputA)\n",
    "# x = Dense(4, activation=\"relu\")(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    " \n",
    "# the second branch opreates on the second input\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputB)\n",
    "y = LSTM(128)(embedding_layer)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(4, activation='relu')(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "# third input\n",
    "tm = Dense(64,activation='relu')(inputTM)\n",
    "tm = Dense(4,activation='relu')(tm)\n",
    "tm = Model(inputs=inputTM,outputs=tm)\n",
    "\n",
    "# fourth input: NER\n",
    "embedding_layer_ner = Embedding(vocab_size4, 20,trainable=True)(inputNER)\n",
    "ner = Conv1D(128, 5, activation='relu')(embedding_layer_ner)\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "ner = GlobalMaxPooling1D()(ner)\n",
    "ner = Dense(10, activation='relu')(ner)\n",
    "ner = Dropout(0.2)(ner)\n",
    "ner = Dense(4, activation=\"relu\")(ner)\n",
    "ner = Model(inputs=inputNER, outputs=ner)\n",
    "\n",
    "# fifth input: POS\n",
    "embedding_layer_pos = Embedding(vocab_size3, 20,trainable=True)(inputPOS)\n",
    "pos = Conv1D(128, 5, activation='relu')(embedding_layer_pos)\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "pos = GlobalMaxPooling1D()(pos)\n",
    "pos = Dense(10, activation='relu')(pos)\n",
    "pos = Dropout(0.2)(pos)\n",
    "pos = Dense(4, activation=\"relu\")(pos)\n",
    "pos = Model(inputs=inputPOS, outputs=pos)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output, tm.output, ner.output, pos.output])\n",
    " \n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(2, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"sigmoid\")(z)\n",
    " \n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input, tm.input, ner.input, pos.input], outputs=z)\n",
    "print(model.summary())\n",
    "adam = optimizers.adam(lr=0.0001)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1734, 500) 500 <class 'numpy.ndarray'>\n",
      "(1734, 100) 100 <class 'numpy.ndarray'>\n",
      "(1734, 15) 15 <class 'numpy.ndarray'>\n",
      "(1734, 215) 215 <class 'numpy.ndarray'>\n",
      "(1734, 4031) 4031 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_transcripts.shape,maxlen,type(X_train_transcripts))\n",
    "print(X_train_headline.shape,maxlen2,type(X_train_headline))\n",
    "print(X_train_tm.shape,15,type(X_train_tm))\n",
    "print(X_train_ner_seq.shape,maxlen4,type(X_train_ner_seq))\n",
    "print(X_train_pos_seq.shape,maxlen3,type(X_train_pos_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot_cnn_tm_ner_pos.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=0.001)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(threshold, predictions):\n",
    "    return [[1 if j > threshold else 0 for j in i.tolist()] for i in predictions]\n",
    "\n",
    "def get_tag_flat(threshold, predictions):\n",
    "    return [1 if j > threshold else 0 for i in predictions for j in i]\n",
    "# predictions_flushed = get_tag(0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_tn_fp_fn(y_test, y_pred, classes):\n",
    "    '''\n",
    "    Return:\n",
    "    pre_score = {\n",
    "        'tag_1': {\n",
    "            'index': ,\n",
    "            'tp': ,\n",
    "            'tn': ,\n",
    "            'fp': ,\n",
    "            'fn': \n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # Create dictionary of tags \n",
    "    pre_score = {}\n",
    "    for index_tag, tag in enumerate(classes):\n",
    "        pre_score[tag] = {\n",
    "            'index':index_tag,\n",
    "            'tp': 0,\n",
    "            'tn': 0,\n",
    "            'fp': 0,\n",
    "            'fn': 0\n",
    "        }\n",
    "    for transcript_index, transcript_value in enumerate(y_test):\n",
    "        if transcript_value == y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['tp'] += 1\n",
    "        elif transcript_value == y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['tn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['fn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['fp'] += 1\n",
    "    return pre_score\n",
    "# scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, ['culture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall_f1(preprocessed_scores):\n",
    "    for key, value in preprocessed_scores.items():\n",
    "        try:\n",
    "            precision = value['tp']/(value['tp']+value['fp'])\n",
    "        except:\n",
    "#             print('precision issue: {}'.format(key))\n",
    "            precision = 0.0\n",
    "        try:\n",
    "            recall = value['tp']/(value['tp']+value['fn'])\n",
    "        except:\n",
    "#             print('recall issue: {}'.format(key))\n",
    "            recall = 0.0\n",
    "        try:\n",
    "            f1 = (2 * precision * recall)/(precision + recall)\n",
    "        except:\n",
    "#             print('f1 issue: {}'.format(key))\n",
    "            f1=0.0\n",
    "        preprocessed_scores[key]['precision'] = round(precision,2)\n",
    "        preprocessed_scores[key]['recall'] = round(recall,2)\n",
    "        preprocessed_scores[key]['f1'] = round(f1,2)\n",
    "    return preprocessed_scores\n",
    "# final_scores = compute_precision_recall_f1(scores_preprocess)\n",
    "# print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scores_df(tag_classes, final_scores):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    for index, value in enumerate(tag_classes):\n",
    "        precision.append(final_scores[value]['precision'])\n",
    "        recall.append(final_scores[value]['recall'])\n",
    "        f1.append(final_scores[value]['f1'])\n",
    "        accuracy.append((final_scores[value]['tp'] + final_scores[value]['tn'])/(final_scores[value]['tp'] + final_scores[value]['tn'] + final_scores[value]['fp'] + final_scores[value]['fn']))\n",
    "    df_result = pd.DataFrame(list(zip(tag_classes, precision, recall, f1, accuracy)), \n",
    "               columns =['class', 'precision', 'recall', 'f1', 'accuracy']) \n",
    "    return df_result\n",
    "# df_results = format_scores_df(['culture'], final_scores)\n",
    "# print_full_dataframe(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(tag,valid_y, predictions):\n",
    "    highest_f1 = 0\n",
    "    f1_i = []\n",
    "    highest_accuracy_f1 = 0\n",
    "    accuracy_f1_i = []\n",
    "    highest_accuracy = 0\n",
    "    accuracy_i = []\n",
    "    for i in range(0, 100):\n",
    "        i = i/100\n",
    "    #     print(i)\n",
    "        predictions_flushed = get_tag(i,predictions)\n",
    "        scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, [tag])\n",
    "        final_scores = compute_precision_recall_f1(scores_preprocess)\n",
    "    #     print(final_scores)\n",
    "        df_results = format_scores_df([tag], final_scores)\n",
    "        print(df_results)\n",
    "        f1 = final_scores[tag]['f1']\n",
    "        accuracy = df_results.accuracy[0]\n",
    "\n",
    "        if f1 > highest_f1:\n",
    "            highest_f1 = f1\n",
    "            f1_i = [i]\n",
    "            if accuracy > highest_accuracy_f1:\n",
    "                highest_accuracy_f1 = accuracy\n",
    "                accuracy_f1_i = [i]\n",
    "            elif accuracy == highest_accuracy_f1:\n",
    "                accuracy_f1_i.append(i)\n",
    "        elif f1 == highest_f1:\n",
    "            f1_i.append(i)\n",
    "            if accuracy > highest_accuracy_f1:\n",
    "                highest_accuracy_f1 = accuracy\n",
    "                accuracy_f1_i = [i]\n",
    "            elif accuracy == highest_accuracy_f1:\n",
    "                accuracy_f1_i.append(i)\n",
    "\n",
    "        if accuracy > highest_accuracy:\n",
    "            highest_accuracy = accuracy\n",
    "            accuracy_i = [i]\n",
    "        elif accuracy == highest_accuracy:\n",
    "            accuracy_i.append(i)\n",
    "\n",
    "    #     print('\\n')\n",
    "\n",
    "#     print(highest_f1,f1_i)\n",
    "#     print(highest_accuracy_f1,accuracy_f1_i)\n",
    "#     print(highest_accuracy,accuracy_i)\n",
    "    return highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluate_on_training_set(y_test, y_pred):\n",
    "  # Calculate AUC\n",
    "  print(\"AUC is: \", roc_auc_score(y_test, y_pred))\n",
    "  # print out recall and precision\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  # print out confusion matrix\n",
    "  print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "  # # calculate points for ROC curve\n",
    "  fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "  # Plot ROC curve\n",
    "  plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_score(y_test, y_pred)) \n",
    "  plt.plot([0, 1], [0, 1], 'k--') # random predictions curve\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.0])\n",
    "  plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "  plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "  plt.title('Receiver Operating Characteristic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_on_training_set(valid_y, get_tag_flat(0.35))\n",
    "# #print(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(tag):\n",
    "    train_y_tag = train_y[tag]\n",
    "    valid_y_tag = valid_y[tag]\n",
    "    history = model.fit([X_train_headline, X_train_transcripts, X_train_tm, X_train_ner_seq, X_train_pos_seq], train_y_tag, batch_size=32, epochs=4, verbose=1, validation_split=0.2)\n",
    "    predictions = model.predict([X_test_headline, X_test_transcripts, X_test_tm, X_test_ner_seq, X_train_pos_seq])\n",
    "    highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i = get_threshold(tag,valid_y_tag,predictions)\n",
    "    print(highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culture\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.7638 - acc: 0.37 - ETA: 36s - loss: 0.7385 - acc: 0.43 - ETA: 38s - loss: 0.7217 - acc: 0.47 - ETA: 36s - loss: 0.7039 - acc: 0.52 - ETA: 36s - loss: 0.7083 - acc: 0.51 - ETA: 35s - loss: 0.7070 - acc: 0.51 - ETA: 33s - loss: 0.7007 - acc: 0.53 - ETA: 32s - loss: 0.7086 - acc: 0.51 - ETA: 32s - loss: 0.7035 - acc: 0.52 - ETA: 31s - loss: 0.7019 - acc: 0.52 - ETA: 30s - loss: 0.7007 - acc: 0.53 - ETA: 29s - loss: 0.7028 - acc: 0.52 - ETA: 28s - loss: 0.7016 - acc: 0.52 - ETA: 26s - loss: 0.7042 - acc: 0.52 - ETA: 26s - loss: 0.7065 - acc: 0.51 - ETA: 25s - loss: 0.7077 - acc: 0.51 - ETA: 24s - loss: 0.7050 - acc: 0.52 - ETA: 23s - loss: 0.7020 - acc: 0.52 - ETA: 22s - loss: 0.7032 - acc: 0.52 - ETA: 21s - loss: 0.7037 - acc: 0.52 - ETA: 20s - loss: 0.7036 - acc: 0.52 - ETA: 19s - loss: 0.7034 - acc: 0.52 - ETA: 19s - loss: 0.7033 - acc: 0.52 - ETA: 18s - loss: 0.7021 - acc: 0.52 - ETA: 17s - loss: 0.7006 - acc: 0.53 - ETA: 16s - loss: 0.7015 - acc: 0.52 - ETA: 15s - loss: 0.7024 - acc: 0.52 - ETA: 14s - loss: 0.7027 - acc: 0.52 - ETA: 13s - loss: 0.7035 - acc: 0.52 - ETA: 12s - loss: 0.7034 - acc: 0.52 - ETA: 11s - loss: 0.7053 - acc: 0.51 - ETA: 10s - loss: 0.7043 - acc: 0.52 - ETA: 9s - loss: 0.7046 - acc: 0.5208 - ETA: 8s - loss: 0.7074 - acc: 0.513 - ETA: 7s - loss: 0.7089 - acc: 0.509 - ETA: 6s - loss: 0.7097 - acc: 0.507 - ETA: 6s - loss: 0.7084 - acc: 0.511 - ETA: 5s - loss: 0.7088 - acc: 0.509 - ETA: 4s - loss: 0.7086 - acc: 0.510 - ETA: 3s - loss: 0.7078 - acc: 0.512 - ETA: 2s - loss: 0.7070 - acc: 0.514 - ETA: 1s - loss: 0.7071 - acc: 0.514 - ETA: 0s - loss: 0.7078 - acc: 0.512 - 44s 31ms/step - loss: 0.7074 - acc: 0.5133 - val_loss: 0.7191 - val_acc: 0.4813\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 45s - loss: 0.7240 - acc: 0.46 - ETA: 40s - loss: 0.7118 - acc: 0.50 - ETA: 38s - loss: 0.6997 - acc: 0.53 - ETA: 37s - loss: 0.7027 - acc: 0.52 - ETA: 38s - loss: 0.7118 - acc: 0.50 - ETA: 36s - loss: 0.7117 - acc: 0.50 - ETA: 35s - loss: 0.7083 - acc: 0.50 - ETA: 34s - loss: 0.7057 - acc: 0.51 - ETA: 33s - loss: 0.7063 - acc: 0.51 - ETA: 32s - loss: 0.6996 - acc: 0.53 - ETA: 31s - loss: 0.6985 - acc: 0.53 - ETA: 30s - loss: 0.6976 - acc: 0.53 - ETA: 29s - loss: 0.6950 - acc: 0.54 - ETA: 28s - loss: 0.7013 - acc: 0.52 - ETA: 27s - loss: 0.6996 - acc: 0.53 - ETA: 26s - loss: 0.6996 - acc: 0.53 - ETA: 25s - loss: 0.7017 - acc: 0.52 - ETA: 24s - loss: 0.7029 - acc: 0.52 - ETA: 23s - loss: 0.7033 - acc: 0.52 - ETA: 22s - loss: 0.7043 - acc: 0.51 - ETA: 21s - loss: 0.7052 - acc: 0.51 - ETA: 20s - loss: 0.7055 - acc: 0.51 - ETA: 19s - loss: 0.7031 - acc: 0.52 - ETA: 18s - loss: 0.7020 - acc: 0.52 - ETA: 17s - loss: 0.7023 - acc: 0.52 - ETA: 16s - loss: 0.7027 - acc: 0.52 - ETA: 15s - loss: 0.7012 - acc: 0.52 - ETA: 14s - loss: 0.7016 - acc: 0.52 - ETA: 13s - loss: 0.7035 - acc: 0.52 - ETA: 12s - loss: 0.7057 - acc: 0.51 - ETA: 12s - loss: 0.7059 - acc: 0.51 - ETA: 11s - loss: 0.7057 - acc: 0.51 - ETA: 10s - loss: 0.7044 - acc: 0.51 - ETA: 9s - loss: 0.7063 - acc: 0.5129 - ETA: 8s - loss: 0.7058 - acc: 0.514 - ETA: 7s - loss: 0.7049 - acc: 0.516 - ETA: 6s - loss: 0.7048 - acc: 0.516 - ETA: 5s - loss: 0.7055 - acc: 0.514 - ETA: 4s - loss: 0.7065 - acc: 0.512 - ETA: 3s - loss: 0.7058 - acc: 0.514 - ETA: 2s - loss: 0.7059 - acc: 0.513 - ETA: 1s - loss: 0.7049 - acc: 0.516 - ETA: 0s - loss: 0.7058 - acc: 0.513 - 45s 32ms/step - loss: 0.7060 - acc: 0.5133 - val_loss: 0.7172 - val_acc: 0.4813\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 48s - loss: 0.7566 - acc: 0.37 - ETA: 42s - loss: 0.7102 - acc: 0.50 - ETA: 43s - loss: 0.7102 - acc: 0.50 - ETA: 41s - loss: 0.7217 - acc: 0.46 - ETA: 40s - loss: 0.7194 - acc: 0.47 - ETA: 40s - loss: 0.7255 - acc: 0.45 - ETA: 39s - loss: 0.7200 - acc: 0.47 - ETA: 38s - loss: 0.7187 - acc: 0.47 - ETA: 36s - loss: 0.7241 - acc: 0.46 - ETA: 35s - loss: 0.7261 - acc: 0.45 - ETA: 34s - loss: 0.7226 - acc: 0.46 - ETA: 33s - loss: 0.7224 - acc: 0.46 - ETA: 32s - loss: 0.7232 - acc: 0.46 - ETA: 31s - loss: 0.7198 - acc: 0.47 - ETA: 29s - loss: 0.7199 - acc: 0.47 - ETA: 28s - loss: 0.7171 - acc: 0.48 - ETA: 27s - loss: 0.7166 - acc: 0.48 - ETA: 26s - loss: 0.7137 - acc: 0.48 - ETA: 25s - loss: 0.7123 - acc: 0.49 - ETA: 24s - loss: 0.7116 - acc: 0.49 - ETA: 23s - loss: 0.7099 - acc: 0.50 - ETA: 21s - loss: 0.7063 - acc: 0.50 - ETA: 21s - loss: 0.7049 - acc: 0.51 - ETA: 19s - loss: 0.7042 - acc: 0.51 - ETA: 18s - loss: 0.7035 - acc: 0.51 - ETA: 17s - loss: 0.7059 - acc: 0.51 - ETA: 16s - loss: 0.7031 - acc: 0.51 - ETA: 15s - loss: 0.7033 - acc: 0.51 - ETA: 14s - loss: 0.7031 - acc: 0.51 - ETA: 13s - loss: 0.7022 - acc: 0.52 - ETA: 12s - loss: 0.7013 - acc: 0.52 - ETA: 11s - loss: 0.7026 - acc: 0.51 - ETA: 10s - loss: 0.7045 - acc: 0.51 - ETA: 9s - loss: 0.7066 - acc: 0.5083 - ETA: 8s - loss: 0.7057 - acc: 0.510 - ETA: 7s - loss: 0.7036 - acc: 0.516 - ETA: 6s - loss: 0.7044 - acc: 0.514 - ETA: 5s - loss: 0.7045 - acc: 0.514 - ETA: 4s - loss: 0.7046 - acc: 0.513 - ETA: 3s - loss: 0.7042 - acc: 0.514 - ETA: 2s - loss: 0.7048 - acc: 0.513 - ETA: 1s - loss: 0.7049 - acc: 0.512 - ETA: 0s - loss: 0.7047 - acc: 0.513 - 46s 33ms/step - loss: 0.7047 - acc: 0.5133 - val_loss: 0.7153 - val_acc: 0.4813\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 44s - loss: 0.7087 - acc: 0.50 - ETA: 40s - loss: 0.6866 - acc: 0.56 - ETA: 40s - loss: 0.6903 - acc: 0.55 - ETA: 39s - loss: 0.6893 - acc: 0.55 - ETA: 37s - loss: 0.6932 - acc: 0.54 - ETA: 37s - loss: 0.6957 - acc: 0.53 - ETA: 36s - loss: 0.6991 - acc: 0.52 - ETA: 34s - loss: 0.6989 - acc: 0.52 - ETA: 34s - loss: 0.6975 - acc: 0.53 - ETA: 32s - loss: 0.6986 - acc: 0.52 - ETA: 31s - loss: 0.6916 - acc: 0.54 - ETA: 31s - loss: 0.6939 - acc: 0.54 - ETA: 30s - loss: 0.6950 - acc: 0.53 - ETA: 28s - loss: 0.6936 - acc: 0.54 - ETA: 28s - loss: 0.6960 - acc: 0.53 - ETA: 26s - loss: 0.6968 - acc: 0.53 - ETA: 25s - loss: 0.7000 - acc: 0.52 - ETA: 25s - loss: 0.7023 - acc: 0.51 - ETA: 24s - loss: 0.7020 - acc: 0.51 - ETA: 23s - loss: 0.7040 - acc: 0.51 - ETA: 22s - loss: 0.7031 - acc: 0.51 - ETA: 21s - loss: 0.7029 - acc: 0.51 - ETA: 19s - loss: 0.7026 - acc: 0.51 - ETA: 19s - loss: 0.7015 - acc: 0.51 - ETA: 18s - loss: 0.7022 - acc: 0.51 - ETA: 17s - loss: 0.7020 - acc: 0.51 - ETA: 16s - loss: 0.7018 - acc: 0.51 - ETA: 15s - loss: 0.7005 - acc: 0.52 - ETA: 14s - loss: 0.7015 - acc: 0.51 - ETA: 13s - loss: 0.7006 - acc: 0.52 - ETA: 12s - loss: 0.7005 - acc: 0.52 - ETA: 11s - loss: 0.7004 - acc: 0.52 - ETA: 10s - loss: 0.7009 - acc: 0.52 - ETA: 9s - loss: 0.7005 - acc: 0.5221 - ETA: 8s - loss: 0.7010 - acc: 0.520 - ETA: 7s - loss: 0.7021 - acc: 0.517 - ETA: 6s - loss: 0.7025 - acc: 0.516 - ETA: 5s - loss: 0.7029 - acc: 0.514 - ETA: 4s - loss: 0.7017 - acc: 0.518 - ETA: 3s - loss: 0.7029 - acc: 0.514 - ETA: 2s - loss: 0.7022 - acc: 0.516 - ETA: 1s - loss: 0.7034 - acc: 0.513 - ETA: 0s - loss: 0.7032 - acc: 0.513 - 45s 32ms/step - loss: 0.7034 - acc: 0.5133 - val_loss: 0.7136 - val_acc: 0.4813\n",
      "0.68 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41] 0.5181347150259067 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41] 0.5181347150259067 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41]\n",
      "politics\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 45s - loss: 0.6019 - acc: 0.81 - ETA: 41s - loss: 0.5862 - acc: 0.85 - ETA: 42s - loss: 0.5774 - acc: 0.88 - ETA: 40s - loss: 0.5783 - acc: 0.88 - ETA: 38s - loss: 0.5788 - acc: 0.88 - ETA: 37s - loss: 0.5792 - acc: 0.88 - ETA: 36s - loss: 0.5794 - acc: 0.87 - ETA: 35s - loss: 0.5835 - acc: 0.86 - ETA: 34s - loss: 0.5831 - acc: 0.86 - ETA: 33s - loss: 0.5839 - acc: 0.86 - ETA: 32s - loss: 0.5825 - acc: 0.86 - ETA: 31s - loss: 0.5814 - acc: 0.87 - ETA: 30s - loss: 0.5788 - acc: 0.87 - ETA: 28s - loss: 0.5796 - acc: 0.87 - ETA: 28s - loss: 0.5773 - acc: 0.88 - ETA: 27s - loss: 0.5761 - acc: 0.88 - ETA: 25s - loss: 0.5755 - acc: 0.88 - ETA: 25s - loss: 0.5751 - acc: 0.88 - ETA: 23s - loss: 0.5734 - acc: 0.89 - ETA: 22s - loss: 0.5736 - acc: 0.89 - ETA: 22s - loss: 0.5717 - acc: 0.89 - ETA: 21s - loss: 0.5703 - acc: 0.90 - ETA: 19s - loss: 0.5696 - acc: 0.90 - ETA: 19s - loss: 0.5689 - acc: 0.90 - ETA: 18s - loss: 0.5682 - acc: 0.90 - ETA: 16s - loss: 0.5685 - acc: 0.90 - ETA: 16s - loss: 0.5691 - acc: 0.90 - ETA: 15s - loss: 0.5701 - acc: 0.89 - ETA: 14s - loss: 0.5698 - acc: 0.89 - ETA: 13s - loss: 0.5699 - acc: 0.89 - ETA: 12s - loss: 0.5692 - acc: 0.89 - ETA: 11s - loss: 0.5686 - acc: 0.90 - ETA: 10s - loss: 0.5686 - acc: 0.89 - ETA: 9s - loss: 0.5677 - acc: 0.9017 - ETA: 8s - loss: 0.5668 - acc: 0.903 - ETA: 7s - loss: 0.5662 - acc: 0.904 - ETA: 6s - loss: 0.5656 - acc: 0.905 - ETA: 5s - loss: 0.5651 - acc: 0.906 - ETA: 4s - loss: 0.5655 - acc: 0.904 - ETA: 3s - loss: 0.5650 - acc: 0.905 - ETA: 2s - loss: 0.5648 - acc: 0.905 - ETA: 1s - loss: 0.5642 - acc: 0.906 - ETA: 0s - loss: 0.5640 - acc: 0.906 - 45s 32ms/step - loss: 0.5639 - acc: 0.9063 - val_loss: 0.5554 - val_acc: 0.9020\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.5538 - acc: 0.90 - ETA: 40s - loss: 0.5413 - acc: 0.93 - ETA: 38s - loss: 0.5451 - acc: 0.92 - ETA: 37s - loss: 0.5439 - acc: 0.92 - ETA: 38s - loss: 0.5479 - acc: 0.91 - ETA: 36s - loss: 0.5464 - acc: 0.92 - ETA: 35s - loss: 0.5435 - acc: 0.92 - ETA: 34s - loss: 0.5490 - acc: 0.91 - ETA: 33s - loss: 0.5491 - acc: 0.91 - ETA: 32s - loss: 0.5479 - acc: 0.91 - ETA: 31s - loss: 0.5468 - acc: 0.91 - ETA: 30s - loss: 0.5470 - acc: 0.91 - ETA: 29s - loss: 0.5490 - acc: 0.91 - ETA: 29s - loss: 0.5480 - acc: 0.91 - ETA: 28s - loss: 0.5479 - acc: 0.91 - ETA: 27s - loss: 0.5478 - acc: 0.91 - ETA: 26s - loss: 0.5462 - acc: 0.91 - ETA: 25s - loss: 0.5455 - acc: 0.91 - ETA: 24s - loss: 0.5461 - acc: 0.91 - ETA: 23s - loss: 0.5467 - acc: 0.91 - ETA: 22s - loss: 0.5479 - acc: 0.90 - ETA: 21s - loss: 0.5465 - acc: 0.91 - ETA: 20s - loss: 0.5463 - acc: 0.91 - ETA: 19s - loss: 0.5462 - acc: 0.91 - ETA: 18s - loss: 0.5471 - acc: 0.90 - ETA: 17s - loss: 0.5485 - acc: 0.90 - ETA: 16s - loss: 0.5477 - acc: 0.90 - ETA: 15s - loss: 0.5470 - acc: 0.90 - ETA: 14s - loss: 0.5473 - acc: 0.90 - ETA: 13s - loss: 0.5457 - acc: 0.90 - ETA: 12s - loss: 0.5464 - acc: 0.90 - ETA: 11s - loss: 0.5461 - acc: 0.90 - ETA: 10s - loss: 0.5455 - acc: 0.90 - ETA: 9s - loss: 0.5449 - acc: 0.9090 - ETA: 8s - loss: 0.5455 - acc: 0.907 - ETA: 7s - loss: 0.5457 - acc: 0.906 - ETA: 6s - loss: 0.5458 - acc: 0.905 - ETA: 5s - loss: 0.5452 - acc: 0.906 - ETA: 4s - loss: 0.5454 - acc: 0.905 - ETA: 3s - loss: 0.5451 - acc: 0.905 - ETA: 2s - loss: 0.5449 - acc: 0.905 - ETA: 1s - loss: 0.5443 - acc: 0.906 - ETA: 0s - loss: 0.5441 - acc: 0.906 - 46s 33ms/step - loss: 0.5440 - acc: 0.9063 - val_loss: 0.5360 - val_acc: 0.9020\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.5482 - acc: 0.87 - ETA: 37s - loss: 0.5267 - acc: 0.92 - ETA: 38s - loss: 0.5289 - acc: 0.91 - ETA: 37s - loss: 0.5298 - acc: 0.91 - ETA: 38s - loss: 0.5275 - acc: 0.91 - ETA: 36s - loss: 0.5306 - acc: 0.91 - ETA: 35s - loss: 0.5287 - acc: 0.91 - ETA: 33s - loss: 0.5326 - acc: 0.90 - ETA: 33s - loss: 0.5340 - acc: 0.90 - ETA: 32s - loss: 0.5337 - acc: 0.90 - ETA: 31s - loss: 0.5333 - acc: 0.90 - ETA: 30s - loss: 0.5330 - acc: 0.90 - ETA: 29s - loss: 0.5304 - acc: 0.90 - ETA: 28s - loss: 0.5345 - acc: 0.89 - ETA: 27s - loss: 0.5361 - acc: 0.89 - ETA: 26s - loss: 0.5347 - acc: 0.89 - ETA: 25s - loss: 0.5369 - acc: 0.89 - ETA: 24s - loss: 0.5355 - acc: 0.89 - ETA: 23s - loss: 0.5366 - acc: 0.89 - ETA: 22s - loss: 0.5346 - acc: 0.89 - ETA: 21s - loss: 0.5342 - acc: 0.89 - ETA: 20s - loss: 0.5325 - acc: 0.90 - ETA: 19s - loss: 0.5315 - acc: 0.90 - ETA: 18s - loss: 0.5300 - acc: 0.90 - ETA: 17s - loss: 0.5310 - acc: 0.90 - ETA: 16s - loss: 0.5301 - acc: 0.90 - ETA: 15s - loss: 0.5287 - acc: 0.90 - ETA: 14s - loss: 0.5285 - acc: 0.90 - ETA: 14s - loss: 0.5284 - acc: 0.90 - ETA: 13s - loss: 0.5297 - acc: 0.90 - ETA: 12s - loss: 0.5285 - acc: 0.90 - ETA: 11s - loss: 0.5283 - acc: 0.90 - ETA: 10s - loss: 0.5295 - acc: 0.90 - ETA: 9s - loss: 0.5311 - acc: 0.8989 - ETA: 8s - loss: 0.5299 - acc: 0.900 - ETA: 7s - loss: 0.5296 - acc: 0.901 - ETA: 6s - loss: 0.5285 - acc: 0.902 - ETA: 5s - loss: 0.5270 - acc: 0.905 - ETA: 4s - loss: 0.5268 - acc: 0.905 - ETA: 3s - loss: 0.5262 - acc: 0.906 - ETA: 2s - loss: 0.5260 - acc: 0.906 - ETA: 1s - loss: 0.5262 - acc: 0.905 - ETA: 0s - loss: 0.5256 - acc: 0.906 - 45s 32ms/step - loss: 0.5256 - acc: 0.9063 - val_loss: 0.5191 - val_acc: 0.9020\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 48s - loss: 0.5170 - acc: 0.90 - ETA: 42s - loss: 0.5087 - acc: 0.92 - ETA: 40s - loss: 0.5058 - acc: 0.92 - ETA: 38s - loss: 0.5002 - acc: 0.93 - ETA: 38s - loss: 0.5098 - acc: 0.91 - ETA: 36s - loss: 0.5079 - acc: 0.92 - ETA: 36s - loss: 0.5089 - acc: 0.91 - ETA: 35s - loss: 0.5096 - acc: 0.91 - ETA: 33s - loss: 0.5101 - acc: 0.91 - ETA: 32s - loss: 0.5121 - acc: 0.91 - ETA: 31s - loss: 0.5197 - acc: 0.89 - ETA: 30s - loss: 0.5274 - acc: 0.88 - ETA: 30s - loss: 0.5262 - acc: 0.88 - ETA: 28s - loss: 0.5252 - acc: 0.88 - ETA: 27s - loss: 0.5255 - acc: 0.88 - ETA: 26s - loss: 0.5246 - acc: 0.88 - ETA: 25s - loss: 0.5209 - acc: 0.89 - ETA: 24s - loss: 0.5185 - acc: 0.89 - ETA: 23s - loss: 0.5163 - acc: 0.90 - ETA: 22s - loss: 0.5151 - acc: 0.90 - ETA: 21s - loss: 0.5141 - acc: 0.90 - ETA: 20s - loss: 0.5139 - acc: 0.90 - ETA: 19s - loss: 0.5122 - acc: 0.90 - ETA: 18s - loss: 0.5156 - acc: 0.90 - ETA: 18s - loss: 0.5160 - acc: 0.90 - ETA: 16s - loss: 0.5163 - acc: 0.89 - ETA: 15s - loss: 0.5148 - acc: 0.90 - ETA: 14s - loss: 0.5133 - acc: 0.90 - ETA: 14s - loss: 0.5131 - acc: 0.90 - ETA: 12s - loss: 0.5134 - acc: 0.90 - ETA: 12s - loss: 0.5121 - acc: 0.90 - ETA: 11s - loss: 0.5124 - acc: 0.90 - ETA: 10s - loss: 0.5117 - acc: 0.90 - ETA: 9s - loss: 0.5115 - acc: 0.9053 - ETA: 8s - loss: 0.5114 - acc: 0.905 - ETA: 7s - loss: 0.5102 - acc: 0.907 - ETA: 6s - loss: 0.5100 - acc: 0.907 - ETA: 5s - loss: 0.5108 - acc: 0.905 - ETA: 4s - loss: 0.5093 - acc: 0.907 - ETA: 3s - loss: 0.5096 - acc: 0.907 - ETA: 2s - loss: 0.5090 - acc: 0.907 - ETA: 1s - loss: 0.5101 - acc: 0.905 - ETA: 0s - loss: 0.5095 - acc: 0.906 - 45s 32ms/step - loss: 0.5094 - acc: 0.9063 - val_loss: 0.5041 - val_acc: 0.9020\n",
      "0.11 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36] 0.06044905008635579 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36] 0.9395509499136442 [0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "science\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 39s - loss: 0.7154 - acc: 0.53 - ETA: 38s - loss: 0.8136 - acc: 0.35 - ETA: 39s - loss: 0.8285 - acc: 0.33 - ETA: 39s - loss: 0.8316 - acc: 0.32 - ETA: 38s - loss: 0.8406 - acc: 0.31 - ETA: 37s - loss: 0.8317 - acc: 0.32 - ETA: 35s - loss: 0.8305 - acc: 0.33 - ETA: 34s - loss: 0.8295 - acc: 0.33 - ETA: 33s - loss: 0.8308 - acc: 0.32 - ETA: 33s - loss: 0.8318 - acc: 0.32 - ETA: 32s - loss: 0.8358 - acc: 0.32 - ETA: 30s - loss: 0.8347 - acc: 0.32 - ETA: 29s - loss: 0.8310 - acc: 0.32 - ETA: 29s - loss: 0.8278 - acc: 0.33 - ETA: 29s - loss: 0.8274 - acc: 0.33 - ETA: 28s - loss: 0.8271 - acc: 0.33 - ETA: 28s - loss: 0.8246 - acc: 0.34 - ETA: 27s - loss: 0.8235 - acc: 0.34 - ETA: 26s - loss: 0.8205 - acc: 0.34 - ETA: 25s - loss: 0.8232 - acc: 0.34 - ETA: 25s - loss: 0.8230 - acc: 0.34 - ETA: 23s - loss: 0.8253 - acc: 0.33 - ETA: 22s - loss: 0.8220 - acc: 0.34 - ETA: 21s - loss: 0.8182 - acc: 0.35 - ETA: 20s - loss: 0.8162 - acc: 0.35 - ETA: 19s - loss: 0.8149 - acc: 0.35 - ETA: 18s - loss: 0.8157 - acc: 0.35 - ETA: 16s - loss: 0.8133 - acc: 0.35 - ETA: 15s - loss: 0.8129 - acc: 0.35 - ETA: 14s - loss: 0.8125 - acc: 0.35 - ETA: 13s - loss: 0.8132 - acc: 0.35 - ETA: 12s - loss: 0.8144 - acc: 0.35 - ETA: 11s - loss: 0.8124 - acc: 0.35 - ETA: 10s - loss: 0.8150 - acc: 0.35 - ETA: 8s - loss: 0.8136 - acc: 0.3563 - ETA: 7s - loss: 0.8117 - acc: 0.359 - ETA: 6s - loss: 0.8109 - acc: 0.360 - ETA: 5s - loss: 0.8119 - acc: 0.358 - ETA: 4s - loss: 0.8115 - acc: 0.359 - ETA: 3s - loss: 0.8086 - acc: 0.364 - ETA: 2s - loss: 0.8075 - acc: 0.365 - ETA: 1s - loss: 0.8061 - acc: 0.368 - ETA: 0s - loss: 0.8051 - acc: 0.369 - 48s 35ms/step - loss: 0.8039 - acc: 0.3720 - val_loss: 0.7886 - val_acc: 0.3890\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 45s - loss: 0.7625 - acc: 0.43 - ETA: 46s - loss: 0.7792 - acc: 0.40 - ETA: 42s - loss: 0.7679 - acc: 0.42 - ETA: 39s - loss: 0.7790 - acc: 0.40 - ETA: 38s - loss: 0.7756 - acc: 0.41 - ETA: 36s - loss: 0.7871 - acc: 0.39 - ETA: 36s - loss: 0.7977 - acc: 0.37 - ETA: 36s - loss: 0.7973 - acc: 0.37 - ETA: 34s - loss: 0.7988 - acc: 0.36 - ETA: 33s - loss: 0.7967 - acc: 0.37 - ETA: 32s - loss: 0.7919 - acc: 0.38 - ETA: 31s - loss: 0.7934 - acc: 0.37 - ETA: 30s - loss: 0.7921 - acc: 0.37 - ETA: 29s - loss: 0.7957 - acc: 0.37 - ETA: 28s - loss: 0.7900 - acc: 0.38 - ETA: 27s - loss: 0.7922 - acc: 0.37 - ETA: 26s - loss: 0.7902 - acc: 0.38 - ETA: 25s - loss: 0.7930 - acc: 0.37 - ETA: 24s - loss: 0.7921 - acc: 0.37 - ETA: 23s - loss: 0.7937 - acc: 0.37 - ETA: 22s - loss: 0.7943 - acc: 0.37 - ETA: 21s - loss: 0.7971 - acc: 0.36 - ETA: 20s - loss: 0.7982 - acc: 0.36 - ETA: 19s - loss: 0.7999 - acc: 0.36 - ETA: 18s - loss: 0.7982 - acc: 0.36 - ETA: 17s - loss: 0.7985 - acc: 0.36 - ETA: 16s - loss: 0.7952 - acc: 0.37 - ETA: 15s - loss: 0.7955 - acc: 0.36 - ETA: 14s - loss: 0.7953 - acc: 0.36 - ETA: 13s - loss: 0.7971 - acc: 0.36 - ETA: 12s - loss: 0.7963 - acc: 0.36 - ETA: 11s - loss: 0.7941 - acc: 0.37 - ETA: 10s - loss: 0.7949 - acc: 0.36 - ETA: 9s - loss: 0.7942 - acc: 0.3704 - ETA: 8s - loss: 0.7935 - acc: 0.371 - ETA: 7s - loss: 0.7920 - acc: 0.374 - ETA: 6s - loss: 0.7919 - acc: 0.374 - ETA: 5s - loss: 0.7913 - acc: 0.375 - ETA: 4s - loss: 0.7920 - acc: 0.373 - ETA: 3s - loss: 0.7918 - acc: 0.373 - ETA: 2s - loss: 0.7920 - acc: 0.372 - ETA: 1s - loss: 0.7919 - acc: 0.372 - ETA: 0s - loss: 0.7913 - acc: 0.373 - 45s 33ms/step - loss: 0.7920 - acc: 0.3720 - val_loss: 0.7774 - val_acc: 0.3890\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.7229 - acc: 0.50 - ETA: 37s - loss: 0.7535 - acc: 0.43 - ETA: 42s - loss: 0.7483 - acc: 0.44 - ETA: 39s - loss: 0.7685 - acc: 0.40 - ETA: 38s - loss: 0.7715 - acc: 0.40 - ETA: 36s - loss: 0.7784 - acc: 0.38 - ETA: 35s - loss: 0.7812 - acc: 0.37 - ETA: 34s - loss: 0.7832 - acc: 0.37 - ETA: 34s - loss: 0.7831 - acc: 0.37 - ETA: 33s - loss: 0.7830 - acc: 0.37 - ETA: 32s - loss: 0.7801 - acc: 0.38 - ETA: 30s - loss: 0.7802 - acc: 0.38 - ETA: 29s - loss: 0.7780 - acc: 0.38 - ETA: 28s - loss: 0.7793 - acc: 0.38 - ETA: 28s - loss: 0.7744 - acc: 0.39 - ETA: 27s - loss: 0.7748 - acc: 0.39 - ETA: 26s - loss: 0.7768 - acc: 0.38 - ETA: 24s - loss: 0.7778 - acc: 0.38 - ETA: 23s - loss: 0.7779 - acc: 0.38 - ETA: 23s - loss: 0.7787 - acc: 0.38 - ETA: 22s - loss: 0.7808 - acc: 0.37 - ETA: 21s - loss: 0.7841 - acc: 0.36 - ETA: 20s - loss: 0.7851 - acc: 0.36 - ETA: 18s - loss: 0.7848 - acc: 0.36 - ETA: 17s - loss: 0.7845 - acc: 0.36 - ETA: 17s - loss: 0.7854 - acc: 0.36 - ETA: 16s - loss: 0.7862 - acc: 0.36 - ETA: 15s - loss: 0.7838 - acc: 0.36 - ETA: 14s - loss: 0.7820 - acc: 0.37 - ETA: 13s - loss: 0.7819 - acc: 0.37 - ETA: 12s - loss: 0.7808 - acc: 0.37 - ETA: 11s - loss: 0.7802 - acc: 0.37 - ETA: 10s - loss: 0.7800 - acc: 0.37 - ETA: 9s - loss: 0.7795 - acc: 0.3759 - ETA: 8s - loss: 0.7790 - acc: 0.376 - ETA: 7s - loss: 0.7785 - acc: 0.377 - ETA: 6s - loss: 0.7780 - acc: 0.378 - ETA: 5s - loss: 0.7775 - acc: 0.379 - ETA: 4s - loss: 0.7778 - acc: 0.378 - ETA: 3s - loss: 0.7777 - acc: 0.378 - ETA: 2s - loss: 0.7783 - acc: 0.376 - ETA: 1s - loss: 0.7805 - acc: 0.371 - ETA: 0s - loss: 0.7804 - acc: 0.371 - 46s 33ms/step - loss: 0.7800 - acc: 0.3720 - val_loss: 0.7668 - val_acc: 0.3890\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.8146 - acc: 0.28 - ETA: 47s - loss: 0.7937 - acc: 0.32 - ETA: 45s - loss: 0.7866 - acc: 0.34 - ETA: 42s - loss: 0.7796 - acc: 0.35 - ETA: 39s - loss: 0.7753 - acc: 0.36 - ETA: 37s - loss: 0.7701 - acc: 0.38 - ETA: 36s - loss: 0.7684 - acc: 0.38 - ETA: 36s - loss: 0.7739 - acc: 0.37 - ETA: 35s - loss: 0.7751 - acc: 0.36 - ETA: 33s - loss: 0.7746 - acc: 0.36 - ETA: 32s - loss: 0.7816 - acc: 0.35 - ETA: 31s - loss: 0.7773 - acc: 0.36 - ETA: 29s - loss: 0.7747 - acc: 0.36 - ETA: 29s - loss: 0.7753 - acc: 0.36 - ETA: 28s - loss: 0.7722 - acc: 0.37 - ETA: 27s - loss: 0.7687 - acc: 0.38 - ETA: 26s - loss: 0.7711 - acc: 0.37 - ETA: 25s - loss: 0.7710 - acc: 0.37 - ETA: 25s - loss: 0.7701 - acc: 0.37 - ETA: 24s - loss: 0.7707 - acc: 0.37 - ETA: 23s - loss: 0.7731 - acc: 0.36 - ETA: 22s - loss: 0.7735 - acc: 0.36 - ETA: 21s - loss: 0.7738 - acc: 0.36 - ETA: 20s - loss: 0.7714 - acc: 0.37 - ETA: 19s - loss: 0.7712 - acc: 0.37 - ETA: 18s - loss: 0.7715 - acc: 0.37 - ETA: 17s - loss: 0.7733 - acc: 0.36 - ETA: 16s - loss: 0.7735 - acc: 0.36 - ETA: 14s - loss: 0.7724 - acc: 0.36 - ETA: 14s - loss: 0.7726 - acc: 0.36 - ETA: 12s - loss: 0.7711 - acc: 0.37 - ETA: 11s - loss: 0.7710 - acc: 0.37 - ETA: 10s - loss: 0.7708 - acc: 0.37 - ETA: 9s - loss: 0.7710 - acc: 0.3704 - ETA: 8s - loss: 0.7708 - acc: 0.370 - ETA: 7s - loss: 0.7707 - acc: 0.370 - ETA: 6s - loss: 0.7701 - acc: 0.371 - ETA: 5s - loss: 0.7700 - acc: 0.371 - ETA: 4s - loss: 0.7689 - acc: 0.374 - ETA: 3s - loss: 0.7684 - acc: 0.375 - ETA: 2s - loss: 0.7674 - acc: 0.377 - ETA: 1s - loss: 0.7679 - acc: 0.375 - ETA: 0s - loss: 0.7689 - acc: 0.372 - 48s 34ms/step - loss: 0.7692 - acc: 0.3720 - val_loss: 0.7572 - val_acc: 0.3890\n",
      "0.8 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4] 0.6632124352331606 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4] 0.6632124352331606 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4]\n",
      "global issues\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 50s - loss: 0.6257 - acc: 0.71 - ETA: 46s - loss: 0.6319 - acc: 0.70 - ETA: 42s - loss: 0.6051 - acc: 0.77 - ETA: 40s - loss: 0.6072 - acc: 0.76 - ETA: 38s - loss: 0.6134 - acc: 0.75 - ETA: 36s - loss: 0.6258 - acc: 0.71 - ETA: 36s - loss: 0.6206 - acc: 0.73 - ETA: 36s - loss: 0.6274 - acc: 0.71 - ETA: 34s - loss: 0.6273 - acc: 0.71 - ETA: 33s - loss: 0.6235 - acc: 0.72 - ETA: 31s - loss: 0.6237 - acc: 0.72 - ETA: 30s - loss: 0.6249 - acc: 0.72 - ETA: 30s - loss: 0.6269 - acc: 0.71 - ETA: 29s - loss: 0.6277 - acc: 0.71 - ETA: 28s - loss: 0.6268 - acc: 0.71 - ETA: 27s - loss: 0.6275 - acc: 0.71 - ETA: 25s - loss: 0.6289 - acc: 0.71 - ETA: 24s - loss: 0.6294 - acc: 0.71 - ETA: 24s - loss: 0.6286 - acc: 0.71 - ETA: 23s - loss: 0.6272 - acc: 0.71 - ETA: 22s - loss: 0.6248 - acc: 0.72 - ETA: 20s - loss: 0.6254 - acc: 0.72 - ETA: 19s - loss: 0.6276 - acc: 0.71 - ETA: 18s - loss: 0.6275 - acc: 0.71 - ETA: 18s - loss: 0.6274 - acc: 0.71 - ETA: 17s - loss: 0.6288 - acc: 0.71 - ETA: 16s - loss: 0.6291 - acc: 0.71 - ETA: 15s - loss: 0.6290 - acc: 0.71 - ETA: 14s - loss: 0.6306 - acc: 0.70 - ETA: 13s - loss: 0.6325 - acc: 0.70 - ETA: 12s - loss: 0.6331 - acc: 0.70 - ETA: 11s - loss: 0.6329 - acc: 0.70 - ETA: 10s - loss: 0.6330 - acc: 0.70 - ETA: 9s - loss: 0.6332 - acc: 0.7004 - ETA: 8s - loss: 0.6333 - acc: 0.700 - ETA: 7s - loss: 0.6327 - acc: 0.701 - ETA: 6s - loss: 0.6315 - acc: 0.704 - ETA: 5s - loss: 0.6300 - acc: 0.708 - ETA: 4s - loss: 0.6296 - acc: 0.709 - ETA: 3s - loss: 0.6295 - acc: 0.709 - ETA: 2s - loss: 0.6290 - acc: 0.710 - ETA: 1s - loss: 0.6289 - acc: 0.710 - ETA: 0s - loss: 0.6283 - acc: 0.712 - 45s 32ms/step - loss: 0.6276 - acc: 0.7138 - val_loss: 0.6346 - val_acc: 0.6945\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 45s - loss: 0.6501 - acc: 0.65 - ETA: 42s - loss: 0.6565 - acc: 0.64 - ETA: 45s - loss: 0.6416 - acc: 0.67 - ETA: 42s - loss: 0.6501 - acc: 0.65 - ETA: 40s - loss: 0.6526 - acc: 0.65 - ETA: 38s - loss: 0.6437 - acc: 0.67 - ETA: 37s - loss: 0.6501 - acc: 0.65 - ETA: 36s - loss: 0.6516 - acc: 0.65 - ETA: 35s - loss: 0.6429 - acc: 0.67 - ETA: 34s - loss: 0.6397 - acc: 0.68 - ETA: 33s - loss: 0.6430 - acc: 0.67 - ETA: 32s - loss: 0.6414 - acc: 0.67 - ETA: 30s - loss: 0.6381 - acc: 0.68 - ETA: 30s - loss: 0.6399 - acc: 0.68 - ETA: 29s - loss: 0.6388 - acc: 0.68 - ETA: 27s - loss: 0.6411 - acc: 0.67 - ETA: 26s - loss: 0.6393 - acc: 0.68 - ETA: 25s - loss: 0.6370 - acc: 0.68 - ETA: 24s - loss: 0.6363 - acc: 0.68 - ETA: 23s - loss: 0.6376 - acc: 0.68 - ETA: 22s - loss: 0.6363 - acc: 0.68 - ETA: 21s - loss: 0.6369 - acc: 0.68 - ETA: 20s - loss: 0.6364 - acc: 0.68 - ETA: 19s - loss: 0.6369 - acc: 0.68 - ETA: 18s - loss: 0.6359 - acc: 0.69 - ETA: 17s - loss: 0.6339 - acc: 0.69 - ETA: 16s - loss: 0.6344 - acc: 0.69 - ETA: 15s - loss: 0.6322 - acc: 0.69 - ETA: 14s - loss: 0.6314 - acc: 0.70 - ETA: 13s - loss: 0.6307 - acc: 0.70 - ETA: 12s - loss: 0.6300 - acc: 0.70 - ETA: 11s - loss: 0.6302 - acc: 0.70 - ETA: 10s - loss: 0.6304 - acc: 0.70 - ETA: 9s - loss: 0.6298 - acc: 0.7040 - ETA: 8s - loss: 0.6281 - acc: 0.708 - ETA: 7s - loss: 0.6276 - acc: 0.709 - ETA: 6s - loss: 0.6253 - acc: 0.714 - ETA: 5s - loss: 0.6252 - acc: 0.714 - ETA: 4s - loss: 0.6248 - acc: 0.715 - ETA: 3s - loss: 0.6247 - acc: 0.715 - ETA: 2s - loss: 0.6247 - acc: 0.715 - ETA: 1s - loss: 0.6240 - acc: 0.717 - ETA: 0s - loss: 0.6249 - acc: 0.715 - 46s 33ms/step - loss: 0.6255 - acc: 0.7138 - val_loss: 0.6325 - val_acc: 0.6945\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 39s - loss: 0.5550 - acc: 0.87 - ETA: 43s - loss: 0.5953 - acc: 0.78 - ETA: 42s - loss: 0.6087 - acc: 0.75 - ETA: 40s - loss: 0.6153 - acc: 0.73 - ETA: 40s - loss: 0.6059 - acc: 0.75 - ETA: 38s - loss: 0.6108 - acc: 0.74 - ETA: 38s - loss: 0.6123 - acc: 0.74 - ETA: 37s - loss: 0.6186 - acc: 0.72 - ETA: 36s - loss: 0.6189 - acc: 0.72 - ETA: 34s - loss: 0.6178 - acc: 0.72 - ETA: 33s - loss: 0.6157 - acc: 0.73 - ETA: 31s - loss: 0.6139 - acc: 0.73 - ETA: 31s - loss: 0.6187 - acc: 0.72 - ETA: 30s - loss: 0.6218 - acc: 0.71 - ETA: 29s - loss: 0.6254 - acc: 0.71 - ETA: 27s - loss: 0.6311 - acc: 0.69 - ETA: 26s - loss: 0.6281 - acc: 0.70 - ETA: 25s - loss: 0.6277 - acc: 0.70 - ETA: 24s - loss: 0.6259 - acc: 0.70 - ETA: 23s - loss: 0.6243 - acc: 0.71 - ETA: 22s - loss: 0.6228 - acc: 0.71 - ETA: 21s - loss: 0.6228 - acc: 0.71 - ETA: 20s - loss: 0.6233 - acc: 0.71 - ETA: 19s - loss: 0.6243 - acc: 0.71 - ETA: 18s - loss: 0.6253 - acc: 0.71 - ETA: 17s - loss: 0.6251 - acc: 0.71 - ETA: 16s - loss: 0.6265 - acc: 0.70 - ETA: 15s - loss: 0.6243 - acc: 0.71 - ETA: 14s - loss: 0.6232 - acc: 0.71 - ETA: 13s - loss: 0.6222 - acc: 0.71 - ETA: 12s - loss: 0.6217 - acc: 0.71 - ETA: 11s - loss: 0.6230 - acc: 0.71 - ETA: 10s - loss: 0.6225 - acc: 0.71 - ETA: 9s - loss: 0.6240 - acc: 0.7123 - ETA: 8s - loss: 0.6239 - acc: 0.712 - ETA: 7s - loss: 0.6234 - acc: 0.713 - ETA: 6s - loss: 0.6260 - acc: 0.707 - ETA: 5s - loss: 0.6266 - acc: 0.706 - ETA: 4s - loss: 0.6260 - acc: 0.707 - ETA: 3s - loss: 0.6245 - acc: 0.710 - ETA: 2s - loss: 0.6237 - acc: 0.712 - ETA: 1s - loss: 0.6233 - acc: 0.713 - ETA: 0s - loss: 0.6232 - acc: 0.713 - 46s 33ms/step - loss: 0.6231 - acc: 0.7138 - val_loss: 0.6307 - val_acc: 0.6945\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 50s - loss: 0.6057 - acc: 0.75 - ETA: 43s - loss: 0.5916 - acc: 0.78 - ETA: 42s - loss: 0.6244 - acc: 0.70 - ETA: 39s - loss: 0.6091 - acc: 0.74 - ETA: 38s - loss: 0.6140 - acc: 0.73 - ETA: 36s - loss: 0.6126 - acc: 0.73 - ETA: 36s - loss: 0.6115 - acc: 0.73 - ETA: 35s - loss: 0.6143 - acc: 0.73 - ETA: 34s - loss: 0.6133 - acc: 0.73 - ETA: 33s - loss: 0.6167 - acc: 0.72 - ETA: 32s - loss: 0.6169 - acc: 0.72 - ETA: 31s - loss: 0.6207 - acc: 0.71 - ETA: 30s - loss: 0.6250 - acc: 0.70 - ETA: 29s - loss: 0.6205 - acc: 0.71 - ETA: 28s - loss: 0.6184 - acc: 0.72 - ETA: 27s - loss: 0.6203 - acc: 0.71 - ETA: 26s - loss: 0.6193 - acc: 0.71 - ETA: 25s - loss: 0.6153 - acc: 0.72 - ETA: 24s - loss: 0.6148 - acc: 0.72 - ETA: 23s - loss: 0.6164 - acc: 0.72 - ETA: 22s - loss: 0.6172 - acc: 0.72 - ETA: 21s - loss: 0.6166 - acc: 0.72 - ETA: 20s - loss: 0.6198 - acc: 0.71 - ETA: 19s - loss: 0.6216 - acc: 0.71 - ETA: 18s - loss: 0.6226 - acc: 0.71 - ETA: 17s - loss: 0.6230 - acc: 0.71 - ETA: 16s - loss: 0.6228 - acc: 0.71 - ETA: 15s - loss: 0.6227 - acc: 0.71 - ETA: 14s - loss: 0.6215 - acc: 0.71 - ETA: 13s - loss: 0.6204 - acc: 0.71 - ETA: 12s - loss: 0.6204 - acc: 0.71 - ETA: 11s - loss: 0.6208 - acc: 0.71 - ETA: 10s - loss: 0.6198 - acc: 0.71 - ETA: 9s - loss: 0.6189 - acc: 0.7188 - ETA: 8s - loss: 0.6168 - acc: 0.723 - ETA: 7s - loss: 0.6192 - acc: 0.717 - ETA: 6s - loss: 0.6212 - acc: 0.713 - ETA: 5s - loss: 0.6196 - acc: 0.717 - ETA: 4s - loss: 0.6203 - acc: 0.715 - ETA: 3s - loss: 0.6206 - acc: 0.714 - ETA: 2s - loss: 0.6201 - acc: 0.715 - ETA: 1s - loss: 0.6208 - acc: 0.714 - ETA: 0s - loss: 0.6204 - acc: 0.715 - 46s 33ms/step - loss: 0.6210 - acc: 0.7138 - val_loss: 0.6290 - val_acc: 0.6945\n",
      "0.47 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38] 0.30397236614853196 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38] 0.696027633851468 [0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "technology\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 49s - loss: 0.6323 - acc: 0.68 - ETA: 42s - loss: 0.6396 - acc: 0.67 - ETA: 43s - loss: 0.6667 - acc: 0.61 - ETA: 41s - loss: 0.6618 - acc: 0.62 - ETA: 40s - loss: 0.6647 - acc: 0.61 - ETA: 38s - loss: 0.6642 - acc: 0.61 - ETA: 37s - loss: 0.6702 - acc: 0.60 - ETA: 35s - loss: 0.6691 - acc: 0.60 - ETA: 35s - loss: 0.6749 - acc: 0.59 - ETA: 34s - loss: 0.6780 - acc: 0.59 - ETA: 32s - loss: 0.6752 - acc: 0.59 - ETA: 31s - loss: 0.6704 - acc: 0.60 - ETA: 30s - loss: 0.6686 - acc: 0.61 - ETA: 29s - loss: 0.6691 - acc: 0.60 - ETA: 28s - loss: 0.6686 - acc: 0.61 - ETA: 27s - loss: 0.6710 - acc: 0.60 - ETA: 26s - loss: 0.6704 - acc: 0.60 - ETA: 25s - loss: 0.6700 - acc: 0.60 - ETA: 24s - loss: 0.6664 - acc: 0.61 - ETA: 23s - loss: 0.6654 - acc: 0.61 - ETA: 22s - loss: 0.6631 - acc: 0.62 - ETA: 21s - loss: 0.6631 - acc: 0.62 - ETA: 20s - loss: 0.6630 - acc: 0.62 - ETA: 19s - loss: 0.6623 - acc: 0.62 - ETA: 18s - loss: 0.6629 - acc: 0.62 - ETA: 17s - loss: 0.6600 - acc: 0.62 - ETA: 16s - loss: 0.6579 - acc: 0.63 - ETA: 15s - loss: 0.6554 - acc: 0.63 - ETA: 14s - loss: 0.6556 - acc: 0.63 - ETA: 13s - loss: 0.6558 - acc: 0.63 - ETA: 12s - loss: 0.6531 - acc: 0.64 - ETA: 11s - loss: 0.6538 - acc: 0.64 - ETA: 10s - loss: 0.6536 - acc: 0.64 - ETA: 9s - loss: 0.6525 - acc: 0.6443 - ETA: 8s - loss: 0.6537 - acc: 0.642 - ETA: 7s - loss: 0.6539 - acc: 0.641 - ETA: 6s - loss: 0.6521 - acc: 0.645 - ETA: 5s - loss: 0.6523 - acc: 0.644 - ETA: 4s - loss: 0.6514 - acc: 0.646 - ETA: 3s - loss: 0.6509 - acc: 0.647 - ETA: 2s - loss: 0.6519 - acc: 0.645 - ETA: 1s - loss: 0.6514 - acc: 0.646 - ETA: 0s - loss: 0.6517 - acc: 0.646 - 46s 33ms/step - loss: 0.6514 - acc: 0.6467 - val_loss: 0.6325 - val_acc: 0.6859\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.6167 - acc: 0.71 - ETA: 37s - loss: 0.6392 - acc: 0.67 - ETA: 40s - loss: 0.6267 - acc: 0.69 - ETA: 39s - loss: 0.6392 - acc: 0.67 - ETA: 37s - loss: 0.6347 - acc: 0.68 - ETA: 37s - loss: 0.6367 - acc: 0.67 - ETA: 35s - loss: 0.6402 - acc: 0.66 - ETA: 34s - loss: 0.6410 - acc: 0.66 - ETA: 34s - loss: 0.6433 - acc: 0.66 - ETA: 33s - loss: 0.6407 - acc: 0.66 - ETA: 32s - loss: 0.6453 - acc: 0.65 - ETA: 31s - loss: 0.6429 - acc: 0.66 - ETA: 29s - loss: 0.6490 - acc: 0.65 - ETA: 28s - loss: 0.6477 - acc: 0.65 - ETA: 28s - loss: 0.6436 - acc: 0.66 - ETA: 27s - loss: 0.6410 - acc: 0.66 - ETA: 26s - loss: 0.6413 - acc: 0.66 - ETA: 25s - loss: 0.6450 - acc: 0.65 - ETA: 24s - loss: 0.6450 - acc: 0.65 - ETA: 23s - loss: 0.6481 - acc: 0.65 - ETA: 22s - loss: 0.6473 - acc: 0.65 - ETA: 21s - loss: 0.6466 - acc: 0.65 - ETA: 20s - loss: 0.6460 - acc: 0.65 - ETA: 19s - loss: 0.6479 - acc: 0.65 - ETA: 18s - loss: 0.6478 - acc: 0.65 - ETA: 17s - loss: 0.6501 - acc: 0.64 - ETA: 16s - loss: 0.6500 - acc: 0.64 - ETA: 15s - loss: 0.6482 - acc: 0.65 - ETA: 14s - loss: 0.6487 - acc: 0.65 - ETA: 13s - loss: 0.6491 - acc: 0.65 - ETA: 12s - loss: 0.6485 - acc: 0.65 - ETA: 11s - loss: 0.6480 - acc: 0.65 - ETA: 10s - loss: 0.6493 - acc: 0.65 - ETA: 9s - loss: 0.6501 - acc: 0.6489 - ETA: 8s - loss: 0.6509 - acc: 0.647 - ETA: 7s - loss: 0.6504 - acc: 0.648 - ETA: 6s - loss: 0.6527 - acc: 0.643 - ETA: 5s - loss: 0.6522 - acc: 0.644 - ETA: 4s - loss: 0.6497 - acc: 0.649 - ETA: 3s - loss: 0.6526 - acc: 0.643 - ETA: 2s - loss: 0.6528 - acc: 0.643 - ETA: 1s - loss: 0.6527 - acc: 0.643 - ETA: 0s - loss: 0.6515 - acc: 0.646 - 45s 33ms/step - loss: 0.6512 - acc: 0.6467 - val_loss: 0.6320 - val_acc: 0.6859\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 45s - loss: 0.6920 - acc: 0.56 - ETA: 40s - loss: 0.6768 - acc: 0.59 - ETA: 38s - loss: 0.6718 - acc: 0.60 - ETA: 39s - loss: 0.6844 - acc: 0.57 - ETA: 37s - loss: 0.6799 - acc: 0.58 - ETA: 39s - loss: 0.6895 - acc: 0.56 - ETA: 37s - loss: 0.6877 - acc: 0.57 - ETA: 36s - loss: 0.6921 - acc: 0.56 - ETA: 35s - loss: 0.6887 - acc: 0.56 - ETA: 34s - loss: 0.6890 - acc: 0.56 - ETA: 33s - loss: 0.6851 - acc: 0.57 - ETA: 32s - loss: 0.6806 - acc: 0.58 - ETA: 31s - loss: 0.6768 - acc: 0.59 - ETA: 29s - loss: 0.6682 - acc: 0.61 - ETA: 29s - loss: 0.6677 - acc: 0.61 - ETA: 27s - loss: 0.6673 - acc: 0.61 - ETA: 26s - loss: 0.6670 - acc: 0.61 - ETA: 25s - loss: 0.6650 - acc: 0.61 - ETA: 24s - loss: 0.6672 - acc: 0.61 - ETA: 23s - loss: 0.6700 - acc: 0.60 - ETA: 22s - loss: 0.6682 - acc: 0.61 - ETA: 21s - loss: 0.6644 - acc: 0.61 - ETA: 20s - loss: 0.6623 - acc: 0.62 - ETA: 19s - loss: 0.6597 - acc: 0.62 - ETA: 18s - loss: 0.6574 - acc: 0.63 - ETA: 17s - loss: 0.6552 - acc: 0.63 - ETA: 16s - loss: 0.6543 - acc: 0.64 - ETA: 15s - loss: 0.6535 - acc: 0.64 - ETA: 14s - loss: 0.6527 - acc: 0.64 - ETA: 13s - loss: 0.6530 - acc: 0.64 - ETA: 12s - loss: 0.6513 - acc: 0.64 - ETA: 11s - loss: 0.6492 - acc: 0.65 - ETA: 10s - loss: 0.6487 - acc: 0.65 - ETA: 9s - loss: 0.6491 - acc: 0.6507 - ETA: 8s - loss: 0.6486 - acc: 0.651 - ETA: 7s - loss: 0.6493 - acc: 0.650 - ETA: 6s - loss: 0.6497 - acc: 0.649 - ETA: 5s - loss: 0.6492 - acc: 0.650 - ETA: 4s - loss: 0.6491 - acc: 0.650 - ETA: 3s - loss: 0.6498 - acc: 0.649 - ETA: 2s - loss: 0.6497 - acc: 0.649 - ETA: 1s - loss: 0.6504 - acc: 0.648 - ETA: 0s - loss: 0.6506 - acc: 0.647 - 47s 34ms/step - loss: 0.6510 - acc: 0.6467 - val_loss: 0.6316 - val_acc: 0.6859\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 44s - loss: 0.6308 - acc: 0.68 - ETA: 40s - loss: 0.6308 - acc: 0.68 - ETA: 45s - loss: 0.6205 - acc: 0.70 - ETA: 42s - loss: 0.6308 - acc: 0.68 - ETA: 42s - loss: 0.6246 - acc: 0.70 - ETA: 42s - loss: 0.6411 - acc: 0.66 - ETA: 39s - loss: 0.6462 - acc: 0.65 - ETA: 42s - loss: 0.6443 - acc: 0.66 - ETA: 41s - loss: 0.6462 - acc: 0.65 - ETA: 40s - loss: 0.6462 - acc: 0.65 - ETA: 38s - loss: 0.6476 - acc: 0.65 - ETA: 38s - loss: 0.6475 - acc: 0.65 - ETA: 37s - loss: 0.6426 - acc: 0.66 - ETA: 36s - loss: 0.6429 - acc: 0.66 - ETA: 34s - loss: 0.6493 - acc: 0.65 - ETA: 32s - loss: 0.6481 - acc: 0.65 - ETA: 31s - loss: 0.6480 - acc: 0.65 - ETA: 30s - loss: 0.6513 - acc: 0.64 - ETA: 28s - loss: 0.6511 - acc: 0.64 - ETA: 27s - loss: 0.6508 - acc: 0.64 - ETA: 25s - loss: 0.6521 - acc: 0.64 - ETA: 24s - loss: 0.6518 - acc: 0.64 - ETA: 23s - loss: 0.6536 - acc: 0.64 - ETA: 21s - loss: 0.6520 - acc: 0.64 - ETA: 20s - loss: 0.6523 - acc: 0.64 - ETA: 19s - loss: 0.6545 - acc: 0.63 - ETA: 18s - loss: 0.6519 - acc: 0.64 - ETA: 17s - loss: 0.6511 - acc: 0.64 - ETA: 16s - loss: 0.6526 - acc: 0.64 - ETA: 14s - loss: 0.6534 - acc: 0.64 - ETA: 13s - loss: 0.6531 - acc: 0.64 - ETA: 12s - loss: 0.6544 - acc: 0.63 - ETA: 11s - loss: 0.6532 - acc: 0.64 - ETA: 10s - loss: 0.6543 - acc: 0.63 - ETA: 9s - loss: 0.6545 - acc: 0.6393 - ETA: 7s - loss: 0.6522 - acc: 0.644 - ETA: 6s - loss: 0.6520 - acc: 0.644 - ETA: 5s - loss: 0.6527 - acc: 0.643 - ETA: 4s - loss: 0.6537 - acc: 0.641 - ETA: 3s - loss: 0.6515 - acc: 0.645 - ETA: 2s - loss: 0.6510 - acc: 0.646 - ETA: 1s - loss: 0.6502 - acc: 0.648 - ETA: 0s - loss: 0.6508 - acc: 0.646 - 49s 35ms/step - loss: 0.6508 - acc: 0.6467 - val_loss: 0.6313 - val_acc: 0.6859\n",
      "0.49 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37] 0.32469775474956825 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37] 0.6753022452504318 [0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "design\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 40s - loss: 0.5994 - acc: 0.75 - ETA: 43s - loss: 0.5994 - acc: 0.75 - ETA: 40s - loss: 0.5786 - acc: 0.79 - ETA: 38s - loss: 0.5799 - acc: 0.78 - ETA: 38s - loss: 0.5775 - acc: 0.79 - ETA: 36s - loss: 0.5837 - acc: 0.78 - ETA: 35s - loss: 0.5837 - acc: 0.78 - ETA: 35s - loss: 0.5778 - acc: 0.79 - ETA: 33s - loss: 0.5767 - acc: 0.79 - ETA: 32s - loss: 0.5726 - acc: 0.80 - ETA: 32s - loss: 0.5779 - acc: 0.79 - ETA: 30s - loss: 0.5757 - acc: 0.79 - ETA: 29s - loss: 0.5738 - acc: 0.80 - ETA: 28s - loss: 0.5756 - acc: 0.79 - ETA: 27s - loss: 0.5760 - acc: 0.79 - ETA: 26s - loss: 0.5755 - acc: 0.79 - ETA: 25s - loss: 0.5759 - acc: 0.79 - ETA: 24s - loss: 0.5780 - acc: 0.79 - ETA: 23s - loss: 0.5757 - acc: 0.79 - ETA: 23s - loss: 0.5744 - acc: 0.79 - ETA: 21s - loss: 0.5740 - acc: 0.79 - ETA: 20s - loss: 0.5751 - acc: 0.79 - ETA: 19s - loss: 0.5775 - acc: 0.79 - ETA: 18s - loss: 0.5783 - acc: 0.79 - ETA: 17s - loss: 0.5778 - acc: 0.79 - ETA: 17s - loss: 0.5767 - acc: 0.79 - ETA: 16s - loss: 0.5763 - acc: 0.79 - ETA: 15s - loss: 0.5764 - acc: 0.79 - ETA: 14s - loss: 0.5772 - acc: 0.79 - ETA: 13s - loss: 0.5767 - acc: 0.79 - ETA: 12s - loss: 0.5769 - acc: 0.79 - ETA: 11s - loss: 0.5755 - acc: 0.79 - ETA: 10s - loss: 0.5746 - acc: 0.79 - ETA: 9s - loss: 0.5757 - acc: 0.7941 - ETA: 8s - loss: 0.5768 - acc: 0.792 - ETA: 7s - loss: 0.5773 - acc: 0.790 - ETA: 6s - loss: 0.5765 - acc: 0.792 - ETA: 5s - loss: 0.5758 - acc: 0.793 - ETA: 4s - loss: 0.5754 - acc: 0.794 - ETA: 3s - loss: 0.5755 - acc: 0.793 - ETA: 2s - loss: 0.5772 - acc: 0.790 - ETA: 1s - loss: 0.5761 - acc: 0.792 - ETA: 0s - loss: 0.5762 - acc: 0.792 - 45s 32ms/step - loss: 0.5768 - acc: 0.7909 - val_loss: 0.5653 - val_acc: 0.8069\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 47s - loss: 0.5624 - acc: 0.81 - ETA: 41s - loss: 0.5871 - acc: 0.76 - ETA: 43s - loss: 0.6175 - acc: 0.70 - ETA: 40s - loss: 0.6036 - acc: 0.73 - ETA: 38s - loss: 0.5986 - acc: 0.74 - ETA: 36s - loss: 0.6035 - acc: 0.73 - ETA: 36s - loss: 0.5880 - acc: 0.76 - ETA: 34s - loss: 0.5847 - acc: 0.76 - ETA: 34s - loss: 0.5877 - acc: 0.76 - ETA: 33s - loss: 0.5817 - acc: 0.77 - ETA: 32s - loss: 0.5783 - acc: 0.78 - ETA: 31s - loss: 0.5754 - acc: 0.78 - ETA: 30s - loss: 0.5808 - acc: 0.77 - ETA: 30s - loss: 0.5781 - acc: 0.78 - ETA: 28s - loss: 0.5803 - acc: 0.77 - ETA: 27s - loss: 0.5801 - acc: 0.77 - ETA: 26s - loss: 0.5780 - acc: 0.78 - ETA: 25s - loss: 0.5779 - acc: 0.78 - ETA: 24s - loss: 0.5779 - acc: 0.78 - ETA: 23s - loss: 0.5752 - acc: 0.78 - ETA: 22s - loss: 0.5753 - acc: 0.78 - ETA: 21s - loss: 0.5761 - acc: 0.78 - ETA: 20s - loss: 0.5754 - acc: 0.78 - ETA: 19s - loss: 0.5762 - acc: 0.78 - ETA: 18s - loss: 0.5762 - acc: 0.78 - ETA: 17s - loss: 0.5755 - acc: 0.78 - ETA: 16s - loss: 0.5761 - acc: 0.78 - ETA: 15s - loss: 0.5755 - acc: 0.78 - ETA: 14s - loss: 0.5755 - acc: 0.78 - ETA: 13s - loss: 0.5732 - acc: 0.78 - ETA: 12s - loss: 0.5750 - acc: 0.78 - ETA: 11s - loss: 0.5744 - acc: 0.78 - ETA: 10s - loss: 0.5750 - acc: 0.78 - ETA: 9s - loss: 0.5750 - acc: 0.7849 - ETA: 8s - loss: 0.5750 - acc: 0.784 - ETA: 7s - loss: 0.5740 - acc: 0.786 - ETA: 6s - loss: 0.5759 - acc: 0.782 - ETA: 5s - loss: 0.5754 - acc: 0.783 - ETA: 4s - loss: 0.5750 - acc: 0.784 - ETA: 3s - loss: 0.5732 - acc: 0.787 - ETA: 2s - loss: 0.5724 - acc: 0.788 - ETA: 1s - loss: 0.5716 - acc: 0.790 - ETA: 0s - loss: 0.5712 - acc: 0.790 - 45s 33ms/step - loss: 0.5711 - acc: 0.7909 - val_loss: 0.5593 - val_acc: 0.8069\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 37s - loss: 0.5033 - acc: 0.90 - ETA: 40s - loss: 0.5473 - acc: 0.82 - ETA: 38s - loss: 0.5796 - acc: 0.77 - ETA: 39s - loss: 0.5736 - acc: 0.78 - ETA: 37s - loss: 0.5771 - acc: 0.77 - ETA: 35s - loss: 0.5824 - acc: 0.76 - ETA: 34s - loss: 0.5836 - acc: 0.76 - ETA: 34s - loss: 0.5867 - acc: 0.75 - ETA: 32s - loss: 0.5851 - acc: 0.76 - ETA: 32s - loss: 0.5821 - acc: 0.76 - ETA: 31s - loss: 0.5845 - acc: 0.76 - ETA: 30s - loss: 0.5865 - acc: 0.75 - ETA: 29s - loss: 0.5785 - acc: 0.77 - ETA: 28s - loss: 0.5794 - acc: 0.77 - ETA: 27s - loss: 0.5825 - acc: 0.76 - ETA: 26s - loss: 0.5807 - acc: 0.76 - ETA: 25s - loss: 0.5770 - acc: 0.77 - ETA: 24s - loss: 0.5767 - acc: 0.77 - ETA: 23s - loss: 0.5774 - acc: 0.77 - ETA: 22s - loss: 0.5753 - acc: 0.77 - ETA: 21s - loss: 0.5734 - acc: 0.77 - ETA: 20s - loss: 0.5700 - acc: 0.78 - ETA: 19s - loss: 0.5677 - acc: 0.78 - ETA: 18s - loss: 0.5679 - acc: 0.78 - ETA: 17s - loss: 0.5680 - acc: 0.78 - ETA: 16s - loss: 0.5681 - acc: 0.78 - ETA: 15s - loss: 0.5675 - acc: 0.78 - ETA: 14s - loss: 0.5663 - acc: 0.79 - ETA: 13s - loss: 0.5671 - acc: 0.78 - ETA: 12s - loss: 0.5672 - acc: 0.78 - ETA: 11s - loss: 0.5679 - acc: 0.78 - ETA: 10s - loss: 0.5686 - acc: 0.78 - ETA: 9s - loss: 0.5664 - acc: 0.7907 - ETA: 9s - loss: 0.5665 - acc: 0.790 - ETA: 8s - loss: 0.5666 - acc: 0.790 - ETA: 7s - loss: 0.5677 - acc: 0.788 - ETA: 6s - loss: 0.5693 - acc: 0.785 - ETA: 5s - loss: 0.5693 - acc: 0.785 - ETA: 4s - loss: 0.5683 - acc: 0.786 - ETA: 3s - loss: 0.5665 - acc: 0.789 - ETA: 2s - loss: 0.5648 - acc: 0.792 - ETA: 1s - loss: 0.5644 - acc: 0.793 - ETA: 0s - loss: 0.5658 - acc: 0.790 - 44s 32ms/step - loss: 0.5657 - acc: 0.7909 - val_loss: 0.5535 - val_acc: 0.8069\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.5876 - acc: 0.75 - ETA: 36s - loss: 0.5782 - acc: 0.76 - ETA: 36s - loss: 0.5750 - acc: 0.77 - ETA: 36s - loss: 0.5593 - acc: 0.79 - ETA: 35s - loss: 0.5498 - acc: 0.81 - ETA: 36s - loss: 0.5529 - acc: 0.80 - ETA: 34s - loss: 0.5443 - acc: 0.82 - ETA: 33s - loss: 0.5520 - acc: 0.80 - ETA: 32s - loss: 0.5538 - acc: 0.80 - ETA: 31s - loss: 0.5571 - acc: 0.80 - ETA: 30s - loss: 0.5563 - acc: 0.80 - ETA: 30s - loss: 0.5573 - acc: 0.79 - ETA: 29s - loss: 0.5595 - acc: 0.79 - ETA: 28s - loss: 0.5601 - acc: 0.79 - ETA: 27s - loss: 0.5555 - acc: 0.80 - ETA: 26s - loss: 0.5562 - acc: 0.80 - ETA: 25s - loss: 0.5535 - acc: 0.80 - ETA: 24s - loss: 0.5532 - acc: 0.80 - ETA: 23s - loss: 0.5569 - acc: 0.79 - ETA: 22s - loss: 0.5584 - acc: 0.79 - ETA: 21s - loss: 0.5634 - acc: 0.78 - ETA: 20s - loss: 0.5617 - acc: 0.79 - ETA: 19s - loss: 0.5594 - acc: 0.79 - ETA: 18s - loss: 0.5581 - acc: 0.79 - ETA: 17s - loss: 0.5608 - acc: 0.79 - ETA: 16s - loss: 0.5602 - acc: 0.79 - ETA: 15s - loss: 0.5576 - acc: 0.79 - ETA: 14s - loss: 0.5593 - acc: 0.79 - ETA: 13s - loss: 0.5595 - acc: 0.79 - ETA: 12s - loss: 0.5591 - acc: 0.79 - ETA: 11s - loss: 0.5605 - acc: 0.79 - ETA: 10s - loss: 0.5589 - acc: 0.79 - ETA: 9s - loss: 0.5591 - acc: 0.7945 - ETA: 8s - loss: 0.5593 - acc: 0.794 - ETA: 8s - loss: 0.5617 - acc: 0.790 - ETA: 7s - loss: 0.5629 - acc: 0.788 - ETA: 6s - loss: 0.5624 - acc: 0.788 - ETA: 5s - loss: 0.5614 - acc: 0.790 - ETA: 4s - loss: 0.5620 - acc: 0.789 - ETA: 3s - loss: 0.5601 - acc: 0.792 - ETA: 2s - loss: 0.5593 - acc: 0.793 - ETA: 1s - loss: 0.5594 - acc: 0.793 - ETA: 0s - loss: 0.5609 - acc: 0.790 - 44s 32ms/step - loss: 0.5607 - acc: 0.7909 - val_loss: 0.5482 - val_acc: 0.8069\n",
      "0.34 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34] 0.20725388601036268 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34] 0.7927461139896373 [0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "business\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 40s - loss: 0.5644 - acc: 0.78 - ETA: 41s - loss: 0.5050 - acc: 0.87 - ETA: 40s - loss: 0.5247 - acc: 0.84 - ETA: 39s - loss: 0.5247 - acc: 0.84 - ETA: 37s - loss: 0.5206 - acc: 0.85 - ETA: 36s - loss: 0.5146 - acc: 0.85 - ETA: 34s - loss: 0.5188 - acc: 0.85 - ETA: 34s - loss: 0.5144 - acc: 0.85 - ETA: 34s - loss: 0.5154 - acc: 0.85 - ETA: 33s - loss: 0.5202 - acc: 0.85 - ETA: 31s - loss: 0.5223 - acc: 0.84 - ETA: 30s - loss: 0.5224 - acc: 0.84 - ETA: 29s - loss: 0.5193 - acc: 0.85 - ETA: 28s - loss: 0.5268 - acc: 0.83 - ETA: 28s - loss: 0.5305 - acc: 0.83 - ETA: 26s - loss: 0.5300 - acc: 0.83 - ETA: 25s - loss: 0.5295 - acc: 0.83 - ETA: 24s - loss: 0.5303 - acc: 0.83 - ETA: 23s - loss: 0.5309 - acc: 0.83 - ETA: 22s - loss: 0.5314 - acc: 0.83 - ETA: 21s - loss: 0.5281 - acc: 0.83 - ETA: 20s - loss: 0.5278 - acc: 0.83 - ETA: 19s - loss: 0.5284 - acc: 0.83 - ETA: 19s - loss: 0.5255 - acc: 0.83 - ETA: 18s - loss: 0.5302 - acc: 0.83 - ETA: 17s - loss: 0.5299 - acc: 0.83 - ETA: 16s - loss: 0.5273 - acc: 0.83 - ETA: 15s - loss: 0.5255 - acc: 0.83 - ETA: 14s - loss: 0.5296 - acc: 0.83 - ETA: 13s - loss: 0.5272 - acc: 0.83 - ETA: 12s - loss: 0.5250 - acc: 0.83 - ETA: 11s - loss: 0.5229 - acc: 0.84 - ETA: 10s - loss: 0.5216 - acc: 0.84 - ETA: 9s - loss: 0.5203 - acc: 0.8465 - ETA: 8s - loss: 0.5191 - acc: 0.848 - ETA: 7s - loss: 0.5185 - acc: 0.849 - ETA: 6s - loss: 0.5174 - acc: 0.850 - ETA: 5s - loss: 0.5190 - acc: 0.847 - ETA: 4s - loss: 0.5174 - acc: 0.850 - ETA: 3s - loss: 0.5169 - acc: 0.850 - ETA: 2s - loss: 0.5190 - acc: 0.847 - ETA: 1s - loss: 0.5184 - acc: 0.848 - ETA: 0s - loss: 0.5174 - acc: 0.849 - 45s 33ms/step - loss: 0.5181 - acc: 0.8486 - val_loss: 0.5032 - val_acc: 0.8646\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.6014 - acc: 0.71 - ETA: 37s - loss: 0.5803 - acc: 0.75 - ETA: 38s - loss: 0.5803 - acc: 0.75 - ETA: 39s - loss: 0.5750 - acc: 0.75 - ETA: 37s - loss: 0.5675 - acc: 0.76 - ETA: 35s - loss: 0.5590 - acc: 0.78 - ETA: 34s - loss: 0.5559 - acc: 0.78 - ETA: 33s - loss: 0.5510 - acc: 0.79 - ETA: 32s - loss: 0.5423 - acc: 0.80 - ETA: 32s - loss: 0.5439 - acc: 0.80 - ETA: 31s - loss: 0.5491 - acc: 0.79 - ETA: 30s - loss: 0.5428 - acc: 0.80 - ETA: 28s - loss: 0.5324 - acc: 0.81 - ETA: 27s - loss: 0.5266 - acc: 0.82 - ETA: 27s - loss: 0.5244 - acc: 0.83 - ETA: 26s - loss: 0.5184 - acc: 0.83 - ETA: 25s - loss: 0.5144 - acc: 0.84 - ETA: 24s - loss: 0.5132 - acc: 0.84 - ETA: 23s - loss: 0.5122 - acc: 0.84 - ETA: 22s - loss: 0.5166 - acc: 0.84 - ETA: 21s - loss: 0.5175 - acc: 0.84 - ETA: 20s - loss: 0.5173 - acc: 0.84 - ETA: 19s - loss: 0.5181 - acc: 0.83 - ETA: 18s - loss: 0.5170 - acc: 0.84 - ETA: 17s - loss: 0.5142 - acc: 0.84 - ETA: 16s - loss: 0.5125 - acc: 0.84 - ETA: 15s - loss: 0.5133 - acc: 0.84 - ETA: 14s - loss: 0.5149 - acc: 0.84 - ETA: 13s - loss: 0.5125 - acc: 0.84 - ETA: 12s - loss: 0.5103 - acc: 0.85 - ETA: 11s - loss: 0.5089 - acc: 0.85 - ETA: 10s - loss: 0.5090 - acc: 0.85 - ETA: 9s - loss: 0.5084 - acc: 0.8523 - ETA: 9s - loss: 0.5092 - acc: 0.851 - ETA: 8s - loss: 0.5124 - acc: 0.846 - ETA: 7s - loss: 0.5117 - acc: 0.847 - ETA: 6s - loss: 0.5099 - acc: 0.849 - ETA: 5s - loss: 0.5100 - acc: 0.849 - ETA: 4s - loss: 0.5117 - acc: 0.847 - ETA: 3s - loss: 0.5105 - acc: 0.848 - ETA: 2s - loss: 0.5116 - acc: 0.846 - ETA: 1s - loss: 0.5105 - acc: 0.848 - ETA: 0s - loss: 0.5100 - acc: 0.848 - 44s 32ms/step - loss: 0.5101 - acc: 0.8486 - val_loss: 0.4948 - val_acc: 0.8646\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 37s - loss: 0.4201 - acc: 0.96 - ETA: 36s - loss: 0.4758 - acc: 0.89 - ETA: 35s - loss: 0.4784 - acc: 0.88 - ETA: 35s - loss: 0.4803 - acc: 0.88 - ETA: 35s - loss: 0.4992 - acc: 0.85 - ETA: 35s - loss: 0.5020 - acc: 0.84 - ETA: 34s - loss: 0.4988 - acc: 0.85 - ETA: 33s - loss: 0.4945 - acc: 0.85 - ETA: 32s - loss: 0.4925 - acc: 0.85 - ETA: 31s - loss: 0.4797 - acc: 0.85 - ETA: 30s - loss: 0.4818 - acc: 0.85 - ETA: 30s - loss: 0.4696 - acc: 0.85 - ETA: 29s - loss: 0.4676 - acc: 0.85 - ETA: 28s - loss: 0.4602 - acc: 0.85 - ETA: 27s - loss: 0.4689 - acc: 0.85 - ETA: 25s - loss: 0.4617 - acc: 0.85 - ETA: 25s - loss: 0.4718 - acc: 0.84 - ETA: 24s - loss: 0.4799 - acc: 0.84 - ETA: 23s - loss: 0.4732 - acc: 0.84 - ETA: 22s - loss: 0.4616 - acc: 0.85 - ETA: 21s - loss: 0.4581 - acc: 0.85 - ETA: 20s - loss: 0.4585 - acc: 0.85 - ETA: 19s - loss: 0.4578 - acc: 0.85 - ETA: 18s - loss: 0.4620 - acc: 0.84 - ETA: 17s - loss: 0.4613 - acc: 0.84 - ETA: 16s - loss: 0.4626 - acc: 0.84 - ETA: 15s - loss: 0.4622 - acc: 0.84 - ETA: 14s - loss: 0.4676 - acc: 0.84 - ETA: 13s - loss: 0.4730 - acc: 0.83 - ETA: 12s - loss: 0.4716 - acc: 0.83 - ETA: 11s - loss: 0.4667 - acc: 0.84 - ETA: 10s - loss: 0.4638 - acc: 0.84 - ETA: 9s - loss: 0.4617 - acc: 0.8438 - ETA: 8s - loss: 0.4625 - acc: 0.842 - ETA: 8s - loss: 0.4618 - acc: 0.842 - ETA: 7s - loss: 0.4598 - acc: 0.843 - ETA: 6s - loss: 0.4552 - acc: 0.846 - ETA: 5s - loss: 0.4544 - acc: 0.846 - ETA: 4s - loss: 0.4570 - acc: 0.844 - ETA: 3s - loss: 0.4510 - acc: 0.847 - ETA: 2s - loss: 0.4538 - acc: 0.846 - ETA: 1s - loss: 0.4495 - acc: 0.848 - ETA: 0s - loss: 0.4504 - acc: 0.847 - 44s 32ms/step - loss: 0.4479 - acc: 0.8486 - val_loss: 0.3979 - val_acc: 0.8646\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 46s - loss: 0.6264 - acc: 0.75 - ETA: 46s - loss: 0.4779 - acc: 0.82 - ETA: 42s - loss: 0.5116 - acc: 0.81 - ETA: 39s - loss: 0.4809 - acc: 0.82 - ETA: 37s - loss: 0.4332 - acc: 0.85 - ETA: 36s - loss: 0.4252 - acc: 0.85 - ETA: 35s - loss: 0.4217 - acc: 0.85 - ETA: 35s - loss: 0.4451 - acc: 0.84 - ETA: 34s - loss: 0.4557 - acc: 0.83 - ETA: 32s - loss: 0.4622 - acc: 0.83 - ETA: 31s - loss: 0.4559 - acc: 0.83 - ETA: 30s - loss: 0.4523 - acc: 0.83 - ETA: 29s - loss: 0.4433 - acc: 0.84 - ETA: 28s - loss: 0.4424 - acc: 0.84 - ETA: 27s - loss: 0.4348 - acc: 0.84 - ETA: 26s - loss: 0.4444 - acc: 0.83 - ETA: 25s - loss: 0.4436 - acc: 0.84 - ETA: 24s - loss: 0.4540 - acc: 0.83 - ETA: 23s - loss: 0.4565 - acc: 0.83 - ETA: 22s - loss: 0.4560 - acc: 0.83 - ETA: 21s - loss: 0.4565 - acc: 0.83 - ETA: 20s - loss: 0.4494 - acc: 0.83 - ETA: 19s - loss: 0.4475 - acc: 0.83 - ETA: 18s - loss: 0.4457 - acc: 0.84 - ETA: 17s - loss: 0.4436 - acc: 0.84 - ETA: 16s - loss: 0.4417 - acc: 0.84 - ETA: 15s - loss: 0.4343 - acc: 0.84 - ETA: 14s - loss: 0.4291 - acc: 0.85 - ETA: 13s - loss: 0.4328 - acc: 0.84 - ETA: 12s - loss: 0.4294 - acc: 0.85 - ETA: 11s - loss: 0.4297 - acc: 0.85 - ETA: 11s - loss: 0.4336 - acc: 0.84 - ETA: 10s - loss: 0.4334 - acc: 0.84 - ETA: 9s - loss: 0.4333 - acc: 0.8483 - ETA: 8s - loss: 0.4321 - acc: 0.849 - ETA: 7s - loss: 0.4305 - acc: 0.849 - ETA: 6s - loss: 0.4315 - acc: 0.849 - ETA: 5s - loss: 0.4254 - acc: 0.852 - ETA: 4s - loss: 0.4285 - acc: 0.851 - ETA: 3s - loss: 0.4299 - acc: 0.850 - ETA: 2s - loss: 0.4272 - acc: 0.851 - ETA: 1s - loss: 0.4298 - acc: 0.849 - ETA: 0s - loss: 0.4328 - acc: 0.848 - 45s 32ms/step - loss: 0.4320 - acc: 0.8486 - val_loss: 0.3947 - val_acc: 0.8646\n",
      "0.32 [0.16] 0.6010362694300518 [0.16] 0.8411053540587219 [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "biomechanics\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 38s - loss: 0.3810 - acc: 0.87 - ETA: 37s - loss: 0.3557 - acc: 0.89 - ETA: 36s - loss: 0.3654 - acc: 0.88 - ETA: 37s - loss: 0.3546 - acc: 0.89 - ETA: 37s - loss: 0.3404 - acc: 0.90 - ETA: 36s - loss: 0.3357 - acc: 0.90 - ETA: 35s - loss: 0.3355 - acc: 0.90 - ETA: 33s - loss: 0.3586 - acc: 0.89 - ETA: 32s - loss: 0.3548 - acc: 0.89 - ETA: 32s - loss: 0.3398 - acc: 0.90 - ETA: 31s - loss: 0.3279 - acc: 0.90 - ETA: 30s - loss: 0.3371 - acc: 0.90 - ETA: 29s - loss: 0.3316 - acc: 0.90 - ETA: 28s - loss: 0.3412 - acc: 0.89 - ETA: 27s - loss: 0.3454 - acc: 0.89 - ETA: 26s - loss: 0.3343 - acc: 0.90 - ETA: 25s - loss: 0.3328 - acc: 0.90 - ETA: 24s - loss: 0.3242 - acc: 0.90 - ETA: 23s - loss: 0.3240 - acc: 0.90 - ETA: 22s - loss: 0.3285 - acc: 0.90 - ETA: 21s - loss: 0.3285 - acc: 0.90 - ETA: 20s - loss: 0.3342 - acc: 0.90 - ETA: 19s - loss: 0.3372 - acc: 0.89 - ETA: 18s - loss: 0.3335 - acc: 0.90 - ETA: 17s - loss: 0.3356 - acc: 0.90 - ETA: 16s - loss: 0.3351 - acc: 0.90 - ETA: 15s - loss: 0.3338 - acc: 0.90 - ETA: 15s - loss: 0.3309 - acc: 0.90 - ETA: 14s - loss: 0.3254 - acc: 0.90 - ETA: 13s - loss: 0.3281 - acc: 0.90 - ETA: 12s - loss: 0.3228 - acc: 0.90 - ETA: 11s - loss: 0.3224 - acc: 0.90 - ETA: 10s - loss: 0.3290 - acc: 0.90 - ETA: 9s - loss: 0.3282 - acc: 0.9026 - ETA: 8s - loss: 0.3305 - acc: 0.901 - ETA: 7s - loss: 0.3281 - acc: 0.902 - ETA: 6s - loss: 0.3280 - acc: 0.902 - ETA: 5s - loss: 0.3273 - acc: 0.903 - ETA: 4s - loss: 0.3237 - acc: 0.904 - ETA: 3s - loss: 0.3265 - acc: 0.903 - ETA: 2s - loss: 0.3265 - acc: 0.903 - ETA: 1s - loss: 0.3265 - acc: 0.903 - ETA: 0s - loss: 0.3280 - acc: 0.902 - 45s 32ms/step - loss: 0.3264 - acc: 0.9034 - val_loss: 0.2950 - val_acc: 0.9164\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 39s - loss: 0.5613 - acc: 0.78 - ETA: 37s - loss: 0.4401 - acc: 0.84 - ETA: 36s - loss: 0.4168 - acc: 0.85 - ETA: 35s - loss: 0.4022 - acc: 0.85 - ETA: 34s - loss: 0.3856 - acc: 0.86 - ETA: 36s - loss: 0.3527 - acc: 0.88 - ETA: 35s - loss: 0.3364 - acc: 0.89 - ETA: 33s - loss: 0.3350 - acc: 0.89 - ETA: 32s - loss: 0.3248 - acc: 0.89 - ETA: 31s - loss: 0.3447 - acc: 0.89 - ETA: 30s - loss: 0.3367 - acc: 0.89 - ETA: 30s - loss: 0.3352 - acc: 0.89 - ETA: 29s - loss: 0.3336 - acc: 0.89 - ETA: 28s - loss: 0.3309 - acc: 0.89 - ETA: 27s - loss: 0.3262 - acc: 0.90 - ETA: 25s - loss: 0.3330 - acc: 0.89 - ETA: 24s - loss: 0.3342 - acc: 0.89 - ETA: 24s - loss: 0.3249 - acc: 0.89 - ETA: 23s - loss: 0.3208 - acc: 0.90 - ETA: 22s - loss: 0.3211 - acc: 0.90 - ETA: 21s - loss: 0.3172 - acc: 0.90 - ETA: 20s - loss: 0.3067 - acc: 0.90 - ETA: 19s - loss: 0.3044 - acc: 0.90 - ETA: 18s - loss: 0.3049 - acc: 0.90 - ETA: 17s - loss: 0.3013 - acc: 0.91 - ETA: 16s - loss: 0.3049 - acc: 0.90 - ETA: 15s - loss: 0.3110 - acc: 0.90 - ETA: 14s - loss: 0.3087 - acc: 0.90 - ETA: 13s - loss: 0.3205 - acc: 0.90 - ETA: 12s - loss: 0.3206 - acc: 0.90 - ETA: 11s - loss: 0.3205 - acc: 0.90 - ETA: 10s - loss: 0.3254 - acc: 0.90 - ETA: 9s - loss: 0.3275 - acc: 0.9006 - ETA: 8s - loss: 0.3266 - acc: 0.900 - ETA: 7s - loss: 0.3331 - acc: 0.897 - ETA: 7s - loss: 0.3292 - acc: 0.899 - ETA: 6s - loss: 0.3273 - acc: 0.900 - ETA: 5s - loss: 0.3253 - acc: 0.901 - ETA: 4s - loss: 0.3235 - acc: 0.902 - ETA: 3s - loss: 0.3248 - acc: 0.901 - ETA: 2s - loss: 0.3249 - acc: 0.901 - ETA: 1s - loss: 0.3244 - acc: 0.901 - ETA: 0s - loss: 0.3230 - acc: 0.902 - 44s 32ms/step - loss: 0.3214 - acc: 0.9034 - val_loss: 0.2965 - val_acc: 0.9164\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.4424 - acc: 0.84 - ETA: 44s - loss: 0.3181 - acc: 0.90 - ETA: 41s - loss: 0.2939 - acc: 0.91 - ETA: 38s - loss: 0.3476 - acc: 0.89 - ETA: 37s - loss: 0.3373 - acc: 0.89 - ETA: 36s - loss: 0.3216 - acc: 0.90 - ETA: 35s - loss: 0.3096 - acc: 0.90 - ETA: 35s - loss: 0.3108 - acc: 0.90 - ETA: 34s - loss: 0.3113 - acc: 0.90 - ETA: 32s - loss: 0.3036 - acc: 0.90 - ETA: 31s - loss: 0.3043 - acc: 0.90 - ETA: 30s - loss: 0.2987 - acc: 0.91 - ETA: 29s - loss: 0.3057 - acc: 0.90 - ETA: 28s - loss: 0.2943 - acc: 0.91 - ETA: 27s - loss: 0.2958 - acc: 0.91 - ETA: 26s - loss: 0.3125 - acc: 0.90 - ETA: 25s - loss: 0.3124 - acc: 0.90 - ETA: 24s - loss: 0.3082 - acc: 0.90 - ETA: 23s - loss: 0.3237 - acc: 0.90 - ETA: 22s - loss: 0.3194 - acc: 0.90 - ETA: 21s - loss: 0.3306 - acc: 0.89 - ETA: 20s - loss: 0.3287 - acc: 0.89 - ETA: 19s - loss: 0.3325 - acc: 0.89 - ETA: 18s - loss: 0.3262 - acc: 0.89 - ETA: 17s - loss: 0.3229 - acc: 0.90 - ETA: 16s - loss: 0.3173 - acc: 0.90 - ETA: 15s - loss: 0.3169 - acc: 0.90 - ETA: 14s - loss: 0.3120 - acc: 0.90 - ETA: 13s - loss: 0.3216 - acc: 0.90 - ETA: 12s - loss: 0.3187 - acc: 0.90 - ETA: 11s - loss: 0.3254 - acc: 0.90 - ETA: 11s - loss: 0.3230 - acc: 0.90 - ETA: 10s - loss: 0.3262 - acc: 0.89 - ETA: 9s - loss: 0.3219 - acc: 0.9017 - ETA: 8s - loss: 0.3195 - acc: 0.902 - ETA: 7s - loss: 0.3189 - acc: 0.902 - ETA: 6s - loss: 0.3225 - acc: 0.901 - ETA: 5s - loss: 0.3168 - acc: 0.903 - ETA: 4s - loss: 0.3164 - acc: 0.903 - ETA: 3s - loss: 0.3159 - acc: 0.903 - ETA: 2s - loss: 0.3188 - acc: 0.902 - ETA: 1s - loss: 0.3156 - acc: 0.904 - ETA: 0s - loss: 0.3172 - acc: 0.903 - 44s 32ms/step - loss: 0.3170 - acc: 0.9034 - val_loss: 0.2869 - val_acc: 0.9164\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.4441 - acc: 0.84 - ETA: 37s - loss: 0.3097 - acc: 0.90 - ETA: 36s - loss: 0.3645 - acc: 0.88 - ETA: 39s - loss: 0.3303 - acc: 0.89 - ETA: 37s - loss: 0.3289 - acc: 0.90 - ETA: 36s - loss: 0.3504 - acc: 0.89 - ETA: 34s - loss: 0.3212 - acc: 0.90 - ETA: 34s - loss: 0.3236 - acc: 0.89 - ETA: 34s - loss: 0.3223 - acc: 0.89 - ETA: 33s - loss: 0.3057 - acc: 0.90 - ETA: 32s - loss: 0.3122 - acc: 0.90 - ETA: 31s - loss: 0.2999 - acc: 0.90 - ETA: 29s - loss: 0.2894 - acc: 0.91 - ETA: 28s - loss: 0.2863 - acc: 0.91 - ETA: 28s - loss: 0.2769 - acc: 0.91 - ETA: 27s - loss: 0.2830 - acc: 0.91 - ETA: 26s - loss: 0.2763 - acc: 0.91 - ETA: 25s - loss: 0.3003 - acc: 0.90 - ETA: 24s - loss: 0.2932 - acc: 0.91 - ETA: 23s - loss: 0.2978 - acc: 0.91 - ETA: 22s - loss: 0.2935 - acc: 0.91 - ETA: 21s - loss: 0.2881 - acc: 0.91 - ETA: 20s - loss: 0.2954 - acc: 0.91 - ETA: 19s - loss: 0.2860 - acc: 0.91 - ETA: 18s - loss: 0.2965 - acc: 0.91 - ETA: 17s - loss: 0.2995 - acc: 0.90 - ETA: 16s - loss: 0.3096 - acc: 0.90 - ETA: 15s - loss: 0.3069 - acc: 0.90 - ETA: 14s - loss: 0.3044 - acc: 0.90 - ETA: 13s - loss: 0.3070 - acc: 0.90 - ETA: 12s - loss: 0.3091 - acc: 0.90 - ETA: 11s - loss: 0.3131 - acc: 0.90 - ETA: 10s - loss: 0.3109 - acc: 0.90 - ETA: 9s - loss: 0.3067 - acc: 0.9062 - ETA: 8s - loss: 0.3059 - acc: 0.906 - ETA: 7s - loss: 0.3056 - acc: 0.906 - ETA: 6s - loss: 0.3093 - acc: 0.904 - ETA: 5s - loss: 0.3077 - acc: 0.905 - ETA: 4s - loss: 0.3104 - acc: 0.903 - ETA: 3s - loss: 0.3113 - acc: 0.903 - ETA: 2s - loss: 0.3141 - acc: 0.901 - ETA: 1s - loss: 0.3132 - acc: 0.902 - ETA: 0s - loss: 0.3116 - acc: 0.903 - 45s 32ms/step - loss: 0.3116 - acc: 0.9034 - val_loss: 0.2820 - val_acc: 0.9164\n",
      "0.24 [0.12] 0.5924006908462867 [0.12] 0.9015544041450777 [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "biodiversity\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 54s - loss: 0.3968 - acc: 0.87 - ETA: 45s - loss: 0.3167 - acc: 0.90 - ETA: 41s - loss: 0.4152 - acc: 0.85 - ETA: 39s - loss: 0.3716 - acc: 0.87 - ETA: 37s - loss: 0.3709 - acc: 0.87 - ETA: 36s - loss: 0.3701 - acc: 0.87 - ETA: 36s - loss: 0.3529 - acc: 0.88 - ETA: 35s - loss: 0.3393 - acc: 0.89 - ETA: 35s - loss: 0.3280 - acc: 0.89 - ETA: 33s - loss: 0.3204 - acc: 0.90 - ETA: 32s - loss: 0.3240 - acc: 0.89 - ETA: 31s - loss: 0.3218 - acc: 0.89 - ETA: 30s - loss: 0.3232 - acc: 0.89 - ETA: 29s - loss: 0.3217 - acc: 0.89 - ETA: 28s - loss: 0.3248 - acc: 0.89 - ETA: 27s - loss: 0.3314 - acc: 0.89 - ETA: 25s - loss: 0.3401 - acc: 0.88 - ETA: 24s - loss: 0.3397 - acc: 0.88 - ETA: 24s - loss: 0.3480 - acc: 0.88 - ETA: 23s - loss: 0.3424 - acc: 0.88 - ETA: 21s - loss: 0.3359 - acc: 0.89 - ETA: 20s - loss: 0.3338 - acc: 0.89 - ETA: 19s - loss: 0.3323 - acc: 0.89 - ETA: 18s - loss: 0.3314 - acc: 0.89 - ETA: 17s - loss: 0.3312 - acc: 0.89 - ETA: 16s - loss: 0.3312 - acc: 0.89 - ETA: 15s - loss: 0.3354 - acc: 0.89 - ETA: 14s - loss: 0.3362 - acc: 0.89 - ETA: 13s - loss: 0.3329 - acc: 0.89 - ETA: 12s - loss: 0.3299 - acc: 0.89 - ETA: 12s - loss: 0.3271 - acc: 0.89 - ETA: 11s - loss: 0.3249 - acc: 0.89 - ETA: 10s - loss: 0.3203 - acc: 0.89 - ETA: 9s - loss: 0.3214 - acc: 0.8989 - ETA: 8s - loss: 0.3213 - acc: 0.899 - ETA: 7s - loss: 0.3188 - acc: 0.900 - ETA: 6s - loss: 0.3225 - acc: 0.898 - ETA: 5s - loss: 0.3166 - acc: 0.901 - ETA: 4s - loss: 0.3213 - acc: 0.899 - ETA: 3s - loss: 0.3231 - acc: 0.898 - ETA: 2s - loss: 0.3211 - acc: 0.899 - ETA: 1s - loss: 0.3175 - acc: 0.901 - ETA: 0s - loss: 0.3179 - acc: 0.901 - 44s 32ms/step - loss: 0.3178 - acc: 0.9012 - val_loss: 0.2549 - val_acc: 0.9280\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 36s - loss: 0.1542 - acc: 0.96 - ETA: 44s - loss: 0.1823 - acc: 0.95 - ETA: 40s - loss: 0.2018 - acc: 0.94 - ETA: 38s - loss: 0.2590 - acc: 0.92 - ETA: 37s - loss: 0.2554 - acc: 0.92 - ETA: 35s - loss: 0.2822 - acc: 0.91 - ETA: 34s - loss: 0.2816 - acc: 0.91 - ETA: 35s - loss: 0.2896 - acc: 0.90 - ETA: 36s - loss: 0.3069 - acc: 0.89 - ETA: 35s - loss: 0.3192 - acc: 0.89 - ETA: 33s - loss: 0.3128 - acc: 0.89 - ETA: 32s - loss: 0.3011 - acc: 0.90 - ETA: 32s - loss: 0.3069 - acc: 0.90 - ETA: 30s - loss: 0.3055 - acc: 0.90 - ETA: 29s - loss: 0.3005 - acc: 0.90 - ETA: 28s - loss: 0.2995 - acc: 0.90 - ETA: 26s - loss: 0.2982 - acc: 0.90 - ETA: 25s - loss: 0.2944 - acc: 0.90 - ETA: 25s - loss: 0.2988 - acc: 0.90 - ETA: 23s - loss: 0.2964 - acc: 0.90 - ETA: 22s - loss: 0.2977 - acc: 0.90 - ETA: 21s - loss: 0.3068 - acc: 0.90 - ETA: 20s - loss: 0.3072 - acc: 0.90 - ETA: 19s - loss: 0.3063 - acc: 0.90 - ETA: 18s - loss: 0.3065 - acc: 0.90 - ETA: 17s - loss: 0.3061 - acc: 0.90 - ETA: 16s - loss: 0.3030 - acc: 0.90 - ETA: 15s - loss: 0.3061 - acc: 0.90 - ETA: 14s - loss: 0.3080 - acc: 0.90 - ETA: 13s - loss: 0.3068 - acc: 0.90 - ETA: 12s - loss: 0.3065 - acc: 0.90 - ETA: 11s - loss: 0.3042 - acc: 0.90 - ETA: 10s - loss: 0.3039 - acc: 0.90 - ETA: 9s - loss: 0.3061 - acc: 0.9017 - ETA: 8s - loss: 0.3057 - acc: 0.901 - ETA: 7s - loss: 0.3074 - acc: 0.901 - ETA: 6s - loss: 0.3079 - acc: 0.900 - ETA: 5s - loss: 0.3066 - acc: 0.901 - ETA: 4s - loss: 0.3041 - acc: 0.902 - ETA: 3s - loss: 0.3005 - acc: 0.903 - ETA: 2s - loss: 0.2994 - acc: 0.904 - ETA: 1s - loss: 0.3042 - acc: 0.903 - ETA: 0s - loss: 0.3069 - acc: 0.901 - 45s 32ms/step - loss: 0.3082 - acc: 0.9012 - val_loss: 0.2536 - val_acc: 0.9280\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 37s - loss: 0.1697 - acc: 0.96 - ETA: 36s - loss: 0.2261 - acc: 0.93 - ETA: 40s - loss: 0.2842 - acc: 0.91 - ETA: 38s - loss: 0.3142 - acc: 0.89 - ETA: 37s - loss: 0.3096 - acc: 0.90 - ETA: 35s - loss: 0.3349 - acc: 0.88 - ETA: 34s - loss: 0.3438 - acc: 0.87 - ETA: 33s - loss: 0.3393 - acc: 0.88 - ETA: 33s - loss: 0.3339 - acc: 0.88 - ETA: 32s - loss: 0.3585 - acc: 0.87 - ETA: 31s - loss: 0.3374 - acc: 0.88 - ETA: 29s - loss: 0.3333 - acc: 0.88 - ETA: 28s - loss: 0.3208 - acc: 0.89 - ETA: 27s - loss: 0.3273 - acc: 0.88 - ETA: 27s - loss: 0.3209 - acc: 0.89 - ETA: 26s - loss: 0.3279 - acc: 0.88 - ETA: 25s - loss: 0.3221 - acc: 0.88 - ETA: 24s - loss: 0.3212 - acc: 0.89 - ETA: 23s - loss: 0.3086 - acc: 0.89 - ETA: 22s - loss: 0.3121 - acc: 0.89 - ETA: 21s - loss: 0.3097 - acc: 0.89 - ETA: 20s - loss: 0.3106 - acc: 0.89 - ETA: 19s - loss: 0.3157 - acc: 0.89 - ETA: 18s - loss: 0.3062 - acc: 0.89 - ETA: 17s - loss: 0.3080 - acc: 0.89 - ETA: 17s - loss: 0.3017 - acc: 0.90 - ETA: 16s - loss: 0.3076 - acc: 0.89 - ETA: 15s - loss: 0.3016 - acc: 0.90 - ETA: 13s - loss: 0.2939 - acc: 0.90 - ETA: 13s - loss: 0.2966 - acc: 0.90 - ETA: 12s - loss: 0.3000 - acc: 0.90 - ETA: 11s - loss: 0.3115 - acc: 0.89 - ETA: 10s - loss: 0.3138 - acc: 0.89 - ETA: 9s - loss: 0.3140 - acc: 0.8961 - ETA: 8s - loss: 0.3139 - acc: 0.896 - ETA: 7s - loss: 0.3117 - acc: 0.897 - ETA: 6s - loss: 0.3093 - acc: 0.898 - ETA: 5s - loss: 0.3086 - acc: 0.898 - ETA: 4s - loss: 0.3072 - acc: 0.899 - ETA: 3s - loss: 0.3080 - acc: 0.899 - ETA: 2s - loss: 0.3057 - acc: 0.900 - ETA: 1s - loss: 0.3037 - acc: 0.901 - ETA: 0s - loss: 0.3051 - acc: 0.900 - 50s 36ms/step - loss: 0.3035 - acc: 0.9012 - val_loss: 0.2549 - val_acc: 0.9280\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 47s - loss: 0.2903 - acc: 0.90 - ETA: 46s - loss: 0.2669 - acc: 0.90 - ETA: 51s - loss: 0.2702 - acc: 0.90 - ETA: 48s - loss: 0.2558 - acc: 0.91 - ETA: 46s - loss: 0.2619 - acc: 0.91 - ETA: 45s - loss: 0.2475 - acc: 0.92 - ETA: 43s - loss: 0.2598 - acc: 0.91 - ETA: 44s - loss: 0.2788 - acc: 0.91 - ETA: 42s - loss: 0.2739 - acc: 0.91 - ETA: 40s - loss: 0.2752 - acc: 0.91 - ETA: 39s - loss: 0.2822 - acc: 0.90 - ETA: 39s - loss: 0.2944 - acc: 0.90 - ETA: 37s - loss: 0.3081 - acc: 0.89 - ETA: 36s - loss: 0.3010 - acc: 0.90 - ETA: 34s - loss: 0.2998 - acc: 0.90 - ETA: 33s - loss: 0.3095 - acc: 0.89 - ETA: 32s - loss: 0.3227 - acc: 0.89 - ETA: 31s - loss: 0.3299 - acc: 0.88 - ETA: 29s - loss: 0.3277 - acc: 0.88 - ETA: 28s - loss: 0.3343 - acc: 0.88 - ETA: 27s - loss: 0.3358 - acc: 0.88 - ETA: 26s - loss: 0.3367 - acc: 0.88 - ETA: 25s - loss: 0.3290 - acc: 0.88 - ETA: 23s - loss: 0.3299 - acc: 0.88 - ETA: 22s - loss: 0.3349 - acc: 0.88 - ETA: 21s - loss: 0.3347 - acc: 0.88 - ETA: 20s - loss: 0.3356 - acc: 0.88 - ETA: 18s - loss: 0.3315 - acc: 0.88 - ETA: 17s - loss: 0.3266 - acc: 0.89 - ETA: 16s - loss: 0.3220 - acc: 0.89 - ETA: 15s - loss: 0.3207 - acc: 0.89 - ETA: 14s - loss: 0.3226 - acc: 0.89 - ETA: 12s - loss: 0.3160 - acc: 0.89 - ETA: 11s - loss: 0.3099 - acc: 0.89 - ETA: 10s - loss: 0.3174 - acc: 0.89 - ETA: 9s - loss: 0.3163 - acc: 0.8958 - ETA: 7s - loss: 0.3113 - acc: 0.897 - ETA: 6s - loss: 0.3065 - acc: 0.899 - ETA: 5s - loss: 0.3053 - acc: 0.900 - ETA: 4s - loss: 0.3073 - acc: 0.899 - ETA: 2s - loss: 0.3047 - acc: 0.900 - ETA: 1s - loss: 0.3058 - acc: 0.899 - ETA: 0s - loss: 0.3013 - acc: 0.901 - 57s 41ms/step - loss: 0.3007 - acc: 0.9012 - val_loss: 0.2496 - val_acc: 0.9280\n",
      "0.2 [0.04] 0.4421416234887737 [0.04] 0.9032815198618307 [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "media\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 1:06 - loss: 0.3428 - acc: 0.875 - ETA: 55s - loss: 0.3267 - acc: 0.890 - ETA: 51s - loss: 0.3187 - acc: 0.89 - ETA: 48s - loss: 0.3484 - acc: 0.89 - ETA: 46s - loss: 0.3496 - acc: 0.88 - ETA: 47s - loss: 0.4130 - acc: 0.87 - ETA: 45s - loss: 0.4171 - acc: 0.87 - ETA: 43s - loss: 0.4101 - acc: 0.87 - ETA: 42s - loss: 0.4242 - acc: 0.87 - ETA: 42s - loss: 0.4247 - acc: 0.86 - ETA: 40s - loss: 0.4115 - acc: 0.87 - ETA: 39s - loss: 0.4063 - acc: 0.87 - ETA: 37s - loss: 0.4043 - acc: 0.87 - ETA: 36s - loss: 0.4075 - acc: 0.87 - ETA: 35s - loss: 0.3994 - acc: 0.87 - ETA: 34s - loss: 0.4014 - acc: 0.87 - ETA: 32s - loss: 0.4035 - acc: 0.87 - ETA: 31s - loss: 0.4008 - acc: 0.87 - ETA: 30s - loss: 0.3962 - acc: 0.87 - ETA: 29s - loss: 0.3905 - acc: 0.87 - ETA: 27s - loss: 0.3920 - acc: 0.87 - ETA: 26s - loss: 0.3901 - acc: 0.88 - ETA: 25s - loss: 0.3935 - acc: 0.88 - ETA: 24s - loss: 0.3944 - acc: 0.88 - ETA: 22s - loss: 0.3910 - acc: 0.88 - ETA: 21s - loss: 0.3910 - acc: 0.88 - ETA: 20s - loss: 0.3874 - acc: 0.88 - ETA: 19s - loss: 0.3865 - acc: 0.88 - ETA: 17s - loss: 0.3847 - acc: 0.88 - ETA: 16s - loss: 0.3808 - acc: 0.88 - ETA: 15s - loss: 0.3840 - acc: 0.88 - ETA: 13s - loss: 0.3857 - acc: 0.88 - ETA: 12s - loss: 0.3898 - acc: 0.88 - ETA: 11s - loss: 0.3889 - acc: 0.88 - ETA: 10s - loss: 0.3838 - acc: 0.88 - ETA: 9s - loss: 0.3870 - acc: 0.8811 - ETA: 7s - loss: 0.3850 - acc: 0.881 - ETA: 6s - loss: 0.3851 - acc: 0.881 - ETA: 5s - loss: 0.3878 - acc: 0.879 - ETA: 4s - loss: 0.3875 - acc: 0.879 - ETA: 2s - loss: 0.3853 - acc: 0.880 - ETA: 1s - loss: 0.3892 - acc: 0.878 - ETA: 0s - loss: 0.3873 - acc: 0.878 - 58s 42ms/step - loss: 0.3854 - acc: 0.8796 - val_loss: 0.3775 - val_acc: 0.8732\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 56s - loss: 0.1407 - acc: 1.00 - ETA: 51s - loss: 0.2266 - acc: 0.95 - ETA: 49s - loss: 0.2549 - acc: 0.93 - ETA: 54s - loss: 0.2996 - acc: 0.91 - ETA: 50s - loss: 0.3737 - acc: 0.88 - ETA: 48s - loss: 0.3637 - acc: 0.88 - ETA: 46s - loss: 0.3383 - acc: 0.89 - ETA: 46s - loss: 0.3361 - acc: 0.89 - ETA: 44s - loss: 0.3478 - acc: 0.89 - ETA: 42s - loss: 0.3455 - acc: 0.89 - ETA: 40s - loss: 0.3420 - acc: 0.89 - ETA: 39s - loss: 0.3511 - acc: 0.88 - ETA: 38s - loss: 0.3629 - acc: 0.88 - ETA: 37s - loss: 0.3495 - acc: 0.88 - ETA: 35s - loss: 0.3566 - acc: 0.88 - ETA: 34s - loss: 0.3485 - acc: 0.88 - ETA: 32s - loss: 0.3425 - acc: 0.89 - ETA: 32s - loss: 0.3505 - acc: 0.88 - ETA: 30s - loss: 0.3617 - acc: 0.88 - ETA: 29s - loss: 0.3585 - acc: 0.88 - ETA: 27s - loss: 0.3657 - acc: 0.87 - ETA: 26s - loss: 0.3628 - acc: 0.88 - ETA: 25s - loss: 0.3636 - acc: 0.88 - ETA: 24s - loss: 0.3632 - acc: 0.88 - ETA: 22s - loss: 0.3606 - acc: 0.88 - ETA: 21s - loss: 0.3704 - acc: 0.87 - ETA: 20s - loss: 0.3683 - acc: 0.87 - ETA: 19s - loss: 0.3655 - acc: 0.87 - ETA: 17s - loss: 0.3675 - acc: 0.87 - ETA: 16s - loss: 0.3695 - acc: 0.87 - ETA: 15s - loss: 0.3660 - acc: 0.87 - ETA: 14s - loss: 0.3644 - acc: 0.87 - ETA: 12s - loss: 0.3632 - acc: 0.87 - ETA: 11s - loss: 0.3617 - acc: 0.88 - ETA: 10s - loss: 0.3641 - acc: 0.87 - ETA: 9s - loss: 0.3630 - acc: 0.8802 - ETA: 7s - loss: 0.3684 - acc: 0.877 - ETA: 6s - loss: 0.3665 - acc: 0.878 - ETA: 5s - loss: 0.3673 - acc: 0.878 - ETA: 4s - loss: 0.3659 - acc: 0.878 - ETA: 2s - loss: 0.3667 - acc: 0.878 - ETA: 1s - loss: 0.3639 - acc: 0.880 - ETA: 0s - loss: 0.3656 - acc: 0.879 - 58s 42ms/step - loss: 0.3652 - acc: 0.8796 - val_loss: 0.3781 - val_acc: 0.8732\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 47s - loss: 0.6141 - acc: 0.75 - ETA: 46s - loss: 0.4673 - acc: 0.82 - ETA: 51s - loss: 0.4717 - acc: 0.82 - ETA: 48s - loss: 0.4041 - acc: 0.85 - ETA: 46s - loss: 0.4002 - acc: 0.86 - ETA: 44s - loss: 0.4150 - acc: 0.85 - ETA: 43s - loss: 0.3926 - acc: 0.86 - ETA: 43s - loss: 0.3951 - acc: 0.86 - ETA: 42s - loss: 0.3855 - acc: 0.86 - ETA: 40s - loss: 0.4054 - acc: 0.85 - ETA: 39s - loss: 0.3964 - acc: 0.86 - ETA: 38s - loss: 0.4018 - acc: 0.85 - ETA: 37s - loss: 0.4097 - acc: 0.85 - ETA: 36s - loss: 0.3946 - acc: 0.86 - ETA: 34s - loss: 0.3931 - acc: 0.86 - ETA: 33s - loss: 0.3971 - acc: 0.86 - ETA: 32s - loss: 0.3897 - acc: 0.86 - ETA: 31s - loss: 0.3865 - acc: 0.86 - ETA: 29s - loss: 0.3923 - acc: 0.86 - ETA: 28s - loss: 0.3910 - acc: 0.86 - ETA: 27s - loss: 0.3872 - acc: 0.86 - ETA: 26s - loss: 0.3786 - acc: 0.87 - ETA: 25s - loss: 0.3831 - acc: 0.87 - ETA: 23s - loss: 0.3875 - acc: 0.86 - ETA: 22s - loss: 0.3806 - acc: 0.87 - ETA: 21s - loss: 0.3787 - acc: 0.87 - ETA: 20s - loss: 0.3784 - acc: 0.87 - ETA: 18s - loss: 0.3760 - acc: 0.87 - ETA: 17s - loss: 0.3759 - acc: 0.87 - ETA: 16s - loss: 0.3718 - acc: 0.87 - ETA: 15s - loss: 0.3699 - acc: 0.87 - ETA: 14s - loss: 0.3678 - acc: 0.87 - ETA: 12s - loss: 0.3665 - acc: 0.87 - ETA: 11s - loss: 0.3649 - acc: 0.88 - ETA: 10s - loss: 0.3637 - acc: 0.88 - ETA: 9s - loss: 0.3606 - acc: 0.8828 - ETA: 7s - loss: 0.3608 - acc: 0.882 - ETA: 6s - loss: 0.3640 - acc: 0.880 - ETA: 5s - loss: 0.3660 - acc: 0.879 - ETA: 4s - loss: 0.3725 - acc: 0.876 - ETA: 2s - loss: 0.3708 - acc: 0.877 - ETA: 1s - loss: 0.3695 - acc: 0.878 - ETA: 0s - loss: 0.3680 - acc: 0.878 - 57s 41ms/step - loss: 0.3660 - acc: 0.8796 - val_loss: 0.3767 - val_acc: 0.8732\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 48s - loss: 0.5029 - acc: 0.81 - ETA: 56s - loss: 0.4081 - acc: 0.85 - ETA: 51s - loss: 0.3595 - acc: 0.88 - ETA: 49s - loss: 0.3779 - acc: 0.87 - ETA: 46s - loss: 0.3868 - acc: 0.86 - ETA: 47s - loss: 0.3511 - acc: 0.88 - ETA: 46s - loss: 0.3173 - acc: 0.90 - ETA: 44s - loss: 0.3238 - acc: 0.89 - ETA: 42s - loss: 0.3279 - acc: 0.89 - ETA: 41s - loss: 0.3244 - acc: 0.89 - ETA: 40s - loss: 0.3335 - acc: 0.89 - ETA: 39s - loss: 0.3481 - acc: 0.88 - ETA: 37s - loss: 0.3635 - acc: 0.87 - ETA: 36s - loss: 0.3641 - acc: 0.87 - ETA: 35s - loss: 0.3571 - acc: 0.88 - ETA: 34s - loss: 0.3703 - acc: 0.87 - ETA: 32s - loss: 0.3739 - acc: 0.87 - ETA: 31s - loss: 0.3702 - acc: 0.87 - ETA: 30s - loss: 0.3739 - acc: 0.87 - ETA: 29s - loss: 0.3727 - acc: 0.87 - ETA: 27s - loss: 0.3647 - acc: 0.87 - ETA: 26s - loss: 0.3766 - acc: 0.87 - ETA: 25s - loss: 0.3761 - acc: 0.87 - ETA: 24s - loss: 0.3705 - acc: 0.87 - ETA: 22s - loss: 0.3753 - acc: 0.87 - ETA: 21s - loss: 0.3793 - acc: 0.87 - ETA: 20s - loss: 0.3771 - acc: 0.87 - ETA: 19s - loss: 0.3727 - acc: 0.87 - ETA: 18s - loss: 0.3685 - acc: 0.87 - ETA: 16s - loss: 0.3687 - acc: 0.87 - ETA: 15s - loss: 0.3687 - acc: 0.87 - ETA: 14s - loss: 0.3689 - acc: 0.87 - ETA: 13s - loss: 0.3667 - acc: 0.87 - ETA: 11s - loss: 0.3648 - acc: 0.87 - ETA: 10s - loss: 0.3683 - acc: 0.87 - ETA: 9s - loss: 0.3702 - acc: 0.8750 - ETA: 8s - loss: 0.3700 - acc: 0.875 - ETA: 6s - loss: 0.3731 - acc: 0.873 - ETA: 5s - loss: 0.3718 - acc: 0.874 - ETA: 4s - loss: 0.3687 - acc: 0.875 - ETA: 2s - loss: 0.3683 - acc: 0.875 - ETA: 1s - loss: 0.3656 - acc: 0.877 - ETA: 0s - loss: 0.3615 - acc: 0.879 - 58s 42ms/step - loss: 0.3609 - acc: 0.8796 - val_loss: 0.3790 - val_acc: 0.8732\n",
      "0.22 [0.05, 0.08] 0.2141623488773748 [0.08] 0.8825561312607945 [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "entertainment\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 49s - loss: 0.2736 - acc: 0.93 - ETA: 47s - loss: 0.3084 - acc: 0.92 - ETA: 46s - loss: 0.3158 - acc: 0.91 - ETA: 48s - loss: 0.3562 - acc: 0.89 - ETA: 46s - loss: 0.3255 - acc: 0.91 - ETA: 45s - loss: 0.3256 - acc: 0.91 - ETA: 43s - loss: 0.3157 - acc: 0.91 - ETA: 42s - loss: 0.3103 - acc: 0.91 - ETA: 42s - loss: 0.3436 - acc: 0.90 - ETA: 40s - loss: 0.3276 - acc: 0.91 - ETA: 39s - loss: 0.3211 - acc: 0.91 - ETA: 37s - loss: 0.3394 - acc: 0.90 - ETA: 37s - loss: 0.3517 - acc: 0.90 - ETA: 36s - loss: 0.3607 - acc: 0.89 - ETA: 34s - loss: 0.3676 - acc: 0.89 - ETA: 33s - loss: 0.3653 - acc: 0.89 - ETA: 32s - loss: 0.3634 - acc: 0.89 - ETA: 31s - loss: 0.3698 - acc: 0.89 - ETA: 29s - loss: 0.3723 - acc: 0.89 - ETA: 28s - loss: 0.3727 - acc: 0.89 - ETA: 27s - loss: 0.3776 - acc: 0.88 - ETA: 26s - loss: 0.3836 - acc: 0.88 - ETA: 25s - loss: 0.3848 - acc: 0.88 - ETA: 23s - loss: 0.3848 - acc: 0.88 - ETA: 22s - loss: 0.3847 - acc: 0.88 - ETA: 21s - loss: 0.3861 - acc: 0.88 - ETA: 20s - loss: 0.3812 - acc: 0.88 - ETA: 18s - loss: 0.3903 - acc: 0.87 - ETA: 17s - loss: 0.3889 - acc: 0.87 - ETA: 16s - loss: 0.3845 - acc: 0.88 - ETA: 15s - loss: 0.3810 - acc: 0.88 - ETA: 14s - loss: 0.3819 - acc: 0.88 - ETA: 12s - loss: 0.3788 - acc: 0.88 - ETA: 11s - loss: 0.3767 - acc: 0.88 - ETA: 10s - loss: 0.3769 - acc: 0.88 - ETA: 9s - loss: 0.3803 - acc: 0.8828 - ETA: 7s - loss: 0.3774 - acc: 0.884 - ETA: 6s - loss: 0.3791 - acc: 0.883 - ETA: 5s - loss: 0.3796 - acc: 0.883 - ETA: 4s - loss: 0.3793 - acc: 0.882 - ETA: 2s - loss: 0.3779 - acc: 0.883 - ETA: 1s - loss: 0.3769 - acc: 0.883 - ETA: 0s - loss: 0.3759 - acc: 0.884 - 57s 41ms/step - loss: 0.3757 - acc: 0.8846 - val_loss: 0.3982 - val_acc: 0.8703\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 48s - loss: 0.4671 - acc: 0.84 - ETA: 47s - loss: 0.4680 - acc: 0.85 - ETA: 51s - loss: 0.4279 - acc: 0.87 - ETA: 49s - loss: 0.3859 - acc: 0.89 - ETA: 47s - loss: 0.3773 - acc: 0.89 - ETA: 45s - loss: 0.3593 - acc: 0.90 - ETA: 43s - loss: 0.3571 - acc: 0.90 - ETA: 44s - loss: 0.3577 - acc: 0.89 - ETA: 43s - loss: 0.3415 - acc: 0.90 - ETA: 41s - loss: 0.3538 - acc: 0.89 - ETA: 39s - loss: 0.3576 - acc: 0.89 - ETA: 39s - loss: 0.3556 - acc: 0.89 - ETA: 37s - loss: 0.3635 - acc: 0.89 - ETA: 36s - loss: 0.3745 - acc: 0.88 - ETA: 35s - loss: 0.3719 - acc: 0.88 - ETA: 33s - loss: 0.3933 - acc: 0.87 - ETA: 33s - loss: 0.3962 - acc: 0.87 - ETA: 31s - loss: 0.3808 - acc: 0.88 - ETA: 30s - loss: 0.3811 - acc: 0.87 - ETA: 28s - loss: 0.3773 - acc: 0.88 - ETA: 28s - loss: 0.3738 - acc: 0.88 - ETA: 26s - loss: 0.3716 - acc: 0.88 - ETA: 25s - loss: 0.3690 - acc: 0.88 - ETA: 24s - loss: 0.3753 - acc: 0.88 - ETA: 22s - loss: 0.3703 - acc: 0.88 - ETA: 21s - loss: 0.3726 - acc: 0.88 - ETA: 20s - loss: 0.3702 - acc: 0.88 - ETA: 19s - loss: 0.3748 - acc: 0.88 - ETA: 17s - loss: 0.3736 - acc: 0.88 - ETA: 16s - loss: 0.3754 - acc: 0.88 - ETA: 15s - loss: 0.3689 - acc: 0.88 - ETA: 14s - loss: 0.3761 - acc: 0.87 - ETA: 12s - loss: 0.3761 - acc: 0.87 - ETA: 11s - loss: 0.3762 - acc: 0.87 - ETA: 10s - loss: 0.3755 - acc: 0.87 - ETA: 9s - loss: 0.3723 - acc: 0.8802 - ETA: 7s - loss: 0.3682 - acc: 0.881 - ETA: 6s - loss: 0.3694 - acc: 0.880 - ETA: 5s - loss: 0.3663 - acc: 0.882 - ETA: 4s - loss: 0.3605 - acc: 0.885 - ETA: 2s - loss: 0.3623 - acc: 0.884 - ETA: 1s - loss: 0.3638 - acc: 0.883 - ETA: 0s - loss: 0.3630 - acc: 0.883 - 58s 42ms/step - loss: 0.3609 - acc: 0.8846 - val_loss: 0.3816 - val_acc: 0.8703\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 47s - loss: 0.2344 - acc: 0.93 - ETA: 55s - loss: 0.2359 - acc: 0.93 - ETA: 55s - loss: 0.2407 - acc: 0.93 - ETA: 52s - loss: 0.2945 - acc: 0.90 - ETA: 49s - loss: 0.3226 - acc: 0.89 - ETA: 49s - loss: 0.3001 - acc: 0.90 - ETA: 47s - loss: 0.3013 - acc: 0.90 - ETA: 45s - loss: 0.3306 - acc: 0.89 - ETA: 44s - loss: 0.3317 - acc: 0.89 - ETA: 42s - loss: 0.3082 - acc: 0.90 - ETA: 41s - loss: 0.3049 - acc: 0.90 - ETA: 40s - loss: 0.3167 - acc: 0.90 - ETA: 38s - loss: 0.3131 - acc: 0.90 - ETA: 37s - loss: 0.3263 - acc: 0.89 - ETA: 36s - loss: 0.3221 - acc: 0.89 - ETA: 35s - loss: 0.3350 - acc: 0.89 - ETA: 33s - loss: 0.3393 - acc: 0.88 - ETA: 32s - loss: 0.3378 - acc: 0.88 - ETA: 30s - loss: 0.3433 - acc: 0.88 - ETA: 29s - loss: 0.3475 - acc: 0.88 - ETA: 28s - loss: 0.3430 - acc: 0.88 - ETA: 26s - loss: 0.3478 - acc: 0.88 - ETA: 25s - loss: 0.3453 - acc: 0.88 - ETA: 24s - loss: 0.3421 - acc: 0.88 - ETA: 23s - loss: 0.3409 - acc: 0.88 - ETA: 21s - loss: 0.3407 - acc: 0.88 - ETA: 20s - loss: 0.3426 - acc: 0.88 - ETA: 19s - loss: 0.3464 - acc: 0.88 - ETA: 18s - loss: 0.3399 - acc: 0.88 - ETA: 16s - loss: 0.3394 - acc: 0.88 - ETA: 15s - loss: 0.3370 - acc: 0.88 - ETA: 14s - loss: 0.3413 - acc: 0.88 - ETA: 13s - loss: 0.3450 - acc: 0.88 - ETA: 11s - loss: 0.3427 - acc: 0.88 - ETA: 10s - loss: 0.3403 - acc: 0.88 - ETA: 9s - loss: 0.3452 - acc: 0.8819 - ETA: 7s - loss: 0.3417 - acc: 0.883 - ETA: 6s - loss: 0.3398 - acc: 0.884 - ETA: 5s - loss: 0.3427 - acc: 0.883 - ETA: 4s - loss: 0.3425 - acc: 0.882 - ETA: 2s - loss: 0.3380 - acc: 0.884 - ETA: 1s - loss: 0.3377 - acc: 0.885 - ETA: 0s - loss: 0.3407 - acc: 0.884 - 58s 42ms/step - loss: 0.3404 - acc: 0.8846 - val_loss: 0.3799 - val_acc: 0.8703\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 49s - loss: 0.3724 - acc: 0.84 - ETA: 47s - loss: 0.2654 - acc: 0.90 - ETA: 46s - loss: 0.2500 - acc: 0.91 - ETA: 44s - loss: 0.2677 - acc: 0.90 - ETA: 46s - loss: 0.2607 - acc: 0.91 - ETA: 45s - loss: 0.2693 - acc: 0.91 - ETA: 43s - loss: 0.2843 - acc: 0.90 - ETA: 42s - loss: 0.2921 - acc: 0.89 - ETA: 40s - loss: 0.2970 - acc: 0.89 - ETA: 40s - loss: 0.3080 - acc: 0.89 - ETA: 39s - loss: 0.3429 - acc: 0.87 - ETA: 37s - loss: 0.3312 - acc: 0.88 - ETA: 36s - loss: 0.3366 - acc: 0.87 - ETA: 36s - loss: 0.3347 - acc: 0.88 - ETA: 34s - loss: 0.3236 - acc: 0.88 - ETA: 33s - loss: 0.3356 - acc: 0.88 - ETA: 32s - loss: 0.3416 - acc: 0.87 - ETA: 30s - loss: 0.3360 - acc: 0.88 - ETA: 29s - loss: 0.3394 - acc: 0.87 - ETA: 28s - loss: 0.3413 - acc: 0.87 - ETA: 27s - loss: 0.3430 - acc: 0.87 - ETA: 25s - loss: 0.3374 - acc: 0.87 - ETA: 25s - loss: 0.3346 - acc: 0.87 - ETA: 23s - loss: 0.3334 - acc: 0.88 - ETA: 22s - loss: 0.3343 - acc: 0.88 - ETA: 21s - loss: 0.3356 - acc: 0.88 - ETA: 19s - loss: 0.3330 - acc: 0.88 - ETA: 18s - loss: 0.3361 - acc: 0.88 - ETA: 17s - loss: 0.3330 - acc: 0.88 - ETA: 16s - loss: 0.3324 - acc: 0.88 - ETA: 15s - loss: 0.3347 - acc: 0.88 - ETA: 13s - loss: 0.3377 - acc: 0.88 - ETA: 12s - loss: 0.3421 - acc: 0.87 - ETA: 11s - loss: 0.3419 - acc: 0.87 - ETA: 10s - loss: 0.3412 - acc: 0.88 - ETA: 8s - loss: 0.3410 - acc: 0.8802 - ETA: 7s - loss: 0.3385 - acc: 0.881 - ETA: 6s - loss: 0.3368 - acc: 0.882 - ETA: 5s - loss: 0.3397 - acc: 0.881 - ETA: 4s - loss: 0.3412 - acc: 0.880 - ETA: 2s - loss: 0.3390 - acc: 0.881 - ETA: 1s - loss: 0.3391 - acc: 0.881 - ETA: 0s - loss: 0.3344 - acc: 0.884 - 57s 41ms/step - loss: 0.3334 - acc: 0.8846 - val_loss: 0.3765 - val_acc: 0.8703\n",
      "0.3 [0.16, 0.18] 0.7547495682210709 [0.18] 0.8618307426597582 [0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "history\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 48s - loss: 0.3894 - acc: 0.84 - ETA: 47s - loss: 0.5876 - acc: 0.76 - ETA: 52s - loss: 0.5505 - acc: 0.78 - ETA: 49s - loss: 0.5122 - acc: 0.80 - ETA: 47s - loss: 0.4836 - acc: 0.82 - ETA: 45s - loss: 0.4674 - acc: 0.83 - ETA: 43s - loss: 0.5008 - acc: 0.82 - ETA: 44s - loss: 0.5483 - acc: 0.80 - ETA: 42s - loss: 0.5223 - acc: 0.81 - ETA: 41s - loss: 0.5054 - acc: 0.81 - ETA: 39s - loss: 0.4943 - acc: 0.82 - ETA: 39s - loss: 0.4999 - acc: 0.82 - ETA: 37s - loss: 0.5032 - acc: 0.81 - ETA: 36s - loss: 0.4932 - acc: 0.82 - ETA: 35s - loss: 0.4912 - acc: 0.82 - ETA: 33s - loss: 0.4855 - acc: 0.82 - ETA: 32s - loss: 0.4788 - acc: 0.83 - ETA: 31s - loss: 0.4763 - acc: 0.83 - ETA: 30s - loss: 0.4729 - acc: 0.83 - ETA: 28s - loss: 0.4677 - acc: 0.83 - ETA: 27s - loss: 0.4671 - acc: 0.83 - ETA: 26s - loss: 0.4702 - acc: 0.83 - ETA: 25s - loss: 0.4669 - acc: 0.83 - ETA: 23s - loss: 0.4612 - acc: 0.84 - ETA: 22s - loss: 0.4639 - acc: 0.83 - ETA: 21s - loss: 0.4654 - acc: 0.83 - ETA: 20s - loss: 0.4729 - acc: 0.83 - ETA: 19s - loss: 0.4733 - acc: 0.83 - ETA: 17s - loss: 0.4697 - acc: 0.83 - ETA: 17s - loss: 0.4726 - acc: 0.82 - ETA: 16s - loss: 0.4727 - acc: 0.82 - ETA: 14s - loss: 0.4711 - acc: 0.82 - ETA: 13s - loss: 0.4692 - acc: 0.82 - ETA: 12s - loss: 0.4682 - acc: 0.83 - ETA: 10s - loss: 0.4659 - acc: 0.83 - ETA: 9s - loss: 0.4624 - acc: 0.8333 - ETA: 8s - loss: 0.4621 - acc: 0.832 - ETA: 7s - loss: 0.4614 - acc: 0.833 - ETA: 5s - loss: 0.4631 - acc: 0.831 - ETA: 4s - loss: 0.4627 - acc: 0.832 - ETA: 3s - loss: 0.4609 - acc: 0.833 - ETA: 1s - loss: 0.4627 - acc: 0.831 - ETA: 0s - loss: 0.4627 - acc: 0.832 - 61s 44ms/step - loss: 0.4617 - acc: 0.8327 - val_loss: 0.4404 - val_acc: 0.8357\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 54s - loss: 0.3652 - acc: 0.87 - ETA: 50s - loss: 0.3572 - acc: 0.89 - ETA: 49s - loss: 0.4098 - acc: 0.86 - ETA: 15:47:54 - loss: 0.3846 - acc: 0.875 - ETA: 12:19:14 - loss: 0.4155 - acc: 0.850 - ETA: 10:01:23 - loss: 0.4211 - acc: 0.843 - ETA: 8:21:52 - loss: 0.3930 - acc: 0.861 - ETA: 7:07:11 - loss: 0.3716 - acc: 0.87 - ETA: 6:09:03 - loss: 0.4065 - acc: 0.85 - ETA: 5:22:35 - loss: 0.4017 - acc: 0.85 - ETA: 4:44:32 - loss: 0.3958 - acc: 0.86 - ETA: 4:12:51 - loss: 0.4015 - acc: 0.85 - ETA: 3:46:00 - loss: 0.4138 - acc: 0.85 - ETA: 3:22:59 - loss: 0.4143 - acc: 0.85 - ETA: 3:03:06 - loss: 0.4203 - acc: 0.84 - ETA: 2:45:39 - loss: 0.4231 - acc: 0.84 - ETA: 2:30:16 - loss: 0.4265 - acc: 0.84 - ETA: 2:16:35 - loss: 0.4273 - acc: 0.84 - ETA: 2:04:19 - loss: 0.4368 - acc: 0.83 - ETA: 1:53:18 - loss: 0.4367 - acc: 0.83 - ETA: 1:43:18 - loss: 0.4376 - acc: 0.83 - ETA: 1:34:13 - loss: 0.4337 - acc: 0.83 - ETA: 1:25:55 - loss: 0.4373 - acc: 0.83 - ETA: 1:18:19 - loss: 0.4425 - acc: 0.83 - ETA: 1:11:19 - loss: 0.4422 - acc: 0.83 - ETA: 1:04:52 - loss: 0.4421 - acc: 0.83 - ETA: 58:53 - loss: 0.4403 - acc: 0.8356 - ETA: 53:20 - loss: 0.4409 - acc: 0.83 - ETA: 48:09 - loss: 0.4504 - acc: 0.82 - ETA: 43:19 - loss: 0.4493 - acc: 0.82 - ETA: 38:47 - loss: 0.4517 - acc: 0.82 - ETA: 34:33 - loss: 0.4510 - acc: 0.82 - ETA: 30:33 - loss: 0.4504 - acc: 0.82 - ETA: 26:47 - loss: 0.4523 - acc: 0.82 - ETA: 23:15 - loss: 0.4537 - acc: 0.82 - ETA: 19:54 - loss: 0.4528 - acc: 0.82 - ETA: 16:43 - loss: 0.4539 - acc: 0.82 - ETA: 13:43 - loss: 0.4531 - acc: 0.82 - ETA: 10:52 - loss: 0.4529 - acc: 0.82 - ETA: 8:09 - loss: 0.4497 - acc: 0.8289 - ETA: 5:34 - loss: 0.4468 - acc: 0.830 - ETA: 3:07 - loss: 0.4458 - acc: 0.831 - ETA: 46s - loss: 0.4446 - acc: 0.832 - 5867s 4s/step - loss: 0.4434 - acc: 0.8327 - val_loss: 0.4378 - val_acc: 0.8357\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 47s - loss: 0.3616 - acc: 0.87 - ETA: 47s - loss: 0.3882 - acc: 0.85 - ETA: 51s - loss: 0.3981 - acc: 0.85 - ETA: 49s - loss: 0.4022 - acc: 0.85 - ETA: 47s - loss: 0.3940 - acc: 0.85 - ETA: 45s - loss: 0.4132 - acc: 0.84 - ETA: 43s - loss: 0.3975 - acc: 0.85 - ETA: 44s - loss: 0.4047 - acc: 0.85 - ETA: 42s - loss: 0.3957 - acc: 0.85 - ETA: 40s - loss: 0.4126 - acc: 0.84 - ETA: 39s - loss: 0.4012 - acc: 0.85 - ETA: 39s - loss: 0.4035 - acc: 0.85 - ETA: 37s - loss: 0.3955 - acc: 0.85 - ETA: 36s - loss: 0.4165 - acc: 0.84 - ETA: 34s - loss: 0.4135 - acc: 0.84 - ETA: 33s - loss: 0.4200 - acc: 0.84 - ETA: 32s - loss: 0.4230 - acc: 0.84 - ETA: 31s - loss: 0.4197 - acc: 0.84 - ETA: 29s - loss: 0.4290 - acc: 0.83 - ETA: 28s - loss: 0.4279 - acc: 0.83 - ETA: 27s - loss: 0.4288 - acc: 0.83 - ETA: 26s - loss: 0.4299 - acc: 0.83 - ETA: 25s - loss: 0.4381 - acc: 0.83 - ETA: 23s - loss: 0.4411 - acc: 0.83 - ETA: 22s - loss: 0.4379 - acc: 0.83 - ETA: 21s - loss: 0.4366 - acc: 0.83 - ETA: 20s - loss: 0.4350 - acc: 0.83 - ETA: 18s - loss: 0.4361 - acc: 0.83 - ETA: 17s - loss: 0.4340 - acc: 0.83 - ETA: 16s - loss: 0.4311 - acc: 0.83 - ETA: 15s - loss: 0.4353 - acc: 0.83 - ETA: 14s - loss: 0.4373 - acc: 0.83 - ETA: 12s - loss: 0.4446 - acc: 0.83 - ETA: 11s - loss: 0.4498 - acc: 0.82 - ETA: 10s - loss: 0.4467 - acc: 0.82 - ETA: 9s - loss: 0.4442 - acc: 0.8299 - ETA: 7s - loss: 0.4433 - acc: 0.830 - ETA: 6s - loss: 0.4419 - acc: 0.831 - ETA: 5s - loss: 0.4442 - acc: 0.830 - ETA: 4s - loss: 0.4438 - acc: 0.831 - ETA: 2s - loss: 0.4461 - acc: 0.830 - ETA: 1s - loss: 0.4418 - acc: 0.832 - ETA: 0s - loss: 0.4388 - acc: 0.834 - 58s 42ms/step - loss: 0.4418 - acc: 0.8327 - val_loss: 0.4372 - val_acc: 0.8357\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 1:06 - loss: 0.5243 - acc: 0.781 - ETA: 55s - loss: 0.4430 - acc: 0.843 - ETA: 51s - loss: 0.4159 - acc: 0.85 - ETA: 49s - loss: 0.4442 - acc: 0.83 - ETA: 46s - loss: 0.4334 - acc: 0.84 - ETA: 47s - loss: 0.4196 - acc: 0.84 - ETA: 45s - loss: 0.4179 - acc: 0.84 - ETA: 43s - loss: 0.4481 - acc: 0.82 - ETA: 42s - loss: 0.4549 - acc: 0.82 - ETA: 40s - loss: 0.4593 - acc: 0.82 - ETA: 40s - loss: 0.4469 - acc: 0.82 - ETA: 39s - loss: 0.4361 - acc: 0.83 - ETA: 38s - loss: 0.4383 - acc: 0.83 - ETA: 36s - loss: 0.4290 - acc: 0.83 - ETA: 36s - loss: 0.4391 - acc: 0.83 - ETA: 34s - loss: 0.4367 - acc: 0.83 - ETA: 33s - loss: 0.4360 - acc: 0.83 - ETA: 31s - loss: 0.4393 - acc: 0.83 - ETA: 30s - loss: 0.4354 - acc: 0.83 - ETA: 29s - loss: 0.4334 - acc: 0.83 - ETA: 28s - loss: 0.4345 - acc: 0.83 - ETA: 26s - loss: 0.4337 - acc: 0.83 - ETA: 25s - loss: 0.4359 - acc: 0.83 - ETA: 24s - loss: 0.4347 - acc: 0.83 - ETA: 23s - loss: 0.4330 - acc: 0.83 - ETA: 21s - loss: 0.4253 - acc: 0.84 - ETA: 20s - loss: 0.4238 - acc: 0.84 - ETA: 19s - loss: 0.4237 - acc: 0.84 - ETA: 17s - loss: 0.4237 - acc: 0.84 - ETA: 16s - loss: 0.4213 - acc: 0.84 - ETA: 15s - loss: 0.4245 - acc: 0.84 - ETA: 14s - loss: 0.4239 - acc: 0.84 - ETA: 12s - loss: 0.4245 - acc: 0.84 - ETA: 11s - loss: 0.4263 - acc: 0.84 - ETA: 10s - loss: 0.4337 - acc: 0.83 - ETA: 9s - loss: 0.4347 - acc: 0.8368 - ETA: 7s - loss: 0.4381 - acc: 0.835 - ETA: 6s - loss: 0.4384 - acc: 0.834 - ETA: 5s - loss: 0.4401 - acc: 0.833 - ETA: 4s - loss: 0.4388 - acc: 0.834 - ETA: 2s - loss: 0.4348 - acc: 0.836 - ETA: 1s - loss: 0.4413 - acc: 0.833 - ETA: 0s - loss: 0.4400 - acc: 0.833 - 58s 42ms/step - loss: 0.4410 - acc: 0.8327 - val_loss: 0.4382 - val_acc: 0.8357\n",
      "0.31 [0.11, 0.12, 0.13, 0.14, 0.15, 0.18] 0.4265975820379965 [0.18] 0.8341968911917098 [0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "future\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 55s - loss: 0.2465 - acc: 0.96 - ETA: 50s - loss: 0.2681 - acc: 0.95 - ETA: 48s - loss: 0.3143 - acc: 0.92 - ETA: 50s - loss: 0.3384 - acc: 0.90 - ETA: 48s - loss: 0.3293 - acc: 0.91 - ETA: 46s - loss: 0.3373 - acc: 0.90 - ETA: 44s - loss: 0.3435 - acc: 0.90 - ETA: 46s - loss: 0.3369 - acc: 0.90 - ETA: 45s - loss: 0.3303 - acc: 0.90 - ETA: 44s - loss: 0.3339 - acc: 0.90 - ETA: 42s - loss: 0.3420 - acc: 0.90 - ETA: 41s - loss: 0.3338 - acc: 0.90 - ETA: 40s - loss: 0.3255 - acc: 0.91 - ETA: 38s - loss: 0.3260 - acc: 0.91 - ETA: 36s - loss: 0.3321 - acc: 0.90 - ETA: 35s - loss: 0.3351 - acc: 0.90 - ETA: 34s - loss: 0.3330 - acc: 0.90 - ETA: 32s - loss: 0.3305 - acc: 0.90 - ETA: 31s - loss: 0.3297 - acc: 0.90 - ETA: 29s - loss: 0.3315 - acc: 0.90 - ETA: 28s - loss: 0.3277 - acc: 0.90 - ETA: 27s - loss: 0.3247 - acc: 0.90 - ETA: 26s - loss: 0.3295 - acc: 0.90 - ETA: 24s - loss: 0.3333 - acc: 0.90 - ETA: 23s - loss: 0.3339 - acc: 0.90 - ETA: 22s - loss: 0.3372 - acc: 0.90 - ETA: 21s - loss: 0.3410 - acc: 0.89 - ETA: 20s - loss: 0.3339 - acc: 0.90 - ETA: 18s - loss: 0.3335 - acc: 0.90 - ETA: 17s - loss: 0.3347 - acc: 0.90 - ETA: 16s - loss: 0.3375 - acc: 0.90 - ETA: 14s - loss: 0.3350 - acc: 0.90 - ETA: 13s - loss: 0.3344 - acc: 0.90 - ETA: 12s - loss: 0.3304 - acc: 0.90 - ETA: 10s - loss: 0.3331 - acc: 0.90 - ETA: 9s - loss: 0.3323 - acc: 0.9019 - ETA: 8s - loss: 0.3332 - acc: 0.901 - ETA: 6s - loss: 0.3314 - acc: 0.902 - ETA: 5s - loss: 0.3274 - acc: 0.903 - ETA: 4s - loss: 0.3272 - acc: 0.903 - ETA: 3s - loss: 0.3223 - acc: 0.906 - ETA: 1s - loss: 0.3239 - acc: 0.905 - ETA: 0s - loss: 0.3208 - acc: 0.907 - 60s 43ms/step - loss: 0.3205 - acc: 0.9070 - val_loss: 0.3207 - val_acc: 0.8991\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 1:07 - loss: 0.3462 - acc: 0.875 - ETA: 56s - loss: 0.2908 - acc: 0.906 - ETA: 52s - loss: 0.3424 - acc: 0.88 - ETA: 49s - loss: 0.2987 - acc: 0.90 - ETA: 50s - loss: 0.2695 - acc: 0.91 - ETA: 48s - loss: 0.2864 - acc: 0.91 - ETA: 46s - loss: 0.2940 - acc: 0.91 - ETA: 44s - loss: 0.2858 - acc: 0.91 - ETA: 42s - loss: 0.2858 - acc: 0.91 - ETA: 42s - loss: 0.2806 - acc: 0.91 - ETA: 40s - loss: 0.2793 - acc: 0.91 - ETA: 39s - loss: 0.2804 - acc: 0.91 - ETA: 37s - loss: 0.2766 - acc: 0.91 - ETA: 37s - loss: 0.2691 - acc: 0.92 - ETA: 35s - loss: 0.2730 - acc: 0.92 - ETA: 34s - loss: 0.2671 - acc: 0.92 - ETA: 32s - loss: 0.2723 - acc: 0.92 - ETA: 31s - loss: 0.2781 - acc: 0.91 - ETA: 30s - loss: 0.2824 - acc: 0.91 - ETA: 29s - loss: 0.2915 - acc: 0.91 - ETA: 27s - loss: 0.3008 - acc: 0.90 - ETA: 26s - loss: 0.2972 - acc: 0.90 - ETA: 25s - loss: 0.2958 - acc: 0.91 - ETA: 24s - loss: 0.2938 - acc: 0.91 - ETA: 22s - loss: 0.2994 - acc: 0.91 - ETA: 21s - loss: 0.3066 - acc: 0.90 - ETA: 20s - loss: 0.3063 - acc: 0.90 - ETA: 20s - loss: 0.3103 - acc: 0.90 - ETA: 19s - loss: 0.3052 - acc: 0.90 - ETA: 18s - loss: 0.3035 - acc: 0.90 - ETA: 17s - loss: 0.3118 - acc: 0.90 - ETA: 15s - loss: 0.3202 - acc: 0.90 - ETA: 14s - loss: 0.3192 - acc: 0.90 - ETA: 13s - loss: 0.3227 - acc: 0.90 - ETA: 11s - loss: 0.3201 - acc: 0.90 - ETA: 10s - loss: 0.3179 - acc: 0.90 - ETA: 8s - loss: 0.3204 - acc: 0.9029 - ETA: 7s - loss: 0.3255 - acc: 0.900 - ETA: 6s - loss: 0.3239 - acc: 0.901 - ETA: 4s - loss: 0.3185 - acc: 0.903 - ETA: 3s - loss: 0.3154 - acc: 0.905 - ETA: 1s - loss: 0.3135 - acc: 0.906 - ETA: 0s - loss: 0.3134 - acc: 0.906 - 64s 46ms/step - loss: 0.3117 - acc: 0.9070 - val_loss: 0.3216 - val_acc: 0.8991\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 54s - loss: 0.4474 - acc: 0.84 - ETA: 49s - loss: 0.3894 - acc: 0.87 - ETA: 47s - loss: 0.3175 - acc: 0.90 - ETA: 50s - loss: 0.3716 - acc: 0.87 - ETA: 48s - loss: 0.3651 - acc: 0.87 - ETA: 46s - loss: 0.3449 - acc: 0.88 - ETA: 47s - loss: 0.3425 - acc: 0.88 - ETA: 47s - loss: 0.3398 - acc: 0.89 - ETA: 45s - loss: 0.3412 - acc: 0.88 - ETA: 43s - loss: 0.3295 - acc: 0.89 - ETA: 41s - loss: 0.3328 - acc: 0.89 - ETA: 39s - loss: 0.3356 - acc: 0.89 - ETA: 37s - loss: 0.3344 - acc: 0.89 - ETA: 35s - loss: 0.3254 - acc: 0.89 - ETA: 33s - loss: 0.3214 - acc: 0.90 - ETA: 31s - loss: 0.3202 - acc: 0.90 - ETA: 30s - loss: 0.3167 - acc: 0.90 - ETA: 28s - loss: 0.3228 - acc: 0.89 - ETA: 27s - loss: 0.3278 - acc: 0.89 - ETA: 26s - loss: 0.3242 - acc: 0.89 - ETA: 24s - loss: 0.3226 - acc: 0.89 - ETA: 23s - loss: 0.3152 - acc: 0.90 - ETA: 22s - loss: 0.3063 - acc: 0.90 - ETA: 20s - loss: 0.3101 - acc: 0.90 - ETA: 19s - loss: 0.3103 - acc: 0.90 - ETA: 18s - loss: 0.3178 - acc: 0.90 - ETA: 17s - loss: 0.3151 - acc: 0.90 - ETA: 16s - loss: 0.3181 - acc: 0.90 - ETA: 15s - loss: 0.3200 - acc: 0.90 - ETA: 14s - loss: 0.3172 - acc: 0.90 - ETA: 12s - loss: 0.3105 - acc: 0.90 - ETA: 11s - loss: 0.3039 - acc: 0.90 - ETA: 10s - loss: 0.2996 - acc: 0.91 - ETA: 9s - loss: 0.3066 - acc: 0.9072 - ETA: 8s - loss: 0.3118 - acc: 0.905 - ETA: 7s - loss: 0.3113 - acc: 0.905 - ETA: 6s - loss: 0.3054 - acc: 0.907 - ETA: 5s - loss: 0.3091 - acc: 0.906 - ETA: 4s - loss: 0.3075 - acc: 0.907 - ETA: 3s - loss: 0.3041 - acc: 0.908 - ETA: 2s - loss: 0.3006 - acc: 0.910 - ETA: 1s - loss: 0.2988 - acc: 0.910 - ETA: 0s - loss: 0.3061 - acc: 0.907 - 46s 33ms/step - loss: 0.3084 - acc: 0.9070 - val_loss: 0.3240 - val_acc: 0.8991\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 40s - loss: 0.2959 - acc: 0.90 - ETA: 38s - loss: 0.2496 - acc: 0.93 - ETA: 37s - loss: 0.2460 - acc: 0.93 - ETA: 35s - loss: 0.2752 - acc: 0.92 - ETA: 34s - loss: 0.2678 - acc: 0.92 - ETA: 34s - loss: 0.2987 - acc: 0.91 - ETA: 33s - loss: 0.3280 - acc: 0.89 - ETA: 32s - loss: 0.2989 - acc: 0.91 - ETA: 31s - loss: 0.3108 - acc: 0.90 - ETA: 30s - loss: 0.3010 - acc: 0.90 - ETA: 29s - loss: 0.3042 - acc: 0.90 - ETA: 28s - loss: 0.3056 - acc: 0.90 - ETA: 27s - loss: 0.3327 - acc: 0.89 - ETA: 26s - loss: 0.3313 - acc: 0.89 - ETA: 25s - loss: 0.3295 - acc: 0.89 - ETA: 24s - loss: 0.3201 - acc: 0.90 - ETA: 24s - loss: 0.3257 - acc: 0.89 - ETA: 23s - loss: 0.3324 - acc: 0.89 - ETA: 22s - loss: 0.3339 - acc: 0.89 - ETA: 21s - loss: 0.3285 - acc: 0.89 - ETA: 20s - loss: 0.3208 - acc: 0.90 - ETA: 19s - loss: 0.3173 - acc: 0.90 - ETA: 18s - loss: 0.3176 - acc: 0.90 - ETA: 17s - loss: 0.3146 - acc: 0.90 - ETA: 16s - loss: 0.3154 - acc: 0.90 - ETA: 15s - loss: 0.3145 - acc: 0.90 - ETA: 14s - loss: 0.3145 - acc: 0.90 - ETA: 13s - loss: 0.3113 - acc: 0.90 - ETA: 13s - loss: 0.3068 - acc: 0.90 - ETA: 12s - loss: 0.3046 - acc: 0.90 - ETA: 11s - loss: 0.3078 - acc: 0.90 - ETA: 10s - loss: 0.3069 - acc: 0.90 - ETA: 9s - loss: 0.3026 - acc: 0.9081 - ETA: 8s - loss: 0.3030 - acc: 0.908 - ETA: 7s - loss: 0.3051 - acc: 0.907 - ETA: 6s - loss: 0.3036 - acc: 0.908 - ETA: 5s - loss: 0.3040 - acc: 0.907 - ETA: 4s - loss: 0.3039 - acc: 0.907 - ETA: 3s - loss: 0.3018 - acc: 0.908 - ETA: 3s - loss: 0.3013 - acc: 0.908 - ETA: 2s - loss: 0.3058 - acc: 0.906 - ETA: 1s - loss: 0.3056 - acc: 0.906 - ETA: 0s - loss: 0.3025 - acc: 0.907 - 42s 30ms/step - loss: 0.3040 - acc: 0.9070 - val_loss: 0.3206 - val_acc: 0.8991\n",
      "0.2 [0.09, 0.1] 0.459412780656304 [0.1] 0.9067357512953368 [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "communication\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 38s - loss: 0.2654 - acc: 0.93 - ETA: 37s - loss: 0.2213 - acc: 0.95 - ETA: 36s - loss: 0.2566 - acc: 0.93 - ETA: 35s - loss: 0.2498 - acc: 0.93 - ETA: 34s - loss: 0.2777 - acc: 0.92 - ETA: 33s - loss: 0.2591 - acc: 0.93 - ETA: 32s - loss: 0.2460 - acc: 0.93 - ETA: 31s - loss: 0.2616 - acc: 0.92 - ETA: 30s - loss: 0.2600 - acc: 0.93 - ETA: 30s - loss: 0.2577 - acc: 0.93 - ETA: 29s - loss: 0.2618 - acc: 0.92 - ETA: 28s - loss: 0.2609 - acc: 0.92 - ETA: 27s - loss: 0.2654 - acc: 0.92 - ETA: 26s - loss: 0.2857 - acc: 0.91 - ETA: 25s - loss: 0.2903 - acc: 0.91 - ETA: 25s - loss: 0.2786 - acc: 0.92 - ETA: 24s - loss: 0.2860 - acc: 0.91 - ETA: 23s - loss: 0.2829 - acc: 0.92 - ETA: 22s - loss: 0.2778 - acc: 0.92 - ETA: 21s - loss: 0.2758 - acc: 0.92 - ETA: 20s - loss: 0.2730 - acc: 0.92 - ETA: 19s - loss: 0.2760 - acc: 0.92 - ETA: 18s - loss: 0.2714 - acc: 0.92 - ETA: 17s - loss: 0.2740 - acc: 0.92 - ETA: 16s - loss: 0.2729 - acc: 0.92 - ETA: 15s - loss: 0.2744 - acc: 0.92 - ETA: 14s - loss: 0.2720 - acc: 0.92 - ETA: 13s - loss: 0.2686 - acc: 0.92 - ETA: 13s - loss: 0.2675 - acc: 0.92 - ETA: 12s - loss: 0.2704 - acc: 0.92 - ETA: 11s - loss: 0.2650 - acc: 0.92 - ETA: 10s - loss: 0.2616 - acc: 0.92 - ETA: 9s - loss: 0.2612 - acc: 0.9299 - ETA: 8s - loss: 0.2694 - acc: 0.926 - ETA: 7s - loss: 0.2747 - acc: 0.924 - ETA: 6s - loss: 0.2718 - acc: 0.925 - ETA: 5s - loss: 0.2740 - acc: 0.924 - ETA: 4s - loss: 0.2755 - acc: 0.924 - ETA: 3s - loss: 0.2707 - acc: 0.926 - ETA: 3s - loss: 0.2703 - acc: 0.926 - ETA: 2s - loss: 0.2664 - acc: 0.928 - ETA: 1s - loss: 0.2640 - acc: 0.929 - ETA: 0s - loss: 0.2651 - acc: 0.928 - 42s 30ms/step - loss: 0.2651 - acc: 0.9286 - val_loss: 0.3102 - val_acc: 0.9049\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 38s - loss: 0.3741 - acc: 0.87 - ETA: 37s - loss: 0.3064 - acc: 0.90 - ETA: 39s - loss: 0.2748 - acc: 0.91 - ETA: 37s - loss: 0.2492 - acc: 0.92 - ETA: 36s - loss: 0.2785 - acc: 0.91 - ETA: 34s - loss: 0.2865 - acc: 0.91 - ETA: 33s - loss: 0.2745 - acc: 0.91 - ETA: 32s - loss: 0.2792 - acc: 0.91 - ETA: 32s - loss: 0.2776 - acc: 0.92 - ETA: 30s - loss: 0.2677 - acc: 0.92 - ETA: 29s - loss: 0.2593 - acc: 0.92 - ETA: 28s - loss: 0.2578 - acc: 0.92 - ETA: 27s - loss: 0.2546 - acc: 0.93 - ETA: 27s - loss: 0.2595 - acc: 0.92 - ETA: 26s - loss: 0.2538 - acc: 0.93 - ETA: 25s - loss: 0.2535 - acc: 0.93 - ETA: 24s - loss: 0.2616 - acc: 0.92 - ETA: 23s - loss: 0.2561 - acc: 0.93 - ETA: 22s - loss: 0.2509 - acc: 0.93 - ETA: 21s - loss: 0.2470 - acc: 0.93 - ETA: 20s - loss: 0.2460 - acc: 0.93 - ETA: 19s - loss: 0.2490 - acc: 0.93 - ETA: 18s - loss: 0.2521 - acc: 0.93 - ETA: 17s - loss: 0.2642 - acc: 0.92 - ETA: 16s - loss: 0.2591 - acc: 0.92 - ETA: 16s - loss: 0.2548 - acc: 0.93 - ETA: 15s - loss: 0.2563 - acc: 0.92 - ETA: 14s - loss: 0.2528 - acc: 0.93 - ETA: 13s - loss: 0.2468 - acc: 0.93 - ETA: 13s - loss: 0.2463 - acc: 0.93 - ETA: 12s - loss: 0.2458 - acc: 0.93 - ETA: 11s - loss: 0.2428 - acc: 0.93 - ETA: 10s - loss: 0.2493 - acc: 0.93 - ETA: 9s - loss: 0.2513 - acc: 0.9311 - ETA: 8s - loss: 0.2480 - acc: 0.932 - ETA: 7s - loss: 0.2454 - acc: 0.933 - ETA: 6s - loss: 0.2479 - acc: 0.932 - ETA: 5s - loss: 0.2487 - acc: 0.931 - ETA: 4s - loss: 0.2462 - acc: 0.932 - ETA: 3s - loss: 0.2477 - acc: 0.932 - ETA: 2s - loss: 0.2539 - acc: 0.929 - ETA: 1s - loss: 0.2549 - acc: 0.929 - ETA: 0s - loss: 0.2542 - acc: 0.929 - 50s 36ms/step - loss: 0.2558 - acc: 0.9286 - val_loss: 0.3127 - val_acc: 0.9049\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 47s - loss: 0.3101 - acc: 0.90 - ETA: 46s - loss: 0.2208 - acc: 0.93 - ETA: 52s - loss: 0.2398 - acc: 0.92 - ETA: 50s - loss: 0.2856 - acc: 0.91 - ETA: 47s - loss: 0.3204 - acc: 0.90 - ETA: 45s - loss: 0.3206 - acc: 0.90 - ETA: 44s - loss: 0.3273 - acc: 0.89 - ETA: 44s - loss: 0.3378 - acc: 0.89 - ETA: 43s - loss: 0.3253 - acc: 0.89 - ETA: 41s - loss: 0.3145 - acc: 0.90 - ETA: 39s - loss: 0.3008 - acc: 0.90 - ETA: 39s - loss: 0.2993 - acc: 0.90 - ETA: 38s - loss: 0.2980 - acc: 0.90 - ETA: 36s - loss: 0.2898 - acc: 0.91 - ETA: 35s - loss: 0.2972 - acc: 0.91 - ETA: 33s - loss: 0.2878 - acc: 0.91 - ETA: 33s - loss: 0.2845 - acc: 0.91 - ETA: 31s - loss: 0.2828 - acc: 0.91 - ETA: 30s - loss: 0.2808 - acc: 0.91 - ETA: 28s - loss: 0.2784 - acc: 0.91 - ETA: 28s - loss: 0.2767 - acc: 0.91 - ETA: 26s - loss: 0.2812 - acc: 0.91 - ETA: 25s - loss: 0.2729 - acc: 0.92 - ETA: 24s - loss: 0.2709 - acc: 0.92 - ETA: 22s - loss: 0.2721 - acc: 0.92 - ETA: 21s - loss: 0.2708 - acc: 0.92 - ETA: 20s - loss: 0.2707 - acc: 0.92 - ETA: 19s - loss: 0.2735 - acc: 0.92 - ETA: 17s - loss: 0.2817 - acc: 0.91 - ETA: 16s - loss: 0.2752 - acc: 0.91 - ETA: 15s - loss: 0.2691 - acc: 0.92 - ETA: 14s - loss: 0.2633 - acc: 0.92 - ETA: 12s - loss: 0.2604 - acc: 0.92 - ETA: 11s - loss: 0.2574 - acc: 0.92 - ETA: 10s - loss: 0.2593 - acc: 0.92 - ETA: 9s - loss: 0.2567 - acc: 0.9280 - ETA: 8s - loss: 0.2540 - acc: 0.929 - ETA: 6s - loss: 0.2516 - acc: 0.930 - ETA: 5s - loss: 0.2510 - acc: 0.930 - ETA: 4s - loss: 0.2498 - acc: 0.930 - ETA: 3s - loss: 0.2475 - acc: 0.931 - ETA: 1s - loss: 0.2509 - acc: 0.930 - ETA: 0s - loss: 0.2559 - acc: 0.928 - 64s 46ms/step - loss: 0.2544 - acc: 0.9286 - val_loss: 0.3203 - val_acc: 0.9049\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 2:13 - loss: 0.1411 - acc: 0.968 - ETA: 1:42 - loss: 0.2709 - acc: 0.921 - ETA: 1:22 - loss: 0.2759 - acc: 0.916 - ETA: 1:26 - loss: 0.2454 - acc: 0.929 - ETA: 1:20 - loss: 0.2438 - acc: 0.931 - ETA: 1:17 - loss: 0.2441 - acc: 0.932 - ETA: 1:16 - loss: 0.2372 - acc: 0.933 - ETA: 1:10 - loss: 0.2587 - acc: 0.925 - ETA: 1:05 - loss: 0.2670 - acc: 0.923 - ETA: 1:00 - loss: 0.2476 - acc: 0.931 - ETA: 58s - loss: 0.2456 - acc: 0.931 - ETA: 55s - loss: 0.2494 - acc: 0.92 - ETA: 52s - loss: 0.2535 - acc: 0.92 - ETA: 49s - loss: 0.2558 - acc: 0.92 - ETA: 46s - loss: 0.2600 - acc: 0.92 - ETA: 44s - loss: 0.2479 - acc: 0.92 - ETA: 42s - loss: 0.2375 - acc: 0.93 - ETA: 41s - loss: 0.2485 - acc: 0.92 - ETA: 39s - loss: 0.2524 - acc: 0.92 - ETA: 37s - loss: 0.2510 - acc: 0.92 - ETA: 35s - loss: 0.2464 - acc: 0.93 - ETA: 33s - loss: 0.2522 - acc: 0.92 - ETA: 31s - loss: 0.2515 - acc: 0.92 - ETA: 30s - loss: 0.2609 - acc: 0.92 - ETA: 28s - loss: 0.2619 - acc: 0.92 - ETA: 26s - loss: 0.2639 - acc: 0.92 - ETA: 24s - loss: 0.2686 - acc: 0.92 - ETA: 23s - loss: 0.2617 - acc: 0.92 - ETA: 21s - loss: 0.2660 - acc: 0.92 - ETA: 19s - loss: 0.2647 - acc: 0.92 - ETA: 18s - loss: 0.2733 - acc: 0.91 - ETA: 16s - loss: 0.2692 - acc: 0.92 - ETA: 15s - loss: 0.2726 - acc: 0.91 - ETA: 13s - loss: 0.2692 - acc: 0.92 - ETA: 12s - loss: 0.2681 - acc: 0.92 - ETA: 10s - loss: 0.2668 - acc: 0.92 - ETA: 9s - loss: 0.2637 - acc: 0.9231 - ETA: 7s - loss: 0.2592 - acc: 0.925 - ETA: 6s - loss: 0.2563 - acc: 0.926 - ETA: 4s - loss: 0.2536 - acc: 0.927 - ETA: 3s - loss: 0.2497 - acc: 0.929 - ETA: 1s - loss: 0.2512 - acc: 0.928 - ETA: 0s - loss: 0.2507 - acc: 0.928 - 66s 47ms/step - loss: 0.2508 - acc: 0.9286 - val_loss: 0.3137 - val_acc: 0.9049\n",
      "0.18 [0.06, 0.07] 0.48877374784110533 [0.07] 0.9084628670120898 [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
      "humanity\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - ETA: 49s - loss: 0.2619 - acc: 0.93 - ETA: 48s - loss: 0.1997 - acc: 0.95 - ETA: 51s - loss: 0.2010 - acc: 0.94 - ETA: 50s - loss: 0.2582 - acc: 0.92 - ETA: 48s - loss: 0.3078 - acc: 0.90 - ETA: 46s - loss: 0.2900 - acc: 0.91 - ETA: 44s - loss: 0.3028 - acc: 0.90 - ETA: 45s - loss: 0.3378 - acc: 0.89 - ETA: 43s - loss: 0.3695 - acc: 0.88 - ETA: 41s - loss: 0.3538 - acc: 0.89 - ETA: 40s - loss: 0.3519 - acc: 0.89 - ETA: 39s - loss: 0.3430 - acc: 0.89 - ETA: 38s - loss: 0.3335 - acc: 0.89 - ETA: 37s - loss: 0.3348 - acc: 0.89 - ETA: 35s - loss: 0.3440 - acc: 0.89 - ETA: 34s - loss: 0.3282 - acc: 0.90 - ETA: 33s - loss: 0.3225 - acc: 0.90 - ETA: 32s - loss: 0.3177 - acc: 0.90 - ETA: 30s - loss: 0.3321 - acc: 0.89 - ETA: 29s - loss: 0.3205 - acc: 0.90 - ETA: 28s - loss: 0.3152 - acc: 0.90 - ETA: 27s - loss: 0.3081 - acc: 0.90 - ETA: 25s - loss: 0.3089 - acc: 0.90 - ETA: 24s - loss: 0.3137 - acc: 0.90 - ETA: 22s - loss: 0.3077 - acc: 0.90 - ETA: 21s - loss: 0.3024 - acc: 0.91 - ETA: 20s - loss: 0.2995 - acc: 0.91 - ETA: 19s - loss: 0.2943 - acc: 0.91 - ETA: 18s - loss: 0.2954 - acc: 0.91 - ETA: 16s - loss: 0.2956 - acc: 0.91 - ETA: 15s - loss: 0.2956 - acc: 0.91 - ETA: 14s - loss: 0.2973 - acc: 0.91 - ETA: 13s - loss: 0.2958 - acc: 0.91 - ETA: 11s - loss: 0.2964 - acc: 0.91 - ETA: 10s - loss: 0.2942 - acc: 0.91 - ETA: 9s - loss: 0.2996 - acc: 0.9123 - ETA: 8s - loss: 0.2996 - acc: 0.912 - ETA: 6s - loss: 0.2958 - acc: 0.913 - ETA: 5s - loss: 0.2949 - acc: 0.914 - ETA: 4s - loss: 0.2957 - acc: 0.914 - ETA: 2s - loss: 0.2945 - acc: 0.914 - ETA: 1s - loss: 0.2947 - acc: 0.914 - ETA: 0s - loss: 0.2900 - acc: 0.916 - 59s 43ms/step - loss: 0.2905 - acc: 0.9164 - val_loss: 0.2825 - val_acc: 0.9164\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 1:08 - loss: 0.0962 - acc: 1.000 - ETA: 57s - loss: 0.1620 - acc: 0.968 - ETA: 52s - loss: 0.2581 - acc: 0.92 - ETA: 49s - loss: 0.2931 - acc: 0.91 - ETA: 48s - loss: 0.2610 - acc: 0.92 - ETA: 48s - loss: 0.2447 - acc: 0.93 - ETA: 46s - loss: 0.2429 - acc: 0.93 - ETA: 44s - loss: 0.2320 - acc: 0.93 - ETA: 42s - loss: 0.2315 - acc: 0.93 - ETA: 42s - loss: 0.2468 - acc: 0.93 - ETA: 41s - loss: 0.2457 - acc: 0.93 - ETA: 39s - loss: 0.2510 - acc: 0.92 - ETA: 37s - loss: 0.2507 - acc: 0.93 - ETA: 36s - loss: 0.2392 - acc: 0.93 - ETA: 35s - loss: 0.2497 - acc: 0.93 - ETA: 34s - loss: 0.2447 - acc: 0.93 - ETA: 33s - loss: 0.2529 - acc: 0.93 - ETA: 31s - loss: 0.2478 - acc: 0.93 - ETA: 32s - loss: 0.2434 - acc: 0.93 - ETA: 30s - loss: 0.2387 - acc: 0.93 - ETA: 29s - loss: 0.2482 - acc: 0.93 - ETA: 27s - loss: 0.2562 - acc: 0.92 - ETA: 26s - loss: 0.2552 - acc: 0.92 - ETA: 25s - loss: 0.2540 - acc: 0.92 - ETA: 23s - loss: 0.2594 - acc: 0.92 - ETA: 22s - loss: 0.2644 - acc: 0.92 - ETA: 21s - loss: 0.2633 - acc: 0.92 - ETA: 19s - loss: 0.2774 - acc: 0.91 - ETA: 18s - loss: 0.2760 - acc: 0.92 - ETA: 17s - loss: 0.2726 - acc: 0.92 - ETA: 15s - loss: 0.2733 - acc: 0.92 - ETA: 14s - loss: 0.2719 - acc: 0.92 - ETA: 13s - loss: 0.2747 - acc: 0.92 - ETA: 12s - loss: 0.2738 - acc: 0.92 - ETA: 10s - loss: 0.2761 - acc: 0.91 - ETA: 9s - loss: 0.2767 - acc: 0.9193 - ETA: 8s - loss: 0.2773 - acc: 0.918 - ETA: 6s - loss: 0.2795 - acc: 0.917 - ETA: 5s - loss: 0.2822 - acc: 0.916 - ETA: 4s - loss: 0.2824 - acc: 0.916 - ETA: 3s - loss: 0.2833 - acc: 0.916 - ETA: 1s - loss: 0.2821 - acc: 0.916 - ETA: 0s - loss: 0.2826 - acc: 0.916 - 64s 46ms/step - loss: 0.2827 - acc: 0.9164 - val_loss: 0.2834 - val_acc: 0.9164\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 1:43 - loss: 0.1474 - acc: 0.968 - ETA: 1:36 - loss: 0.1955 - acc: 0.953 - ETA: 1:39 - loss: 0.1833 - acc: 0.958 - ETA: 1:37 - loss: 0.1783 - acc: 0.960 - ETA: 1:36 - loss: 0.2017 - acc: 0.950 - ETA: 1:32 - loss: 0.2423 - acc: 0.932 - ETA: 1:34 - loss: 0.2416 - acc: 0.933 - ETA: 1:25 - loss: 0.2221 - acc: 0.941 - ETA: 1:18 - loss: 0.2133 - acc: 0.944 - ETA: 1:12 - loss: 0.2314 - acc: 0.937 - ETA: 1:07 - loss: 0.2306 - acc: 0.937 - ETA: 1:04 - loss: 0.2353 - acc: 0.934 - ETA: 59s - loss: 0.2338 - acc: 0.935 - ETA: 56s - loss: 0.2432 - acc: 0.93 - ETA: 53s - loss: 0.2419 - acc: 0.93 - ETA: 51s - loss: 0.2519 - acc: 0.92 - ETA: 48s - loss: 0.2454 - acc: 0.93 - ETA: 45s - loss: 0.2491 - acc: 0.92 - ETA: 43s - loss: 0.2444 - acc: 0.93 - ETA: 41s - loss: 0.2467 - acc: 0.92 - ETA: 38s - loss: 0.2584 - acc: 0.92 - ETA: 36s - loss: 0.2601 - acc: 0.92 - ETA: 34s - loss: 0.2582 - acc: 0.92 - ETA: 32s - loss: 0.2581 - acc: 0.92 - ETA: 30s - loss: 0.2644 - acc: 0.92 - ETA: 28s - loss: 0.2628 - acc: 0.92 - ETA: 26s - loss: 0.2646 - acc: 0.92 - ETA: 24s - loss: 0.2770 - acc: 0.91 - ETA: 23s - loss: 0.2750 - acc: 0.91 - ETA: 21s - loss: 0.2758 - acc: 0.91 - ETA: 19s - loss: 0.2792 - acc: 0.91 - ETA: 17s - loss: 0.2780 - acc: 0.91 - ETA: 16s - loss: 0.2766 - acc: 0.91 - ETA: 14s - loss: 0.2775 - acc: 0.91 - ETA: 12s - loss: 0.2803 - acc: 0.91 - ETA: 11s - loss: 0.2786 - acc: 0.91 - ETA: 9s - loss: 0.2755 - acc: 0.9189 - ETA: 8s - loss: 0.2796 - acc: 0.916 - ETA: 6s - loss: 0.2806 - acc: 0.916 - ETA: 5s - loss: 0.2826 - acc: 0.915 - ETA: 3s - loss: 0.2783 - acc: 0.917 - ETA: 2s - loss: 0.2834 - acc: 0.915 - ETA: 0s - loss: 0.2824 - acc: 0.915 - 70s 51ms/step - loss: 0.2810 - acc: 0.9164 - val_loss: 0.2817 - val_acc: 0.9164\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 48s - loss: 0.2822 - acc: 0.90 - ETA: 47s - loss: 0.2261 - acc: 0.93 - ETA: 45s - loss: 0.2771 - acc: 0.91 - ETA: 49s - loss: 0.2510 - acc: 0.92 - ETA: 47s - loss: 0.2625 - acc: 0.92 - ETA: 45s - loss: 0.2348 - acc: 0.93 - ETA: 43s - loss: 0.2579 - acc: 0.92 - ETA: 42s - loss: 0.2561 - acc: 0.92 - ETA: 42s - loss: 0.2665 - acc: 0.92 - ETA: 41s - loss: 0.2639 - acc: 0.92 - ETA: 39s - loss: 0.2614 - acc: 0.92 - ETA: 38s - loss: 0.2756 - acc: 0.91 - ETA: 37s - loss: 0.2862 - acc: 0.91 - ETA: 36s - loss: 0.2818 - acc: 0.91 - ETA: 34s - loss: 0.2886 - acc: 0.91 - ETA: 33s - loss: 0.2812 - acc: 0.91 - ETA: 32s - loss: 0.2815 - acc: 0.91 - ETA: 31s - loss: 0.2747 - acc: 0.91 - ETA: 30s - loss: 0.2764 - acc: 0.91 - ETA: 29s - loss: 0.2817 - acc: 0.91 - ETA: 27s - loss: 0.2787 - acc: 0.91 - ETA: 26s - loss: 0.2790 - acc: 0.91 - ETA: 25s - loss: 0.2706 - acc: 0.91 - ETA: 24s - loss: 0.2696 - acc: 0.92 - ETA: 22s - loss: 0.2711 - acc: 0.92 - ETA: 21s - loss: 0.2691 - acc: 0.92 - ETA: 20s - loss: 0.2696 - acc: 0.92 - ETA: 19s - loss: 0.2629 - acc: 0.92 - ETA: 17s - loss: 0.2618 - acc: 0.92 - ETA: 16s - loss: 0.2623 - acc: 0.92 - ETA: 15s - loss: 0.2582 - acc: 0.92 - ETA: 14s - loss: 0.2556 - acc: 0.92 - ETA: 12s - loss: 0.2583 - acc: 0.92 - ETA: 11s - loss: 0.2624 - acc: 0.92 - ETA: 10s - loss: 0.2632 - acc: 0.92 - ETA: 9s - loss: 0.2667 - acc: 0.9210 - ETA: 7s - loss: 0.2650 - acc: 0.921 - ETA: 6s - loss: 0.2683 - acc: 0.920 - ETA: 5s - loss: 0.2714 - acc: 0.919 - ETA: 4s - loss: 0.2735 - acc: 0.918 - ETA: 2s - loss: 0.2740 - acc: 0.917 - ETA: 1s - loss: 0.2747 - acc: 0.917 - ETA: 0s - loss: 0.2751 - acc: 0.917 - 58s 42ms/step - loss: 0.2770 - acc: 0.9164 - val_loss: 0.2810 - val_acc: 0.9164\n",
      "0.22 [0.0, 0.01, 0.02, 0.03, 0.04, 0.07] 0.45768566493955093 [0.07] 0.8756476683937824 [0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n"
     ]
    }
   ],
   "source": [
    "tag_results = {}\n",
    "for i in range(len(all_tags)):\n",
    "    tag = all_tags[i]\n",
    "    print(tag)\n",
    "    train_y_tag = train_y[tag]\n",
    "    valid_y_tag = valid_y[tag]\n",
    "    history = model.fit([X_train_headline, X_train_transcripts, X_train_tm, X_train_ner_seq, X_train_pos_seq], train_y_tag, batch_size=32, epochs=4, verbose=1, validation_split=0.2)\n",
    "    predictions = model.predict([X_test_headline, X_test_transcripts, X_test_tm, X_test_ner_seq, X_train_pos_seq])\n",
    "    highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i = get_threshold(tag,valid_y_tag,predictions)\n",
    "    print(highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i)\n",
    "    tag_results[tag] = [highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>global issues</th>\n",
       "      <th>technology</th>\n",
       "      <th>design</th>\n",
       "      <th>business</th>\n",
       "      <th>biomechanics</th>\n",
       "      <th>biodiversity</th>\n",
       "      <th>media</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>history</th>\n",
       "      <th>future</th>\n",
       "      <th>communication</th>\n",
       "      <th>humanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.16]</td>\n",
       "      <td>[0.12]</td>\n",
       "      <td>[0.04]</td>\n",
       "      <td>[0.05, 0.08]</td>\n",
       "      <td>[0.16, 0.18]</td>\n",
       "      <td>[0.11, 0.12, 0.13, 0.14, 0.15, 0.18]</td>\n",
       "      <td>[0.09, 0.1]</td>\n",
       "      <td>[0.06, 0.07]</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.0604491</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>0.303972</td>\n",
       "      <td>0.324698</td>\n",
       "      <td>0.207254</td>\n",
       "      <td>0.601036</td>\n",
       "      <td>0.592401</td>\n",
       "      <td>0.442142</td>\n",
       "      <td>0.214162</td>\n",
       "      <td>0.75475</td>\n",
       "      <td>0.426598</td>\n",
       "      <td>0.459413</td>\n",
       "      <td>0.488774</td>\n",
       "      <td>0.457686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.16]</td>\n",
       "      <td>[0.12]</td>\n",
       "      <td>[0.04]</td>\n",
       "      <td>[0.08]</td>\n",
       "      <td>[0.18]</td>\n",
       "      <td>[0.18]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[0.07]</td>\n",
       "      <td>[0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.939551</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>0.696028</td>\n",
       "      <td>0.675302</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>0.841105</td>\n",
       "      <td>0.901554</td>\n",
       "      <td>0.903282</td>\n",
       "      <td>0.882556</td>\n",
       "      <td>0.861831</td>\n",
       "      <td>0.834197</td>\n",
       "      <td>0.906736</td>\n",
       "      <td>0.908463</td>\n",
       "      <td>0.875648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46...</td>\n",
       "      <td>[0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45...</td>\n",
       "      <td>[0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42...</td>\n",
       "      <td>[0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.2...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32...</td>\n",
       "      <td>[0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.3...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...</td>\n",
       "      <td>[0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             culture  \\\n",
       "0                                               0.68   \n",
       "1  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "2                                           0.518135   \n",
       "3  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "4                                           0.518135   \n",
       "5  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "\n",
       "                                            politics  \\\n",
       "0                                               0.11   \n",
       "1  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "2                                          0.0604491   \n",
       "3  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "4                                           0.939551   \n",
       "5  [0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44...   \n",
       "\n",
       "                                             science  \\\n",
       "0                                                0.8   \n",
       "1  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "2                                           0.663212   \n",
       "3  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "4                                           0.663212   \n",
       "5  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "\n",
       "                                       global issues  \\\n",
       "0                                               0.47   \n",
       "1  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "2                                           0.303972   \n",
       "3  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "4                                           0.696028   \n",
       "5  [0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46...   \n",
       "\n",
       "                                          technology  \\\n",
       "0                                               0.49   \n",
       "1  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "2                                           0.324698   \n",
       "3  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "4                                           0.675302   \n",
       "5  [0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45...   \n",
       "\n",
       "                                              design  \\\n",
       "0                                               0.34   \n",
       "1  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "2                                           0.207254   \n",
       "3  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "4                                           0.792746   \n",
       "5  [0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42...   \n",
       "\n",
       "                                            business  \\\n",
       "0                                               0.32   \n",
       "1                                             [0.16]   \n",
       "2                                           0.601036   \n",
       "3                                             [0.16]   \n",
       "4                                           0.841105   \n",
       "5  [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.2...   \n",
       "\n",
       "                                        biomechanics  \\\n",
       "0                                               0.24   \n",
       "1                                             [0.12]   \n",
       "2                                           0.592401   \n",
       "3                                             [0.12]   \n",
       "4                                           0.901554   \n",
       "5  [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                        biodiversity  \\\n",
       "0                                                0.2   \n",
       "1                                             [0.04]   \n",
       "2                                           0.442142   \n",
       "3                                             [0.04]   \n",
       "4                                           0.903282   \n",
       "5  [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...   \n",
       "\n",
       "                                               media  \\\n",
       "0                                               0.22   \n",
       "1                                       [0.05, 0.08]   \n",
       "2                                           0.214162   \n",
       "3                                             [0.08]   \n",
       "4                                           0.882556   \n",
       "5  [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                       entertainment  \\\n",
       "0                                                0.3   \n",
       "1                                       [0.16, 0.18]   \n",
       "2                                            0.75475   \n",
       "3                                             [0.18]   \n",
       "4                                           0.861831   \n",
       "5  [0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32...   \n",
       "\n",
       "                                             history  \\\n",
       "0                                               0.31   \n",
       "1               [0.11, 0.12, 0.13, 0.14, 0.15, 0.18]   \n",
       "2                                           0.426598   \n",
       "3                                             [0.18]   \n",
       "4                                           0.834197   \n",
       "5  [0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.3...   \n",
       "\n",
       "                                              future  \\\n",
       "0                                                0.2   \n",
       "1                                        [0.09, 0.1]   \n",
       "2                                           0.459413   \n",
       "3                                              [0.1]   \n",
       "4                                           0.906736   \n",
       "5  [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                       communication  \\\n",
       "0                                               0.18   \n",
       "1                                       [0.06, 0.07]   \n",
       "2                                           0.488774   \n",
       "3                                             [0.07]   \n",
       "4                                           0.908463   \n",
       "5  [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...   \n",
       "\n",
       "                                            humanity  \n",
       "0                                               0.22  \n",
       "1                [0.0, 0.01, 0.02, 0.03, 0.04, 0.07]  \n",
       "2                                           0.457686  \n",
       "3                                             [0.07]  \n",
       "4                                           0.875648  \n",
       "5  [0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame.from_dict(tag_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>global issues</th>\n",
       "      <th>technology</th>\n",
       "      <th>design</th>\n",
       "      <th>business</th>\n",
       "      <th>biomechanics</th>\n",
       "      <th>biodiversity</th>\n",
       "      <th>media</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>history</th>\n",
       "      <th>future</th>\n",
       "      <th>communication</th>\n",
       "      <th>humanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_index</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.16]</td>\n",
       "      <td>[0.12]</td>\n",
       "      <td>[0.04]</td>\n",
       "      <td>[0.05, 0.08]</td>\n",
       "      <td>[0.16, 0.18]</td>\n",
       "      <td>[0.11, 0.12, 0.13, 0.14, 0.15, 0.18]</td>\n",
       "      <td>[0.09, 0.1]</td>\n",
       "      <td>[0.06, 0.07]</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_f1</th>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.0604491</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>0.303972</td>\n",
       "      <td>0.324698</td>\n",
       "      <td>0.207254</td>\n",
       "      <td>0.601036</td>\n",
       "      <td>0.592401</td>\n",
       "      <td>0.442142</td>\n",
       "      <td>0.214162</td>\n",
       "      <td>0.75475</td>\n",
       "      <td>0.426598</td>\n",
       "      <td>0.459413</td>\n",
       "      <td>0.488774</td>\n",
       "      <td>0.457686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_f1_index</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.16]</td>\n",
       "      <td>[0.12]</td>\n",
       "      <td>[0.04]</td>\n",
       "      <td>[0.08]</td>\n",
       "      <td>[0.18]</td>\n",
       "      <td>[0.18]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[0.07]</td>\n",
       "      <td>[0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.939551</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>0.696028</td>\n",
       "      <td>0.675302</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>0.841105</td>\n",
       "      <td>0.901554</td>\n",
       "      <td>0.903282</td>\n",
       "      <td>0.882556</td>\n",
       "      <td>0.861831</td>\n",
       "      <td>0.834197</td>\n",
       "      <td>0.906736</td>\n",
       "      <td>0.908463</td>\n",
       "      <td>0.875648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_index</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46...</td>\n",
       "      <td>[0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45...</td>\n",
       "      <td>[0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42...</td>\n",
       "      <td>[0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.2...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32...</td>\n",
       "      <td>[0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.3...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...</td>\n",
       "      <td>[0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             culture  \\\n",
       "f1                                                              0.68   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                                 0.518135   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                                    0.518135   \n",
       "accuracy_index     [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "\n",
       "                                                            politics  \\\n",
       "f1                                                              0.11   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                                0.0604491   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                                    0.939551   \n",
       "accuracy_index     [0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44...   \n",
       "\n",
       "                                                             science  \\\n",
       "f1                                                               0.8   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                                 0.663212   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                                    0.663212   \n",
       "accuracy_index     [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "\n",
       "                                                       global issues  \\\n",
       "f1                                                              0.47   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                                 0.303972   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                                    0.696028   \n",
       "accuracy_index     [0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46...   \n",
       "\n",
       "                                                          technology  \\\n",
       "f1                                                              0.49   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                                 0.324698   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                                    0.675302   \n",
       "accuracy_index     [0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45...   \n",
       "\n",
       "                                                              design  \\\n",
       "f1                                                              0.34   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                                 0.207254   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                                    0.792746   \n",
       "accuracy_index     [0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42...   \n",
       "\n",
       "                                                            business  \\\n",
       "f1                                                              0.32   \n",
       "f1_index                                                      [0.16]   \n",
       "accuracy_f1                                                 0.601036   \n",
       "accuracy_f1_index                                             [0.16]   \n",
       "accuracy                                                    0.841105   \n",
       "accuracy_index     [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.2...   \n",
       "\n",
       "                                                        biomechanics  \\\n",
       "f1                                                              0.24   \n",
       "f1_index                                                      [0.12]   \n",
       "accuracy_f1                                                 0.592401   \n",
       "accuracy_f1_index                                             [0.12]   \n",
       "accuracy                                                    0.901554   \n",
       "accuracy_index     [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                                        biodiversity  \\\n",
       "f1                                                               0.2   \n",
       "f1_index                                                      [0.04]   \n",
       "accuracy_f1                                                 0.442142   \n",
       "accuracy_f1_index                                             [0.04]   \n",
       "accuracy                                                    0.903282   \n",
       "accuracy_index     [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...   \n",
       "\n",
       "                                                               media  \\\n",
       "f1                                                              0.22   \n",
       "f1_index                                                [0.05, 0.08]   \n",
       "accuracy_f1                                                 0.214162   \n",
       "accuracy_f1_index                                             [0.08]   \n",
       "accuracy                                                    0.882556   \n",
       "accuracy_index     [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                                       entertainment  \\\n",
       "f1                                                               0.3   \n",
       "f1_index                                                [0.16, 0.18]   \n",
       "accuracy_f1                                                  0.75475   \n",
       "accuracy_f1_index                                             [0.18]   \n",
       "accuracy                                                    0.861831   \n",
       "accuracy_index     [0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32...   \n",
       "\n",
       "                                                             history  \\\n",
       "f1                                                              0.31   \n",
       "f1_index                        [0.11, 0.12, 0.13, 0.14, 0.15, 0.18]   \n",
       "accuracy_f1                                                 0.426598   \n",
       "accuracy_f1_index                                             [0.18]   \n",
       "accuracy                                                    0.834197   \n",
       "accuracy_index     [0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.3...   \n",
       "\n",
       "                                                              future  \\\n",
       "f1                                                               0.2   \n",
       "f1_index                                                 [0.09, 0.1]   \n",
       "accuracy_f1                                                 0.459413   \n",
       "accuracy_f1_index                                              [0.1]   \n",
       "accuracy                                                    0.906736   \n",
       "accuracy_index     [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                                       communication  \\\n",
       "f1                                                              0.18   \n",
       "f1_index                                                [0.06, 0.07]   \n",
       "accuracy_f1                                                 0.488774   \n",
       "accuracy_f1_index                                             [0.07]   \n",
       "accuracy                                                    0.908463   \n",
       "accuracy_index     [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...   \n",
       "\n",
       "                                                            humanity  \n",
       "f1                                                              0.22  \n",
       "f1_index                         [0.0, 0.01, 0.02, 0.03, 0.04, 0.07]  \n",
       "accuracy_f1                                                 0.457686  \n",
       "accuracy_f1_index                                             [0.07]  \n",
       "accuracy                                                    0.875648  \n",
       "accuracy_index     [0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.rename(index = {0:'f1',1:'f1_index',2:'accuracy_f1',3:'accuracy_f1_index',4:'accuracy',5:'accuracy_index'})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "culture          object\n",
       "politics         object\n",
       "science          object\n",
       "global issues    object\n",
       "technology       object\n",
       "design           object\n",
       "business         object\n",
       "biomechanics     object\n",
       "biodiversity     object\n",
       "media            object\n",
       "entertainment    object\n",
       "history          object\n",
       "future           object\n",
       "communication    object\n",
       "humanity         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(DATA_DIR+'multi_headline_transcript_tm_ner_pos_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>global issues</th>\n",
       "      <th>technology</th>\n",
       "      <th>design</th>\n",
       "      <th>business</th>\n",
       "      <th>biomechanics</th>\n",
       "      <th>biodiversity</th>\n",
       "      <th>media</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>history</th>\n",
       "      <th>future</th>\n",
       "      <th>communication</th>\n",
       "      <th>humanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_index</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.16]</td>\n",
       "      <td>[0.12]</td>\n",
       "      <td>[0.04]</td>\n",
       "      <td>[0.05, 0.08]</td>\n",
       "      <td>[0.16, 0.18]</td>\n",
       "      <td>[0.11, 0.12, 0.13, 0.14, 0.15, 0.18]</td>\n",
       "      <td>[0.09, 0.1]</td>\n",
       "      <td>[0.06, 0.07]</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_f1</th>\n",
       "      <td>0.5181347150259067</td>\n",
       "      <td>0.06044905008635579</td>\n",
       "      <td>0.6632124352331606</td>\n",
       "      <td>0.30397236614853196</td>\n",
       "      <td>0.32469775474956825</td>\n",
       "      <td>0.20725388601036268</td>\n",
       "      <td>0.6010362694300518</td>\n",
       "      <td>0.5924006908462867</td>\n",
       "      <td>0.4421416234887737</td>\n",
       "      <td>0.2141623488773748</td>\n",
       "      <td>0.7547495682210709</td>\n",
       "      <td>0.4265975820379965</td>\n",
       "      <td>0.459412780656304</td>\n",
       "      <td>0.48877374784110533</td>\n",
       "      <td>0.45768566493955093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_f1_index</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.16]</td>\n",
       "      <td>[0.12]</td>\n",
       "      <td>[0.04]</td>\n",
       "      <td>[0.08]</td>\n",
       "      <td>[0.18]</td>\n",
       "      <td>[0.18]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[0.07]</td>\n",
       "      <td>[0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.5181347150259067</td>\n",
       "      <td>0.9395509499136442</td>\n",
       "      <td>0.6632124352331606</td>\n",
       "      <td>0.696027633851468</td>\n",
       "      <td>0.6753022452504318</td>\n",
       "      <td>0.7927461139896373</td>\n",
       "      <td>0.8411053540587219</td>\n",
       "      <td>0.9015544041450777</td>\n",
       "      <td>0.9032815198618307</td>\n",
       "      <td>0.8825561312607945</td>\n",
       "      <td>0.8618307426597582</td>\n",
       "      <td>0.8341968911917098</td>\n",
       "      <td>0.9067357512953368</td>\n",
       "      <td>0.9084628670120898</td>\n",
       "      <td>0.8756476683937824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_index</th>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44...</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>[0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46...</td>\n",
       "      <td>[0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45...</td>\n",
       "      <td>[0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42...</td>\n",
       "      <td>[0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.2...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32...</td>\n",
       "      <td>[0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.3...</td>\n",
       "      <td>[0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...</td>\n",
       "      <td>[0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...</td>\n",
       "      <td>[0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             culture  \\\n",
       "f1                                                              0.68   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                       0.5181347150259067   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                          0.5181347150259067   \n",
       "accuracy_index     [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "\n",
       "                                                            politics  \\\n",
       "f1                                                              0.11   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                      0.06044905008635579   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                          0.9395509499136442   \n",
       "accuracy_index     [0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44...   \n",
       "\n",
       "                                                             science  \\\n",
       "f1                                                               0.8   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                       0.6632124352331606   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                          0.6632124352331606   \n",
       "accuracy_index     [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "\n",
       "                                                       global issues  \\\n",
       "f1                                                              0.47   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                      0.30397236614853196   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                           0.696027633851468   \n",
       "accuracy_index     [0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46...   \n",
       "\n",
       "                                                          technology  \\\n",
       "f1                                                              0.49   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                      0.32469775474956825   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                          0.6753022452504318   \n",
       "accuracy_index     [0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45...   \n",
       "\n",
       "                                                              design  \\\n",
       "f1                                                              0.34   \n",
       "f1_index           [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy_f1                                      0.20725388601036268   \n",
       "accuracy_f1_index  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "accuracy                                          0.7927461139896373   \n",
       "accuracy_index     [0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42...   \n",
       "\n",
       "                                                            business  \\\n",
       "f1                                                              0.32   \n",
       "f1_index                                                      [0.16]   \n",
       "accuracy_f1                                       0.6010362694300518   \n",
       "accuracy_f1_index                                             [0.16]   \n",
       "accuracy                                          0.8411053540587219   \n",
       "accuracy_index     [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.2...   \n",
       "\n",
       "                                                        biomechanics  \\\n",
       "f1                                                              0.24   \n",
       "f1_index                                                      [0.12]   \n",
       "accuracy_f1                                       0.5924006908462867   \n",
       "accuracy_f1_index                                             [0.12]   \n",
       "accuracy                                          0.9015544041450777   \n",
       "accuracy_index     [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                                        biodiversity  \\\n",
       "f1                                                               0.2   \n",
       "f1_index                                                      [0.04]   \n",
       "accuracy_f1                                       0.4421416234887737   \n",
       "accuracy_f1_index                                             [0.04]   \n",
       "accuracy                                          0.9032815198618307   \n",
       "accuracy_index     [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...   \n",
       "\n",
       "                                                               media  \\\n",
       "f1                                                              0.22   \n",
       "f1_index                                                [0.05, 0.08]   \n",
       "accuracy_f1                                       0.2141623488773748   \n",
       "accuracy_f1_index                                             [0.08]   \n",
       "accuracy                                          0.8825561312607945   \n",
       "accuracy_index     [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                                       entertainment  \\\n",
       "f1                                                               0.3   \n",
       "f1_index                                                [0.16, 0.18]   \n",
       "accuracy_f1                                       0.7547495682210709   \n",
       "accuracy_f1_index                                             [0.18]   \n",
       "accuracy                                          0.8618307426597582   \n",
       "accuracy_index     [0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32...   \n",
       "\n",
       "                                                             history  \\\n",
       "f1                                                              0.31   \n",
       "f1_index                        [0.11, 0.12, 0.13, 0.14, 0.15, 0.18]   \n",
       "accuracy_f1                                       0.4265975820379965   \n",
       "accuracy_f1_index                                             [0.18]   \n",
       "accuracy                                          0.8341968911917098   \n",
       "accuracy_index     [0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.3...   \n",
       "\n",
       "                                                              future  \\\n",
       "f1                                                               0.2   \n",
       "f1_index                                                 [0.09, 0.1]   \n",
       "accuracy_f1                                        0.459412780656304   \n",
       "accuracy_f1_index                                              [0.1]   \n",
       "accuracy                                          0.9067357512953368   \n",
       "accuracy_index     [0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25...   \n",
       "\n",
       "                                                       communication  \\\n",
       "f1                                                              0.18   \n",
       "f1_index                                                [0.06, 0.07]   \n",
       "accuracy_f1                                      0.48877374784110533   \n",
       "accuracy_f1_index                                             [0.07]   \n",
       "accuracy                                          0.9084628670120898   \n",
       "accuracy_index     [0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24...   \n",
       "\n",
       "                                                            humanity  \n",
       "f1                                                              0.22  \n",
       "f1_index                         [0.0, 0.01, 0.02, 0.03, 0.04, 0.07]  \n",
       "accuracy_f1                                      0.45768566493955093  \n",
       "accuracy_f1_index                                             [0.07]  \n",
       "accuracy                                          0.8756476683937824  \n",
       "accuracy_index     [0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_csv = pd.read_csv(DATA_DIR+'multi_headline_transcript_tm_ner_pos_results.csv',index_col = 0)\n",
    "results_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    squash15_tags  counts  no_count     ratio  overall_ratio\n",
      "0         science    1467       846  1.734043       0.634241\n",
      "1         culture    1155      1158  0.997409       0.499351\n",
      "2      technology     787      1526  0.515727       0.340251\n",
      "3   global issues     679      1634  0.415545       0.293558\n",
      "4          design     477      1836  0.259804       0.206226\n",
      "5         history     385      1928  0.199689       0.166450\n",
      "6        business     349      1964  0.177699       0.150886\n",
      "7   entertainment     285      2028  0.140533       0.123217\n",
      "8           media     279      2034  0.137168       0.120623\n",
      "9    biomechanics     220      2093  0.105112       0.095115\n",
      "10         future     218      2095  0.104057       0.094250\n",
      "11   biodiversity     218      2095  0.104057       0.094250\n",
      "12       humanity     217      2096  0.103531       0.093818\n",
      "13       politics     199      2114  0.094134       0.086035\n",
      "14  communication     185      2128  0.086936       0.079983\n"
     ]
    }
   ],
   "source": [
    "print_full_dataframe(squashed_tag_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41]\n",
      "[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41]\n"
     ]
    }
   ],
   "source": [
    "print(results['culture']['f1_index'])\n",
    "print(results['culture']['accuracy_f1_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - ETA: 48s - loss: 0.7071 - acc: 0.68 - ETA: 54s - loss: 0.8374 - acc: 0.60 - ETA: 50s - loss: 0.9577 - acc: 0.54 - ETA: 47s - loss: 1.0075 - acc: 0.51 - ETA: 45s - loss: 1.0240 - acc: 0.50 - ETA: 45s - loss: 1.0560 - acc: 0.47 - ETA: 44s - loss: 1.0740 - acc: 0.45 - ETA: 42s - loss: 1.0404 - acc: 0.47 - ETA: 40s - loss: 1.0575 - acc: 0.45 - ETA: 39s - loss: 1.0349 - acc: 0.46 - ETA: 38s - loss: 1.0154 - acc: 0.47 - ETA: 37s - loss: 1.0129 - acc: 0.47 - ETA: 36s - loss: 0.9947 - acc: 0.48 - ETA: 35s - loss: 0.9877 - acc: 0.48 - ETA: 33s - loss: 0.9729 - acc: 0.48 - ETA: 33s - loss: 0.9628 - acc: 0.49 - ETA: 31s - loss: 0.9587 - acc: 0.49 - ETA: 30s - loss: 0.9536 - acc: 0.49 - ETA: 29s - loss: 0.9473 - acc: 0.49 - ETA: 28s - loss: 0.9353 - acc: 0.50 - ETA: 27s - loss: 0.9264 - acc: 0.50 - ETA: 25s - loss: 0.9198 - acc: 0.50 - ETA: 24s - loss: 0.9210 - acc: 0.50 - ETA: 23s - loss: 0.9263 - acc: 0.49 - ETA: 22s - loss: 0.9229 - acc: 0.49 - ETA: 21s - loss: 0.9169 - acc: 0.49 - ETA: 19s - loss: 0.9118 - acc: 0.49 - ETA: 18s - loss: 0.9040 - acc: 0.50 - ETA: 17s - loss: 0.9010 - acc: 0.50 - ETA: 16s - loss: 0.8906 - acc: 0.50 - ETA: 15s - loss: 0.8881 - acc: 0.50 - ETA: 13s - loss: 0.8891 - acc: 0.50 - ETA: 12s - loss: 0.8852 - acc: 0.50 - ETA: 11s - loss: 0.8783 - acc: 0.51 - ETA: 10s - loss: 0.8802 - acc: 0.50 - ETA: 9s - loss: 0.8760 - acc: 0.5078 - ETA: 7s - loss: 0.8714 - acc: 0.510 - ETA: 6s - loss: 0.8679 - acc: 0.511 - ETA: 5s - loss: 0.8656 - acc: 0.512 - ETA: 4s - loss: 0.8628 - acc: 0.512 - ETA: 2s - loss: 0.8598 - acc: 0.513 - ETA: 1s - loss: 0.8532 - acc: 0.517 - ETA: 0s - loss: 0.8545 - acc: 0.513 - 57s 41ms/step - loss: 0.8545 - acc: 0.5133 - val_loss: 0.7822 - val_acc: 0.4813\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - ETA: 1:07 - loss: 0.8502 - acc: 0.406 - ETA: 56s - loss: 0.8373 - acc: 0.437 - ETA: 52s - loss: 0.7974 - acc: 0.47 - ETA: 49s - loss: 0.7795 - acc: 0.50 - ETA: 47s - loss: 0.7599 - acc: 0.52 - ETA: 48s - loss: 0.7571 - acc: 0.52 - ETA: 46s - loss: 0.7523 - acc: 0.53 - ETA: 44s - loss: 0.7454 - acc: 0.53 - ETA: 42s - loss: 0.7460 - acc: 0.53 - ETA: 42s - loss: 0.7518 - acc: 0.52 - ETA: 41s - loss: 0.7603 - acc: 0.51 - ETA: 39s - loss: 0.7729 - acc: 0.49 - ETA: 38s - loss: 0.7716 - acc: 0.49 - ETA: 37s - loss: 0.7694 - acc: 0.50 - ETA: 36s - loss: 0.7693 - acc: 0.50 - ETA: 34s - loss: 0.7668 - acc: 0.50 - ETA: 33s - loss: 0.7716 - acc: 0.49 - ETA: 31s - loss: 0.7683 - acc: 0.49 - ETA: 31s - loss: 0.7665 - acc: 0.50 - ETA: 30s - loss: 0.7659 - acc: 0.50 - ETA: 28s - loss: 0.7618 - acc: 0.50 - ETA: 27s - loss: 0.7638 - acc: 0.50 - ETA: 26s - loss: 0.7588 - acc: 0.50 - ETA: 24s - loss: 0.7593 - acc: 0.50 - ETA: 23s - loss: 0.7556 - acc: 0.51 - ETA: 22s - loss: 0.7551 - acc: 0.51 - ETA: 21s - loss: 0.7538 - acc: 0.51 - ETA: 19s - loss: 0.7529 - acc: 0.51 - ETA: 18s - loss: 0.7555 - acc: 0.50 - ETA: 17s - loss: 0.7529 - acc: 0.51 - ETA: 15s - loss: 0.7515 - acc: 0.51 - ETA: 14s - loss: 0.7523 - acc: 0.51 - ETA: 13s - loss: 0.7494 - acc: 0.51 - ETA: 11s - loss: 0.7480 - acc: 0.51 - ETA: 10s - loss: 0.7492 - acc: 0.51 - ETA: 9s - loss: 0.7510 - acc: 0.5122 - ETA: 8s - loss: 0.7507 - acc: 0.511 - ETA: 6s - loss: 0.7516 - acc: 0.510 - ETA: 5s - loss: 0.7510 - acc: 0.511 - ETA: 4s - loss: 0.7516 - acc: 0.510 - ETA: 2s - loss: 0.7522 - acc: 0.508 - ETA: 1s - loss: 0.7502 - acc: 0.511 - ETA: 0s - loss: 0.7481 - acc: 0.514 - 59s 42ms/step - loss: 0.7486 - acc: 0.5133 - val_loss: 0.7517 - val_acc: 0.4813\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - ETA: 50s - loss: 0.7640 - acc: 0.46 - ETA: 48s - loss: 0.7952 - acc: 0.43 - ETA: 46s - loss: 0.7880 - acc: 0.44 - ETA: 49s - loss: 0.7870 - acc: 0.44 - ETA: 47s - loss: 0.7867 - acc: 0.44 - ETA: 45s - loss: 0.7751 - acc: 0.45 - ETA: 44s - loss: 0.7624 - acc: 0.47 - ETA: 44s - loss: 0.7523 - acc: 0.49 - ETA: 42s - loss: 0.7549 - acc: 0.48 - ETA: 41s - loss: 0.7550 - acc: 0.48 - ETA: 39s - loss: 0.7428 - acc: 0.50 - ETA: 38s - loss: 0.7444 - acc: 0.50 - ETA: 37s - loss: 0.7383 - acc: 0.51 - ETA: 36s - loss: 0.7369 - acc: 0.51 - ETA: 35s - loss: 0.7362 - acc: 0.51 - ETA: 33s - loss: 0.7352 - acc: 0.51 - ETA: 32s - loss: 0.7333 - acc: 0.52 - ETA: 31s - loss: 0.7334 - acc: 0.51 - ETA: 30s - loss: 0.7313 - acc: 0.52 - ETA: 28s - loss: 0.7292 - acc: 0.52 - ETA: 27s - loss: 0.7269 - acc: 0.52 - ETA: 26s - loss: 0.7251 - acc: 0.53 - ETA: 25s - loss: 0.7249 - acc: 0.53 - ETA: 23s - loss: 0.7224 - acc: 0.53 - ETA: 22s - loss: 0.7255 - acc: 0.53 - ETA: 21s - loss: 0.7246 - acc: 0.53 - ETA: 20s - loss: 0.7277 - acc: 0.52 - ETA: 19s - loss: 0.7290 - acc: 0.52 - ETA: 17s - loss: 0.7268 - acc: 0.52 - ETA: 16s - loss: 0.7276 - acc: 0.52 - ETA: 15s - loss: 0.7289 - acc: 0.52 - ETA: 14s - loss: 0.7281 - acc: 0.52 - ETA: 12s - loss: 0.7285 - acc: 0.52 - ETA: 11s - loss: 0.7290 - acc: 0.52 - ETA: 10s - loss: 0.7310 - acc: 0.52 - ETA: 9s - loss: 0.7326 - acc: 0.5182 - ETA: 7s - loss: 0.7351 - acc: 0.514 - ETA: 6s - loss: 0.7371 - acc: 0.511 - ETA: 5s - loss: 0.7368 - acc: 0.512 - ETA: 4s - loss: 0.7372 - acc: 0.510 - ETA: 2s - loss: 0.7365 - acc: 0.512 - ETA: 1s - loss: 0.7369 - acc: 0.511 - ETA: 0s - loss: 0.7362 - acc: 0.512 - 57s 41ms/step - loss: 0.7355 - acc: 0.5133 - val_loss: 0.7435 - val_acc: 0.4813\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - ETA: 47s - loss: 0.7068 - acc: 0.56 - ETA: 56s - loss: 0.7099 - acc: 0.54 - ETA: 52s - loss: 0.7106 - acc: 0.54 - ETA: 49s - loss: 0.6997 - acc: 0.56 - ETA: 47s - loss: 0.6965 - acc: 0.56 - ETA: 45s - loss: 0.6925 - acc: 0.57 - ETA: 46s - loss: 0.6985 - acc: 0.56 - ETA: 44s - loss: 0.7097 - acc: 0.55 - ETA: 43s - loss: 0.7102 - acc: 0.54 - ETA: 41s - loss: 0.7186 - acc: 0.53 - ETA: 41s - loss: 0.7198 - acc: 0.53 - ETA: 39s - loss: 0.7246 - acc: 0.52 - ETA: 38s - loss: 0.7252 - acc: 0.52 - ETA: 36s - loss: 0.7290 - acc: 0.51 - ETA: 35s - loss: 0.7260 - acc: 0.52 - ETA: 34s - loss: 0.7285 - acc: 0.51 - ETA: 33s - loss: 0.7303 - acc: 0.51 - ETA: 31s - loss: 0.7238 - acc: 0.52 - ETA: 30s - loss: 0.7262 - acc: 0.51 - ETA: 29s - loss: 0.7285 - acc: 0.51 - ETA: 28s - loss: 0.7248 - acc: 0.52 - ETA: 26s - loss: 0.7213 - acc: 0.52 - ETA: 25s - loss: 0.7236 - acc: 0.52 - ETA: 24s - loss: 0.7268 - acc: 0.51 - ETA: 23s - loss: 0.7263 - acc: 0.51 - ETA: 21s - loss: 0.7244 - acc: 0.52 - ETA: 20s - loss: 0.7236 - acc: 0.52 - ETA: 19s - loss: 0.7238 - acc: 0.52 - ETA: 18s - loss: 0.7264 - acc: 0.51 - ETA: 16s - loss: 0.7278 - acc: 0.51 - ETA: 15s - loss: 0.7257 - acc: 0.51 - ETA: 14s - loss: 0.7258 - acc: 0.51 - ETA: 13s - loss: 0.7249 - acc: 0.51 - ETA: 11s - loss: 0.7273 - acc: 0.51 - ETA: 10s - loss: 0.7289 - acc: 0.51 - ETA: 9s - loss: 0.7275 - acc: 0.5130 - ETA: 7s - loss: 0.7288 - acc: 0.510 - ETA: 6s - loss: 0.7283 - acc: 0.510 - ETA: 5s - loss: 0.7287 - acc: 0.509 - ETA: 4s - loss: 0.7269 - acc: 0.512 - ETA: 2s - loss: 0.7262 - acc: 0.513 - ETA: 1s - loss: 0.7251 - acc: 0.515 - ETA: 0s - loss: 0.7272 - acc: 0.511 - 59s 43ms/step - loss: 0.7262 - acc: 0.5133 - val_loss: 0.7366 - val_acc: 0.4813\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall    f1  accuracy\n",
      "0  culture       0.52     1.0  0.68  0.518135\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n",
      "     class  precision  recall   f1  accuracy\n",
      "0  culture        0.0     0.0  0.0  0.481865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37] 0.5181347150259067 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37] 0.5181347150259067 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37]\n"
     ]
    }
   ],
   "source": [
    "find_threshold('culture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
