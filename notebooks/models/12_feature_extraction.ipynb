{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links\n",
    "\n",
    "1. For the architecture https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada\n",
    "2. https://androidkt.com/multi-label-text-classification-in-tensorflow-keras/\n",
    "3. https://keras.io/preprocessing/sequence/\n",
    "4. https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/ ( Not really)\n",
    "5. For deep learning using word embeddings https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/processed/\"\n",
    "INPUT_FILE_NAME = 'cleaned_squashed15_final.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>sim_tags</th>\n",
       "      <th>squash15_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>[thank, chris, truly, great, honor, opportunit...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>cars,solar system,energy,culture,politics,scie...</td>\n",
       "      <td>culture,politics,science,global issues,technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>[term, invention, like, tell, tale, favorite, ...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>macarthur grant,simplicity,design,solar system...</td>\n",
       "      <td>design,global issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>[public, dewey, long, ago, observe, constitute...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>corruption,inequality,science,investment,war,c...</td>\n",
       "      <td>science,culture,politics,global issues,business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>[want, start, say, houston, problem, enter, se...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>flight,design,nasa,science,invention,entrepren...</td>\n",
       "      <td>design,science,business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>[want, talk, background, idea, car, art, actua...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>cars,design,transportation,invention,technolog...</td>\n",
       "      <td>design,technology,business,science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker                              headline  \\\n",
       "0       Al Gore           Averting the climate crisis   \n",
       "1     Amy Smith         Simple designs to save a life   \n",
       "2  Ashraf Ghani         How to rebuild a broken state   \n",
       "3    Burt Rutan  The real future of space exploration   \n",
       "4  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                         description duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4  American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                tags  \\\n",
       "0  cars,alternative energy,culture,politics,scien...   \n",
       "1  MacArthur grant,simplicity,industrial design,a...   \n",
       "2  corruption,poverty,economics,investment,milita...   \n",
       "3  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript      WC  \\\n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "\n",
       "                                    clean_transcript  \\\n",
       "0  [thank, chris, truly, great, honor, opportunit...   \n",
       "1  [term, invention, like, tell, tale, favorite, ...   \n",
       "2  [public, dewey, long, ago, observe, constitute...   \n",
       "3  [want, start, say, houston, problem, enter, se...   \n",
       "4  [want, talk, background, idea, car, art, actua...   \n",
       "\n",
       "                             clean_transcript_string  \\\n",
       "0  thank chris truly great honor opportunity come...   \n",
       "1  term invention like tell tale favorite project...   \n",
       "2  public dewey long ago observe constitute discu...   \n",
       "3  want start say houston problem enter second ge...   \n",
       "4  want talk background idea car art actually mea...   \n",
       "\n",
       "                                            sim_tags  \\\n",
       "0  cars,solar system,energy,culture,politics,scie...   \n",
       "1  macarthur grant,simplicity,design,solar system...   \n",
       "2  corruption,inequality,science,investment,war,c...   \n",
       "3  flight,design,nasa,science,invention,entrepren...   \n",
       "4  cars,design,transportation,invention,technolog...   \n",
       "\n",
       "                                       squash15_tags  \n",
       "0  culture,politics,science,global issues,technology  \n",
       "1                               design,global issues  \n",
       "2    science,culture,politics,global issues,business  \n",
       "3                            design,science,business  \n",
       "4                 design,technology,business,science  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (42.0.1.post20191125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.17.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.2.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.23)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.39.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\kayes\\anaconda3\\envs\\cds\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (7.2.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get POS representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pos_sequence_full = []\n",
    "# for doc in nlp.pipe(df['clean_transcript_string']):\n",
    "# #     doc = nlp(rows['clean_transcript_string'])\n",
    "# #     pos_sequence = ''\n",
    "# #     for token in doc:\n",
    "# #         pos_sequence += token.pos_ + ' '\n",
    "#     pos_sequence = ' '.join(token.pos_ for token in doc)\n",
    "#     print(pos_sequence)\n",
    "#     pos_sequence_full.append(pos_sequence)\n",
    "# df['pos_sequence'] = pos_sequence_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pos_sequence_full = ([' '.join(token.pos_ for token in doc)] for doc in nlp.pipe(df['clean_transcript_string']))\n",
    "# for doc in nlp.pipe(df.loc[ :5 ,'clean_transcript_string']):\n",
    "# #     doc = nlp(rows['clean_transcript_string'])\n",
    "# #     pos_sequence = ''\n",
    "# #     for token in doc:\n",
    "# #         pos_sequence += token.pos_ + ' '\n",
    "#     pos_sequence = ' '.join(token.pos_ for token in doc)\n",
    "#     print(pos_sequence)\n",
    "#     pos_sequence_full.append(pos_sequence)\n",
    "# pos_sequence_full\n",
    "df['pos_sequence'] = pd.Series((' '.join(token.pos_ for token in doc)) for doc in nlp.pipe(df['clean_transcript_string']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get named entity representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_sequence_full = []\n",
    "# for doc in nlp.pipe(df['clean_transcript_string']):\n",
    "# #     doc = nlp(rows['clean_transcript_string'])\n",
    "#     ner_sequence = ''\n",
    "#     for ent in doc.ents:\n",
    "#         ner_sequence += ent.label_ + ' '\n",
    "#     ner_sequence_full.append(ner_sequence)\n",
    "# df['ner_sequence'] = ner_sequence_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['ner_sequence'] = pd.Series((' '.join(ent.label_ for ent in doc.ents)) for doc in nlp.pipe(df['clean_transcript_string']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>sim_tags</th>\n",
       "      <th>squash15_tags</th>\n",
       "      <th>pos_sequence</th>\n",
       "      <th>ner_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>[thank, chris, truly, great, honor, opportunit...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>cars,solar system,energy,culture,politics,scie...</td>\n",
       "      <td>culture,politics,science,global issues,technology</td>\n",
       "      <td>VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...</td>\n",
       "      <td>PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>[term, invention, like, tell, tale, favorite, ...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>macarthur grant,simplicity,design,solar system...</td>\n",
       "      <td>design,global issues</td>\n",
       "      <td>NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...</td>\n",
       "      <td>GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>[public, dewey, long, ago, observe, constitute...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>corruption,inequality,science,investment,war,c...</td>\n",
       "      <td>science,culture,politics,global issues,business</td>\n",
       "      <td>ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...</td>\n",
       "      <td>DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>[want, start, say, houston, problem, enter, se...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>flight,design,nasa,science,invention,entrepren...</td>\n",
       "      <td>design,science,business</td>\n",
       "      <td>VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>[want, talk, background, idea, car, art, actua...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>cars,design,transportation,invention,technolog...</td>\n",
       "      <td>design,technology,business,science</td>\n",
       "      <td>VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...</td>\n",
       "      <td>PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>Justin Davidson</td>\n",
       "      <td>Why glass towers are bad for city life -- and ...</td>\n",
       "      <td>There's a creepy transformation taking over ou...</td>\n",
       "      <td>0:12:39</td>\n",
       "      <td>architecture,beauty,cities,community,journalis...</td>\n",
       "      <td>0:11\\r\\r\\rImagine that when you walked\\rin her...</td>\n",
       "      <td>1693.0</td>\n",
       "      <td>[imagine, walk, even, discover, everybody, roo...</td>\n",
       "      <td>imagine walk even discover everybody room look...</td>\n",
       "      <td>architecture,beauty,cities,community,journalis...</td>\n",
       "      <td>science,history,culture</td>\n",
       "      <td>VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...</td>\n",
       "      <td>ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>Mehdi Ordikhani-Seyedlar</td>\n",
       "      <td>What happens in your brain when you pay attent...</td>\n",
       "      <td>Attention isn't just about what we focus on  i...</td>\n",
       "      <td>0:06:32</td>\n",
       "      <td>AI,algorithm,brain,cognitive science,machine l...</td>\n",
       "      <td>0:11\\r\\r\\rPaying close attention to something:...</td>\n",
       "      <td>846.0</td>\n",
       "      <td>[pay, close, attention, easy, attention, pull,...</td>\n",
       "      <td>pay close attention easy attention pull differ...</td>\n",
       "      <td>ai,algorithm,brain,science,machine learning,te...</td>\n",
       "      <td>science,technology</td>\n",
       "      <td>VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...</td>\n",
       "      <td>ORDINAL PERSON PRODUCT DATE DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>Tim Ferriss</td>\n",
       "      <td>Why you should define your fears instead of yo...</td>\n",
       "      <td>The hard choices  what we most fear doing, ask...</td>\n",
       "      <td>0:13:21</td>\n",
       "      <td>choice,fear,goal-setting,humanity,life,persona...</td>\n",
       "      <td>0:11\\r\\r\\rSo, this happy pic of me\\rwas taken ...</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>[happy, pic, take, senior, college, right, dan...</td>\n",
       "      <td>happy pic take senior college right dance prac...</td>\n",
       "      <td>choice,fear,goal-setting,humanity,life,culture...</td>\n",
       "      <td>humanity,culture</td>\n",
       "      <td>ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...</td>\n",
       "      <td>DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>Anne Lamott</td>\n",
       "      <td>12 truths I learned from life and writing</td>\n",
       "      <td>A few days before she turned 61, writer Anne L...</td>\n",
       "      <td>0:15:55</td>\n",
       "      <td>Christianity,God,aging,art,beauty,birds,books,...</td>\n",
       "      <td>0:12\\r\\r\\rMy seven-year-old grandson\\rsleeps j...</td>\n",
       "      <td>2255.0</td>\n",
       "      <td>[sevenyearold, grandson, sleep, hall, wake, lo...</td>\n",
       "      <td>sevenyearold grandson sleep hall wake lot morn...</td>\n",
       "      <td>buddhism,religion,god,aging,science,beauty,ins...</td>\n",
       "      <td>science,communication,humanity,culture,history</td>\n",
       "      <td>PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...</td>\n",
       "      <td>PERSON PERSON PERSON PERSON PERSON DATE CARDIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>Richard Browning</td>\n",
       "      <td>How I built a jet suit</td>\n",
       "      <td>We've all dreamed of flying  but for Richard B...</td>\n",
       "      <td>0:07:08</td>\n",
       "      <td>adventure,collaboration,demo,design,engineerin...</td>\n",
       "      <td>0:12\\r\\r\\rMichael Browning: engineer,\\rinnovat...</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>[michael, brown, engineer, innovator, inventor...</td>\n",
       "      <td>michael brown engineer innovator inventor insp...</td>\n",
       "      <td>adventure,collaboration,demo,design,entreprene...</td>\n",
       "      <td>design,future,culture,technology</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...</td>\n",
       "      <td>PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2313 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       speaker  \\\n",
       "0                      Al Gore   \n",
       "1                    Amy Smith   \n",
       "2                 Ashraf Ghani   \n",
       "3                   Burt Rutan   \n",
       "4                 Chris Bangle   \n",
       "...                        ...   \n",
       "2308           Justin Davidson   \n",
       "2309  Mehdi Ordikhani-Seyedlar   \n",
       "2310               Tim Ferriss   \n",
       "2311               Anne Lamott   \n",
       "2312          Richard Browning   \n",
       "\n",
       "                                               headline  \\\n",
       "0                           Averting the climate crisis   \n",
       "1                         Simple designs to save a life   \n",
       "2                         How to rebuild a broken state   \n",
       "3                  The real future of space exploration   \n",
       "4                              Great cars are great art   \n",
       "...                                                 ...   \n",
       "2308  Why glass towers are bad for city life -- and ...   \n",
       "2309  What happens in your brain when you pay attent...   \n",
       "2310  Why you should define your fears instead of yo...   \n",
       "2311          12 truths I learned from life and writing   \n",
       "2312                             How I built a jet suit   \n",
       "\n",
       "                                            description duration  \\\n",
       "0     With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1     Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2     Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3     In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4     American designer Chris Bangle explains his ph...  0:20:04   \n",
       "...                                                 ...      ...   \n",
       "2308  There's a creepy transformation taking over ou...  0:12:39   \n",
       "2309  Attention isn't just about what we focus on  i...  0:06:32   \n",
       "2310  The hard choices  what we most fear doing, ask...  0:13:21   \n",
       "2311  A few days before she turned 61, writer Anne L...  0:15:55   \n",
       "2312  We've all dreamed of flying  but for Richard B...  0:07:08   \n",
       "\n",
       "                                                   tags  \\\n",
       "0     cars,alternative energy,culture,politics,scien...   \n",
       "1     MacArthur grant,simplicity,industrial design,a...   \n",
       "2     corruption,poverty,economics,investment,milita...   \n",
       "3     aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4     cars,industrial design,transportation,inventio...   \n",
       "...                                                 ...   \n",
       "2308  architecture,beauty,cities,community,journalis...   \n",
       "2309  AI,algorithm,brain,cognitive science,machine l...   \n",
       "2310  choice,fear,goal-setting,humanity,life,persona...   \n",
       "2311  Christianity,God,aging,art,beauty,birds,books,...   \n",
       "2312  adventure,collaboration,demo,design,engineerin...   \n",
       "\n",
       "                                             transcript      WC  \\\n",
       "0     0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1     0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2     0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3     0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4     0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "...                                                 ...     ...   \n",
       "2308  0:11\\r\\r\\rImagine that when you walked\\rin her...  1693.0   \n",
       "2309  0:11\\r\\r\\rPaying close attention to something:...   846.0   \n",
       "2310  0:11\\r\\r\\rSo, this happy pic of me\\rwas taken ...  2264.0   \n",
       "2311  0:12\\r\\r\\rMy seven-year-old grandson\\rsleeps j...  2255.0   \n",
       "2312  0:12\\r\\r\\rMichael Browning: engineer,\\rinnovat...  1237.0   \n",
       "\n",
       "                                       clean_transcript  \\\n",
       "0     [thank, chris, truly, great, honor, opportunit...   \n",
       "1     [term, invention, like, tell, tale, favorite, ...   \n",
       "2     [public, dewey, long, ago, observe, constitute...   \n",
       "3     [want, start, say, houston, problem, enter, se...   \n",
       "4     [want, talk, background, idea, car, art, actua...   \n",
       "...                                                 ...   \n",
       "2308  [imagine, walk, even, discover, everybody, roo...   \n",
       "2309  [pay, close, attention, easy, attention, pull,...   \n",
       "2310  [happy, pic, take, senior, college, right, dan...   \n",
       "2311  [sevenyearold, grandson, sleep, hall, wake, lo...   \n",
       "2312  [michael, brown, engineer, innovator, inventor...   \n",
       "\n",
       "                                clean_transcript_string  \\\n",
       "0     thank chris truly great honor opportunity come...   \n",
       "1     term invention like tell tale favorite project...   \n",
       "2     public dewey long ago observe constitute discu...   \n",
       "3     want start say houston problem enter second ge...   \n",
       "4     want talk background idea car art actually mea...   \n",
       "...                                                 ...   \n",
       "2308  imagine walk even discover everybody room look...   \n",
       "2309  pay close attention easy attention pull differ...   \n",
       "2310  happy pic take senior college right dance prac...   \n",
       "2311  sevenyearold grandson sleep hall wake lot morn...   \n",
       "2312  michael brown engineer innovator inventor insp...   \n",
       "\n",
       "                                               sim_tags  \\\n",
       "0     cars,solar system,energy,culture,politics,scie...   \n",
       "1     macarthur grant,simplicity,design,solar system...   \n",
       "2     corruption,inequality,science,investment,war,c...   \n",
       "3     flight,design,nasa,science,invention,entrepren...   \n",
       "4     cars,design,transportation,invention,technolog...   \n",
       "...                                                 ...   \n",
       "2308  architecture,beauty,cities,community,journalis...   \n",
       "2309  ai,algorithm,brain,science,machine learning,te...   \n",
       "2310  choice,fear,goal-setting,humanity,life,culture...   \n",
       "2311  buddhism,religion,god,aging,science,beauty,ins...   \n",
       "2312  adventure,collaboration,demo,design,entreprene...   \n",
       "\n",
       "                                          squash15_tags  \\\n",
       "0     culture,politics,science,global issues,technology   \n",
       "1                                  design,global issues   \n",
       "2       science,culture,politics,global issues,business   \n",
       "3                               design,science,business   \n",
       "4                    design,technology,business,science   \n",
       "...                                                 ...   \n",
       "2308                            science,history,culture   \n",
       "2309                                 science,technology   \n",
       "2310                                   humanity,culture   \n",
       "2311     science,communication,humanity,culture,history   \n",
       "2312                   design,future,culture,technology   \n",
       "\n",
       "                                           pos_sequence  \\\n",
       "0     VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...   \n",
       "1     NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...   \n",
       "2     ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...   \n",
       "3     VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...   \n",
       "4     VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...   \n",
       "...                                                 ...   \n",
       "2308  VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...   \n",
       "2309  VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...   \n",
       "2310  ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...   \n",
       "2311  PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...   \n",
       "2312  PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...   \n",
       "\n",
       "                                           ner_sequence  \n",
       "0     PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...  \n",
       "1     GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...  \n",
       "2     DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...  \n",
       "3     GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...  \n",
       "4     PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...  \n",
       "...                                                 ...  \n",
       "2308  ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...  \n",
       "2309                   ORDINAL PERSON PRODUCT DATE DATE  \n",
       "2310  DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...  \n",
       "2311  PERSON PERSON PERSON PERSON PERSON DATE CARDIN...  \n",
       "2312  PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...  \n",
       "\n",
       "[2313 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../../data/processed/final_squash15_with_pos_ner.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction via Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "y = []\n",
    "for index, row in df.iterrows():\n",
    "    y.append(set(row['squash_tags'].split(',')))\n",
    "    \n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(encoded_y[0])\n",
    "print(len(encoded_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['speaker', 'duration', 'tags', 'transcript', 'WC', 'clean_transcript', 'squash_tags'],1), \n",
    "                                                    encoded_y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(X_train)  # At this point the headline and description have not been cleaned so we won't use it\n",
    "# valid headers are clean_transcript_string, pos_sequence, ner_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 tfidf to get the relative frequencies of NER and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_pos = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_pos.fit(df['pos_sequence'])\n",
    "tfidf_vect_ner = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect_ner.fit(df['ner_sequence'])\n",
    "\n",
    "xtrain_tfidf_pos =  tfidf_vect_pos.transform(X_train['pos_sequence'])\n",
    "xtest_tfidf_pos =  tfidf_vect_pos.transform(X_test['pos_sequence'])\n",
    "\n",
    "xtrain_tfidf_ner =  tfidf_vect_ner.transform(X_train['ner_sequence'])\n",
    "xtest_tfidf_ner =  tfidf_vect_ner.transform(X_test['ner_sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Use word embeddings for the main transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-12948704921d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_transcript_string'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_transcript_string'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_transcript_string'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train['clean_transcript_string'].tolist())\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train['clean_transcript_string'].tolist())\n",
    "X_test = tokenizer.texts_to_sequences(X_test['clean_transcript_string'].tolist())\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 2500 # since the average length is about there\n",
    "\n",
    "X_train = pad_sequences(X_train['clean_transcript_string'].tolist(), padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test['clean_transcript_string'].tolist(), padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t0.22007032958907916\n",
      "  (0, 13)\t0.2893461276667263\n",
      "  (0, 11)\t0.25584204855572545\n",
      "  (0, 9)\t0.2546057714793977\n",
      "  (0, 8)\t0.10397572751994356\n",
      "  (0, 4)\t0.6496448885437085\n",
      "  (0, 1)\t0.5299697390018687\n",
      "  (0, 0)\t0.1544385262071525\n",
      "  (1, 16)\t0.11370520226562829\n",
      "  (1, 14)\t0.38855464466318806\n",
      "  (1, 13)\t0.5108675123545295\n",
      "  (1, 11)\t0.37642745257879706\n",
      "  (1, 9)\t0.26971810852765726\n",
      "  (1, 7)\t0.47358478327583303\n",
      "  (1, 4)\t0.3277167145309368\n",
      "  (1, 1)\t0.1439555212991835\n",
      "  (1, 0)\t0.09089186289488255\n",
      "  (2, 16)\t0.17377210440997654\n",
      "  (2, 14)\t0.14845397777768018\n",
      "  (2, 13)\t0.2788367290642293\n",
      "  (2, 11)\t0.23011292109394593\n",
      "  (2, 9)\t0.20610087481677836\n",
      "  (2, 7)\t0.3618824062866172\n",
      "  (2, 4)\t0.6886539072049239\n",
      "  (2, 2)\t0.24592570035796474\n",
      "  :\t:\n",
      "  (1904, 1)\t0.1117323383978754\n",
      "  (1905, 16)\t0.031028105374608338\n",
      "  (1905, 13)\t0.1394065591543476\n",
      "  (1905, 11)\t0.2259846247130782\n",
      "  (1905, 10)\t0.06187554953754911\n",
      "  (1905, 9)\t0.24533740837660853\n",
      "  (1905, 7)\t0.5169310908032412\n",
      "  (1905, 4)\t0.6930669947705734\n",
      "  (1905, 2)\t0.04391158506677554\n",
      "  (1905, 1)\t0.2553386781636001\n",
      "  (1905, 0)\t0.22322479702447567\n",
      "  (1906, 13)\t0.3216713922135117\n",
      "  (1906, 12)\t0.16627276907995187\n",
      "  (1906, 11)\t0.3318283424453252\n",
      "  (1906, 10)\t0.16656934938020532\n",
      "  (1906, 8)\t0.13485700850294235\n",
      "  (1906, 7)\t0.1739476984710315\n",
      "  (1906, 4)\t0.6620369496852755\n",
      "  (1906, 1)\t0.4758737874786256\n",
      "  (1906, 0)\t0.13353833077952418\n",
      "  (1907, 13)\t0.9865299048538427\n",
      "  (1907, 11)\t0.06784534176275323\n",
      "  (1907, 9)\t0.08102100031626386\n",
      "  (1907, 7)\t0.10669559716909352\n",
      "  (1907, 1)\t0.06486450094347165\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('./glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
