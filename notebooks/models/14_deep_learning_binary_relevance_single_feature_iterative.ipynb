{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links\n",
    "\n",
    "1. For the architecture https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada\n",
    "2. https://androidkt.com/multi-label-text-classification-in-tensorflow-keras/\n",
    "3. https://keras.io/preprocessing/sequence/\n",
    "4. https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/ ( Not really)\n",
    "5. For deep learning using word embeddings https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/processed/\"\n",
    "INPUT_FILE_NAME = 'final_squash15_with_pos_ner_tm.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>WC</th>\n",
       "      <th>clean_transcript</th>\n",
       "      <th>clean_transcript_string</th>\n",
       "      <th>sim_tags</th>\n",
       "      <th>squash15_tags</th>\n",
       "      <th>pos_sequence</th>\n",
       "      <th>ner_sequence</th>\n",
       "      <th>tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>0:16:17</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "      <td>2281.0</td>\n",
       "      <td>b'[\"thank\", \"chris\", \"truly\", \"great\", \"honor\"...</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>cars,solar system,energy,culture,politics,scie...</td>\n",
       "      <td>culture,politics,science,global issues,technology</td>\n",
       "      <td>VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...</td>\n",
       "      <td>PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...</td>\n",
       "      <td>[0.04325945698517057, 0.0, 0.00142482934694180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>0:15:06</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>b'[\"term\", \"invention\", \"like\", \"tell\", \"tale\"...</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>macarthur grant,simplicity,design,solar system...</td>\n",
       "      <td>design,global issues</td>\n",
       "      <td>NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...</td>\n",
       "      <td>GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...</td>\n",
       "      <td>[0.013287880838036227, 0.0, 0.0, 0.00511725094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>0:18:45</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>b'[\"public\", \"dewey\", \"long\", \"ago\", \"observe\"...</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>corruption,inequality,science,investment,war,c...</td>\n",
       "      <td>science,culture,politics,global issues,business</td>\n",
       "      <td>ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...</td>\n",
       "      <td>DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...</td>\n",
       "      <td>[0.0, 0.006699599134802422, 0.0, 0.00564851883...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>0:19:37</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>b'[\"want\", \"start\", \"say\", \"houston\", \"problem...</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>flight,design,nasa,science,invention,entrepren...</td>\n",
       "      <td>design,science,business</td>\n",
       "      <td>VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...</td>\n",
       "      <td>[0.040282108339079505, 0.03732895646484358, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>0:20:04</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>b'[\"want\", \"talk\", \"background\", \"idea\", \"car\"...</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>cars,design,transportation,invention,technolog...</td>\n",
       "      <td>design,technology,business,science</td>\n",
       "      <td>VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...</td>\n",
       "      <td>PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...</td>\n",
       "      <td>[0.08049208168957463, 0.0, 0.0, 0.008031187136...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker                              headline  \\\n",
       "0       Al Gore           Averting the climate crisis   \n",
       "1     Amy Smith         Simple designs to save a life   \n",
       "2  Ashraf Ghani         How to rebuild a broken state   \n",
       "3    Burt Rutan  The real future of space exploration   \n",
       "4  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                         description duration  \\\n",
       "0  With the same humor and humanity he exuded in ...  0:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...  0:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...  0:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...  0:19:37   \n",
       "4  American designer Chris Bangle explains his ph...  0:20:04   \n",
       "\n",
       "                                                tags  \\\n",
       "0  cars,alternative energy,culture,politics,scien...   \n",
       "1  MacArthur grant,simplicity,industrial design,a...   \n",
       "2  corruption,poverty,economics,investment,milita...   \n",
       "3  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript      WC  \\\n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  2281.0   \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  2687.0   \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  2506.0   \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  3092.0   \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  3781.0   \n",
       "\n",
       "                                    clean_transcript  \\\n",
       "0  b'[\"thank\", \"chris\", \"truly\", \"great\", \"honor\"...   \n",
       "1  b'[\"term\", \"invention\", \"like\", \"tell\", \"tale\"...   \n",
       "2  b'[\"public\", \"dewey\", \"long\", \"ago\", \"observe\"...   \n",
       "3  b'[\"want\", \"start\", \"say\", \"houston\", \"problem...   \n",
       "4  b'[\"want\", \"talk\", \"background\", \"idea\", \"car\"...   \n",
       "\n",
       "                             clean_transcript_string  \\\n",
       "0  thank chris truly great honor opportunity come...   \n",
       "1  term invention like tell tale favorite project...   \n",
       "2  public dewey long ago observe constitute discu...   \n",
       "3  want start say houston problem enter second ge...   \n",
       "4  want talk background idea car art actually mea...   \n",
       "\n",
       "                                            sim_tags  \\\n",
       "0  cars,solar system,energy,culture,politics,scie...   \n",
       "1  macarthur grant,simplicity,design,solar system...   \n",
       "2  corruption,inequality,science,investment,war,c...   \n",
       "3  flight,design,nasa,science,invention,entrepren...   \n",
       "4  cars,design,transportation,invention,technolog...   \n",
       "\n",
       "                                       squash15_tags  \\\n",
       "0  culture,politics,science,global issues,technology   \n",
       "1                               design,global issues   \n",
       "2    science,culture,politics,global issues,business   \n",
       "3                            design,science,business   \n",
       "4                 design,technology,business,science   \n",
       "\n",
       "                                        pos_sequence  \\\n",
       "0  VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...   \n",
       "1  NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...   \n",
       "2  ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...   \n",
       "3  VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...   \n",
       "4  VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...   \n",
       "\n",
       "                                        ner_sequence  \\\n",
       "0  PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...   \n",
       "1  GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...   \n",
       "2  DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...   \n",
       "3  GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...   \n",
       "4  PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...   \n",
       "\n",
       "                                                  tm  \n",
       "0  [0.04325945698517057, 0.0, 0.00142482934694180...  \n",
       "1  [0.013287880838036227, 0.0, 0.0, 0.00511725094...  \n",
       "2  [0.0, 0.006699599134802422, 0.0, 0.00564851883...  \n",
       "3  [0.040282108339079505, 0.03732895646484358, 0....  \n",
       "4  [0.08049208168957463, 0.0, 0.0, 0.008031187136...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_DIR + INPUT_FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2313 entries, 0 to 2312\n",
      "Data columns (total 14 columns):\n",
      "speaker                    2313 non-null object\n",
      "headline                   2313 non-null object\n",
      "description                2313 non-null object\n",
      "duration                   2313 non-null object\n",
      "tags                       2313 non-null object\n",
      "transcript                 2313 non-null object\n",
      "WC                         2313 non-null float64\n",
      "clean_transcript           2313 non-null object\n",
      "clean_transcript_string    2313 non-null object\n",
      "sim_tags                   2313 non-null object\n",
      "squash15_tags              2313 non-null object\n",
      "pos_sequence               2313 non-null object\n",
      "ner_sequence               2313 non-null object\n",
      "tm                         2313 non-null object\n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 253.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.iloc[:,:14].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    squash15_tags  counts  no_count     ratio  overall_ratio\n",
      "0         science    1467       846  1.734043       0.634241\n",
      "1         culture    1155      1158  0.997409       0.499351\n",
      "2      technology     787      1526  0.515727       0.340251\n",
      "3   global issues     679      1634  0.415545       0.293558\n",
      "4          design     477      1836  0.259804       0.206226\n",
      "5         history     385      1928  0.199689       0.166450\n",
      "6        business     349      1964  0.177699       0.150886\n",
      "7   entertainment     285      2028  0.140533       0.123217\n",
      "8           media     279      2034  0.137168       0.120623\n",
      "9    biomechanics     220      2093  0.105112       0.095115\n",
      "10         future     218      2095  0.104057       0.094250\n",
      "11   biodiversity     218      2095  0.104057       0.094250\n",
      "12       humanity     217      2096  0.103531       0.093818\n",
      "13       politics     199      2114  0.094134       0.086035\n",
      "14  communication     185      2128  0.086936       0.079983\n"
     ]
    }
   ],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    \n",
    "def compute_tag_ratio(target_column, df=df):\n",
    "    tags = df[target_column].str.replace(', ',',').str.lower().str.strip()\n",
    "    split_tags = tags.str.split(',')\n",
    "    tag_counts_per_talk = split_tags.apply(len)\n",
    "\n",
    "    joined_tags = tags.str.cat(sep=',').split(',')\n",
    "    all_tags = pd.Series(joined_tags)\n",
    "\n",
    "    tag_counts = all_tags.value_counts().rename_axis(target_column).reset_index(name='counts')\n",
    "    tag_counts['no_count'] = len(df)-tag_counts['counts']\n",
    "    tag_counts['ratio'] = tag_counts['counts']/tag_counts['no_count']\n",
    "    tag_counts['overall_ratio'] = tag_counts['counts']/(tag_counts['no_count'] + tag_counts['counts'])\n",
    "    return tag_counts\n",
    "\n",
    "#print(compute_tag_ratio('squash3_tags', df))\n",
    "squashed_tag_counts = compute_tag_ratio('squash15_tags', df)\n",
    "print_full_dataframe(squashed_tag_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6342412451361867\n"
     ]
    }
   ],
   "source": [
    "print(squashed_tag_counts['overall_ratio'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction via Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['culture', 'politics', 'science', 'global issues', 'technology', 'design', 'business', 'biomechanics', 'biodiversity', 'media', 'entertainment', 'history', 'future', 'communication', 'humanity']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "joined_tags = df['squash15_tags'].str.cat(sep=',').split(',')\n",
    "all_tags = pd.Series(joined_tags).str.strip().str.lower()\n",
    "all_tags = list(dict.fromkeys(all_tags))\n",
    "try:\n",
    "    all_tags.remove('')\n",
    "except:\n",
    "    pass\n",
    "print(all_tags)\n",
    "print(len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encode(df=df):\n",
    "    complete_transcripts_tags = []\n",
    "    for rows, value in df.iterrows():\n",
    "        one_hot_encoding = [0] * len(all_tags)\n",
    "        headline = [value['headline']]\n",
    "        transcript = [value['clean_transcript_string']]\n",
    "        pos_sequence = [value['pos_sequence']]\n",
    "        ner_sequence = [value['ner_sequence']]\n",
    "        indiv_tags = value['squash15_tags'].split(',')\n",
    "        for tags in indiv_tags:\n",
    "            if tags == '':\n",
    "                continue\n",
    "            index = all_tags.index(tags.lower().lstrip(' '))\n",
    "            one_hot_encoding[index] = 1\n",
    "        indiv_transcript_tags = headline + transcript + pos_sequence + ner_sequence + one_hot_encoding\n",
    "        complete_transcripts_tags.append(indiv_transcript_tags)\n",
    "    return pd.DataFrame(complete_transcripts_tags, columns=['headline', 'transcript', 'pos_sequence', 'ner_sequence'] + all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>transcript</th>\n",
       "      <th>pos_sequence</th>\n",
       "      <th>ner_sequence</th>\n",
       "      <th>culture</th>\n",
       "      <th>politics</th>\n",
       "      <th>science</th>\n",
       "      <th>global issues</th>\n",
       "      <th>technology</th>\n",
       "      <th>design</th>\n",
       "      <th>business</th>\n",
       "      <th>biomechanics</th>\n",
       "      <th>biodiversity</th>\n",
       "      <th>media</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>history</th>\n",
       "      <th>future</th>\n",
       "      <th>communication</th>\n",
       "      <th>humanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>thank chris truly great honor opportunity come...</td>\n",
       "      <td>VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...</td>\n",
       "      <td>PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>term invention like tell tale favorite project...</td>\n",
       "      <td>NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...</td>\n",
       "      <td>GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>public dewey long ago observe constitute discu...</td>\n",
       "      <td>ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...</td>\n",
       "      <td>DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>want start say houston problem enter second ge...</td>\n",
       "      <td>VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...</td>\n",
       "      <td>GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>want talk background idea car art actually mea...</td>\n",
       "      <td>VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...</td>\n",
       "      <td>PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>Why glass towers are bad for city life -- and ...</td>\n",
       "      <td>imagine walk even discover everybody room look...</td>\n",
       "      <td>VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...</td>\n",
       "      <td>ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>What happens in your brain when you pay attent...</td>\n",
       "      <td>pay close attention easy attention pull differ...</td>\n",
       "      <td>VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...</td>\n",
       "      <td>ORDINAL PERSON PRODUCT DATE DATE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>Why you should define your fears instead of yo...</td>\n",
       "      <td>happy pic take senior college right dance prac...</td>\n",
       "      <td>ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...</td>\n",
       "      <td>DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>12 truths I learned from life and writing</td>\n",
       "      <td>sevenyearold grandson sleep hall wake lot morn...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...</td>\n",
       "      <td>PERSON PERSON PERSON PERSON PERSON DATE CARDIN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>How I built a jet suit</td>\n",
       "      <td>michael brown engineer innovator inventor insp...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...</td>\n",
       "      <td>PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2313 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headline  \\\n",
       "0                           Averting the climate crisis   \n",
       "1                         Simple designs to save a life   \n",
       "2                         How to rebuild a broken state   \n",
       "3                  The real future of space exploration   \n",
       "4                              Great cars are great art   \n",
       "...                                                 ...   \n",
       "2308  Why glass towers are bad for city life -- and ...   \n",
       "2309  What happens in your brain when you pay attent...   \n",
       "2310  Why you should define your fears instead of yo...   \n",
       "2311          12 truths I learned from life and writing   \n",
       "2312                             How I built a jet suit   \n",
       "\n",
       "                                             transcript  \\\n",
       "0     thank chris truly great honor opportunity come...   \n",
       "1     term invention like tell tale favorite project...   \n",
       "2     public dewey long ago observe constitute discu...   \n",
       "3     want start say houston problem enter second ge...   \n",
       "4     want talk background idea car art actually mea...   \n",
       "...                                                 ...   \n",
       "2308  imagine walk even discover everybody room look...   \n",
       "2309  pay close attention easy attention pull differ...   \n",
       "2310  happy pic take senior college right dance prac...   \n",
       "2311  sevenyearold grandson sleep hall wake lot morn...   \n",
       "2312  michael brown engineer innovator inventor insp...   \n",
       "\n",
       "                                           pos_sequence  \\\n",
       "0     VERB PROPN ADV ADJ NOUN NOUN VERB NOUN ADV ADV...   \n",
       "1     NOUN NOUN SCONJ VERB PROPN ADJ NOUN VERB NOUN ...   \n",
       "2     ADJ PROPN ADV ADV VERB ADJ NOUN NOUN PROPN PRO...   \n",
       "3     VERB NOUN VERB PROPN NOUN VERB ADJ NOUN NOUN N...   \n",
       "4     VERB NOUN NOUN NOUN NOUN NOUN ADV ADJ NOUN NOU...   \n",
       "...                                                 ...   \n",
       "2308  VERB NOUN ADV VERB PRON NOUN VERB ADV NOUN NOU...   \n",
       "2309  VERB ADJ NOUN ADJ NOUN VERB ADJ NOUN NOUN NOUN...   \n",
       "2310  ADJ PROPN VERB ADJ NOUN ADJ NOUN NOUN ADJ VERB...   \n",
       "2311  PROPN PROPN PROPN PROPN VERB NOUN NOUN VERB VE...   \n",
       "2312  PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN...   \n",
       "\n",
       "                                           ner_sequence  culture  politics  \\\n",
       "0     PERSON ORG ORG GPE LOC ORG PRODUCT GPE GPE PER...        1         1   \n",
       "1     GPE DATE CARDINAL DATE ORG PERSON LOC ORG GPE ...        0         0   \n",
       "2     DATE NORP ORDINAL DATE MONEY DATE DATE DATE EV...        1         1   \n",
       "3     GPE ORDINAL ORG PERSON DATE DATE DATE TIME PER...        0         0   \n",
       "4     PERSON PRODUCT ORG ORG PERSON PERSON PERSON OR...        0         0   \n",
       "...                                                 ...      ...       ...   \n",
       "2308  ORG GPE ORG GPE GPE GPE GPE GPE GPE GPE GPE PE...        1         0   \n",
       "2309                   ORDINAL PERSON PRODUCT DATE DATE        0         0   \n",
       "2310  DATE PERSON ORG PERSON PERSON GPE PERSON GPE O...        1         0   \n",
       "2311  PERSON PERSON PERSON PERSON PERSON DATE CARDIN...        1         0   \n",
       "2312  PERSON DATE DATE PERSON GPE CARDINAL GPE GPE O...        1         0   \n",
       "\n",
       "      science  global issues  technology  design  business  biomechanics  \\\n",
       "0           1              1           1       0         0             0   \n",
       "1           0              1           0       1         0             0   \n",
       "2           1              1           0       0         1             0   \n",
       "3           1              0           0       1         1             0   \n",
       "4           1              0           1       1         1             0   \n",
       "...       ...            ...         ...     ...       ...           ...   \n",
       "2308        1              0           0       0         0             0   \n",
       "2309        1              0           1       0         0             0   \n",
       "2310        0              0           0       0         0             0   \n",
       "2311        1              0           0       0         0             0   \n",
       "2312        0              0           1       1         0             0   \n",
       "\n",
       "      biodiversity  media  entertainment  history  future  communication  \\\n",
       "0                0      0              0        0       0              0   \n",
       "1                0      0              0        0       0              0   \n",
       "2                0      0              0        0       0              0   \n",
       "3                0      0              0        0       0              0   \n",
       "4                0      0              0        0       0              0   \n",
       "...            ...    ...            ...      ...     ...            ...   \n",
       "2308             0      0              0        1       0              0   \n",
       "2309             0      0              0        0       0              0   \n",
       "2310             0      0              0        0       0              0   \n",
       "2311             0      0              0        1       0              1   \n",
       "2312             0      0              0        0       1              0   \n",
       "\n",
       "      humanity  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "2308         0  \n",
       "2309         0  \n",
       "2310         1  \n",
       "2311         1  \n",
       "2312         0  \n",
       "\n",
       "[2313 rows x 19 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_one_hot_encode()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_column(target_tag, df=df):\n",
    "    '''Returns a dataframe of a single tag\n",
    "    headline | transcript | pos_sequence | ner_sequence | <tag>\n",
    "    '''\n",
    "    return df[['headline', 'transcript','pos_sequence', 'ner_sequence', target_tag]]\n",
    "# single_class = get_target_column('culture', df) # Retrieve a single data frame with teh foll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = list(single_class['transcript']) # List of sentences\n",
    "df_y = df[all_tags] # Dataframe of all the y columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, train_y, valid_y = train_test_split(df_x, df_y, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Use word embeddings for the main transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 800 # too many and the model cant tell the difference \n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('./glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2844 1728  574 1072  826 4761 4500   61 1728 1750  332   98  151  332\n",
      " 4672   42  332  208   16   42 1750   16   61  348 1270  620 4117  157\n",
      " 4117   56 1943 1187  826 1773  630   35   56 1943   56  826   56  630\n",
      "   56 4498   56  163  780  826 1773  630   75  926  525   17   13   75\n",
      "  168  551 1225   65   27   56 1187  168  551  462  525  309 1072 2379\n",
      "  376   20   85  714   28  376    3   20   56  826   56 2303   56  977\n",
      "   44   56  464  404    1   28   27  561  322 4990  503   56 1091 1091\n",
      "  270  102  561   32   12    1 1168 3210 1314 3210  172  518  430    1\n",
      " 1477 3210  567   56  264   56 1364   47 4060 1051  172   15   10 1773\n",
      "  264   82 1773  264  270  957   16 2344    1   37  714 1187 4116  567\n",
      "   46    1 1477 2344    2   20  977 1187  630 1538 1281  157  630   20\n",
      " 1684  105   30   12   20   57   10  630  249   57   18 1477 2964  561\n",
      "  561 2227  290 3604 1052 1442  290    1  134   26    7  281   26  300\n",
      "   10   14 1364    1 1477 3183   12 2343   14  410  630    7  561 2227\n",
      "  561 1300   24 2227    6   27   41  473  561  893  721  473  933  473\n",
      "  561    3 4501   20  561   24 2227   13 3183  264   25   77    2  466\n",
      " 1145   24 4002 1536 3254 2041 3704 4060  332   88   61   16 3704 1897\n",
      "  561 3254   88  559   26 2041  444  100   72    3   20 1587 1231  127\n",
      " 2941    1  159   20   58 3183   50 4060  264 2303  303 4331   17 4061\n",
      "   42   57  264   20 1005  561   15  957 1005 1806 1849 2429 3384 4991\n",
      "  251    8   80   15  256  195  221 3436 1187 4991 2160    9 1187 4991\n",
      " 3608  281 3436    6   20   85 1442 4762   71  327  105   40   20 1364\n",
      " 3436   11  769 3498   81    8   40   20  105 1364   20   57   18   40\n",
      "  586 3608  586 3608 4763 2874  803   10 3608   20   58  225 3436  998\n",
      " 1839  561  587 4991   88 1573   29  264 2024 3210 2086 4060  332 2573\n",
      "  689 1785  153 2677   26   72   17   52  397 2065  977    1  423  179\n",
      "  153   91   17  138 4416 3143 2065    4    5  269 1121 2595 2024  163\n",
      " 2990    5 1516   89  163   20 1961   84  684 1188  611   20  253   92\n",
      "  611   17  601  155 1051   11 1284  106  423  986 2678  322 1204  264\n",
      "  155 1051   10    5   67    3   31   14  238  482  854  382  238  482\n",
      "  150  215   16  386   67   19   18   69   26    4  563    4 1378   31\n",
      "   13   38 2042 3142   44   99  563   31 4992   23   13    6  392   23\n",
      "  253   92   20   48  476 1930   68 2194 1495 4584  532 3098   23  408\n",
      "    7  532 2161  136 1411  311 4585  587 4332  316 4332  316  316  167\n",
      "   92  316  957 1411  532   27 1109  512  641 2304  641 2304 2024   72\n",
      "  657  747   10  747 2286    4   49 1422  364  243  998  506  747 2500\n",
      "  657 4332  316   10   10  998  506 1429  561    1   92   20 1306 4993\n",
      "   33  220 4062 1411  148   57  158   20   57 3943  148   69   57  214\n",
      " 1740  153   40 1471  148   27  352  998 2086 3943   63  152  998 1412\n",
      "  554 1698  506   10 4332  316 1109  561 1751  157   72  163  163 3212\n",
      "  223   35  641 2304  846   29 4060 1051  166  386  163  706  326 1180\n",
      "  128  172   25 4417    2 4994  453  100  163   33 1524  289 1141  821\n",
      " 1685    2   60    2  289  660 1553  493   60 1127  386  163 4764 1786\n",
      "  386  163 3383 4764   62 2124  386  163  264 3437  386  163  418  100\n",
      "  163   10 1524   13   39  713  151  163  151  163  784  769  100  163\n",
      "   10  391    4   47  386  163  100  163 3212  223   10  391  326 3881\n",
      "   27  473  386  163   11  561 1314 1051 1051  583 2124  386  163 1348\n",
      " 2326  392    1 1920   29  473 1105   31 3184   79  879  261   21   12\n",
      " 1729   33  865  216 2042   35 1285  155  325 1261  571  281 1599  749\n",
      "  472   26  322  774  122   14   12   49  571  194   28   24    3   11\n",
      "  266  102    7   18    5   20  804  882  563   71   71 1370   35   71\n",
      " 2622  804  882    8 2622  196   14 3759 1294   86  845  413  272 3307\n",
      "  337    5   23 1156  575   92  894    8 2622  266    8 3183  163    3\n",
      "   68 1429  115 2273  163  794 3210 3880    2  192  775    3   27 1683\n",
      " 2273  561    3   11  348  616  841  604 1641  125 4995   47  841    3\n",
      "    3   27 4118    7  561    7  272  692  266    3  895   71  444    3\n",
      " 1668  561 4060 2086    3  793   71    3 1029  775    5   11  927  744\n",
      "   18   45]\n",
      "(1734, 800)\n"
     ]
    }
   ],
   "source": [
    "#print(X_train[1])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1573    1\n",
      "236     1\n",
      "2079    1\n",
      "1830    0\n",
      "584     1\n",
      "       ..\n",
      "769     1\n",
      "1372    1\n",
      "2119    1\n",
      "599     1\n",
      "1459    1\n",
      "Name: culture, Length: 1734, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_y['culture'])\n",
    "print(train_y['culture'].value_counts())\n",
    "print(train_y['culture'].value_counts()[0])\n",
    "print(train_y['culture'].value_counts()[1])\n",
    "print(type(train_y['culture']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 10.187096774193549}\n"
     ]
    }
   ],
   "source": [
    "#from keras import backend as K\n",
    "def compute_class_weight(tag):\n",
    "    if train_y[tag].value_counts()[0] >train_y[tag].value_counts()[1]:\n",
    "        class_weight = {0: 1,\n",
    "                        1: 1/(train_y[tag].value_counts()[1]/train_y[tag].value_counts()[0]),\n",
    "                       }\n",
    "    else:\n",
    "        class_weight = {0: 1/(train_y[tag].value_counts()[0]/train_y[tag].value_counts()[1]),\n",
    "                    1: 1,\n",
    "                   }\n",
    "    return class_weight\n",
    "print(compute_class_weight('humanity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "dense_layer_1 = Dense(1, activation='sigmoid')(LSTM_Layer_1)\n",
    "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "#history = model.fit(X_train, train_y, batch_size=128, epochs=4, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(threshold, predictions=predictions):\n",
    "    ''' Flush predictions to either 1 or 0\n",
    "    Returns\n",
    "    [[1], [0], ..., [1]]\n",
    "    '''\n",
    "    return [[1 if j > threshold else 0 for j in i.tolist()] for i in predictions]\n",
    "\n",
    "def get_tag_flat(threshold, predictions=predictions):\n",
    "    ''' Flush predictions to either 1 or 0\n",
    "    Returns\n",
    "    [[1], [0], ..., [1]]\n",
    "    '''\n",
    "    return [1 if j > threshold else 0 for i in predictions for j in i]\n",
    "#predictions_flushed = get_tag(0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_tn_fp_fn(y_test, y_pred, classes):\n",
    "    '''\n",
    "    parameters\n",
    "    y_test (list): [1,...,0]\n",
    "    y_pred (list): [[1], [0], ..., [1]]\n",
    "    classes: [tag1, tag2, ..., tagn]\n",
    "    \n",
    "    Return:\n",
    "    pre_score = {\n",
    "        'tag_1': {\n",
    "            'index': ,\n",
    "            'tp': ,\n",
    "            'tn': ,\n",
    "            'fp': ,\n",
    "            'fn': \n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # Create dictionary of tags \n",
    "    pre_score = {}\n",
    "    for index_tag, tag in enumerate(classes):\n",
    "        pre_score[tag] = {\n",
    "            'index':index_tag,\n",
    "            'tp': 0,\n",
    "            'tn': 0,\n",
    "            'fp': 0,\n",
    "            'fn': 0\n",
    "        }\n",
    "    for transcript_index, transcript_value in enumerate(y_test):\n",
    "        if transcript_value == y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['tp'] += 1\n",
    "        elif transcript_value == y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['tn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 1:\n",
    "            pre_score[classes[0]]['fn'] += 1\n",
    "        elif transcript_value != y_pred[transcript_index][0] and transcript_value == 0:\n",
    "            pre_score[classes[0]]['fp'] += 1\n",
    "    return pre_score\n",
    "#scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, ['culture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall_f1(preprocessed_scores):\n",
    "    '''\n",
    "    parameters\n",
    "    preprocessed_scores = {\n",
    "        'tag_1': {\n",
    "            'index': ,\n",
    "            'tp': ,\n",
    "            'tn': ,\n",
    "            'fp': ,\n",
    "            'fn': \n",
    "        }\n",
    "    }\n",
    "    return\n",
    "    preprocessed_scores = {\n",
    "        'tag_1': {\n",
    "            'index': ,\n",
    "            'tp': ,\n",
    "            'tn': ,\n",
    "            'fp': ,\n",
    "            'fn': ,\n",
    "            'precision': ,\n",
    "            'recall': ,\n",
    "            'f1': ,\n",
    "            'accuracy': ,\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    for key, value in preprocessed_scores.items():\n",
    "        try:\n",
    "            precision = value['tp']/(value['tp']+value['fp'])\n",
    "        except:\n",
    "            #print('precision issue: {}'.format(key))\n",
    "            precision = 0.0\n",
    "        try:\n",
    "            recall = value['tp']/(value['tp']+value['fn'])\n",
    "        except:\n",
    "            #print('recall issue: {}'.format(key))\n",
    "            recall = 0.0\n",
    "        try:\n",
    "            f1 = (2 * precision * recall)/(precision + recall)\n",
    "        except:\n",
    "            #print('f1 issue: {}'.format(key))\n",
    "            f1=0.0\n",
    "        try:\n",
    "            accuracy = (value['tp'] + value['tn'])/(value['tp']+value['fn'] + value['fp'] + value['tn'])\n",
    "        except:\n",
    "            accuracy = 0.0\n",
    "        preprocessed_scores[key]['precision'] = round(precision,4)\n",
    "        preprocessed_scores[key]['recall'] = round(recall,4)\n",
    "        preprocessed_scores[key]['f1'] = round(f1,4)\n",
    "        preprocessed_scores[key]['accuracy'] = round(accuracy,4)\n",
    "    return preprocessed_scores\n",
    "# final_scores = compute_precision_recall_f1(scores_preprocess)\n",
    "# print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_dataframe(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scores_df(tag_classes, final_scores):\n",
    "    '''\n",
    "    Given the list of classes and the dicitionary of all the classes and their scores, we write to a dataframe\n",
    "    '''\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    for index, value in enumerate(tag_classes):\n",
    "        precision.append(final_scores[value]['precision'])\n",
    "        recall.append(final_scores[value]['recall'])\n",
    "        f1.append(final_scores[value]['f1'])\n",
    "        accuracy.append(final_scores[value]['accuracy'])\n",
    "    df_result = pd.DataFrame(list(zip(tag_classes, precision, recall, f1, accuracy)), \n",
    "               columns =['class', 'precision', 'recall', 'f1', 'accuracy']) \n",
    "    return df_result\n",
    "# df_results = format_scores_df(['culture'], final_scores)\n",
    "# print_full_dataframe(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(tag,valid_y, predictions):\n",
    "    '''\n",
    "    tag (string): Specific tag\n",
    "    valid_y (List of tags): [0,1,1, ... ,1]\n",
    "    predictions (list of lists)\n",
    "    '''\n",
    "    # We want to find the threshold that gives the highest recall and accuracy\n",
    "    highest_f1 = 0\n",
    "    f1_i = []\n",
    "    highest_accuracy_f1 = 0\n",
    "    accuracy_f1_i = []\n",
    "    highest_accuracy = 0\n",
    "    accuracy_i = []\n",
    "    f1_metrics = [0, 0, 0, 0] # tp, tn, fp, fn\n",
    "    accuracy_metrics = [0, 0, 0, 0]\n",
    "    for i in range(0, 100):\n",
    "        i = i/100\n",
    "        predictions_flushed = get_tag(i,predictions)\n",
    "        scores_preprocess = compute_tp_tn_fp_fn(valid_y, predictions_flushed, [tag])\n",
    "        final_scores = compute_precision_recall_f1(scores_preprocess)\n",
    "    #     print(final_scores)\n",
    "        df_results = format_scores_df([tag], final_scores)\n",
    "        #print(df_results)\n",
    "        f1 = final_scores[tag]['f1']\n",
    "        accuracy = df_results.accuracy[0]\n",
    "\n",
    "        if f1 > highest_f1:\n",
    "            highest_f1 = f1\n",
    "            f1_i = [i]\n",
    "            if accuracy > highest_accuracy_f1:\n",
    "                highest_accuracy_f1 = accuracy\n",
    "                accuracy_f1_i = [i]\n",
    "                f1_metrics[0] = scores_preprocess[tag]['tp']\n",
    "                f1_metrics[1] = scores_preprocess[tag]['tn']\n",
    "                f1_metrics[2] = scores_preprocess[tag]['fp']\n",
    "                f1_metrics[3] = scores_preprocess[tag]['fn']\n",
    "            elif accuracy == highest_accuracy_f1:\n",
    "                accuracy_f1_i.append(i)\n",
    "        elif f1 == highest_f1:\n",
    "            f1_i.append(i)\n",
    "            if accuracy > highest_accuracy_f1:\n",
    "                highest_accuracy_f1 = accuracy\n",
    "                accuracy_f1_i = [i]\n",
    "                \n",
    "            elif accuracy == highest_accuracy_f1:\n",
    "                accuracy_f1_i.append(i)\n",
    "\n",
    "        if accuracy > highest_accuracy:\n",
    "            highest_accuracy = accuracy\n",
    "            accuracy_i = [i]\n",
    "            accuracy_metrics[0] = scores_preprocess[tag]['tp']\n",
    "            accuracy_metrics[1] = scores_preprocess[tag]['tn']\n",
    "            accuracy_metrics[2] = scores_preprocess[tag]['fp']\n",
    "            accuracy_metrics[3] = scores_preprocess[tag]['fn']\n",
    "        elif accuracy == highest_accuracy:\n",
    "            accuracy_i.append(i)\n",
    "\n",
    "    #     print('\\n')\n",
    "\n",
    "#     print(highest_f1,f1_i)\n",
    "#     print(highest_accuracy_f1,accuracy_f1_i)\n",
    "#     print(highest_accuracy,accuracy_i)\n",
    "    return highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i, f1_metrics, accuracy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culture\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 151s 109ms/step - loss: 0.6976 - acc: 0.5242 - val_loss: 0.7123 - val_acc: 0.4524\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 149s 107ms/step - loss: 0.6896 - acc: 0.5494 - val_loss: 0.7275 - val_acc: 0.4640\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 152s 110ms/step - loss: 0.6836 - acc: 0.5537 - val_loss: 0.7201 - val_acc: 0.4726\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 155s 112ms/step - loss: 0.6647 - acc: 0.5732 - val_loss: 0.7232 - val_acc: 0.4899\n",
      "0.6826 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17] 0.5181 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17] 0.5389 [0.52, 0.54] [300, 0, 279, 0] [98, 214, 65, 202]\n",
      "politics\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 158s 114ms/step - loss: 1.2551 - acc: 0.5703 - val_loss: 1.4889 - val_acc: 0.5850\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 257s 185ms/step - loss: 1.1818 - acc: 0.7650 - val_loss: 1.3627 - val_acc: 0.6974\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 340s 245ms/step - loss: 1.1595 - acc: 0.6777 - val_loss: 1.6288 - val_acc: 0.6715\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 434s 313ms/step - loss: 1.1383 - acc: 0.6799 - val_loss: 1.4881 - val_acc: 0.8991\n",
      "0.2609 [0.4] 0.8826 [0.4] 0.9188 [0.51, 0.54, 0.55, 0.7] [12, 499, 27, 41] [8, 524, 2, 45]\n",
      "science\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 335s 241ms/step - loss: 0.9167 - acc: 0.4924 - val_loss: 0.8720 - val_acc: 0.5360\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 292s 211ms/step - loss: 0.8827 - acc: 0.4722 - val_loss: 0.8740 - val_acc: 0.5274\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 343s 247ms/step - loss: 0.8751 - acc: 0.4996 - val_loss: 0.8723 - val_acc: 0.5331\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 318s 229ms/step - loss: 0.8703 - acc: 0.5191 - val_loss: 0.8718 - val_acc: 0.5072\n",
      "0.7767 [0.4] 0.6356 [0.4] 0.6373 [0.44] [367, 1, 211, 0] [363, 6, 206, 4]\n",
      "global issues\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 349s 251ms/step - loss: 0.9800 - acc: 0.6049 - val_loss: 0.9544 - val_acc: 0.5994\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 340s 245ms/step - loss: 0.9647 - acc: 0.6323 - val_loss: 0.9523 - val_acc: 0.6023\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 327s 236ms/step - loss: 0.9438 - acc: 0.6489 - val_loss: 0.9242 - val_acc: 0.6571\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 347s 250ms/step - loss: 0.9130 - acc: 0.7138 - val_loss: 0.9373 - val_acc: 0.7378\n",
      "0.4539 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19] 0.2936 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19] 0.7254 [0.53, 0.54, 0.55] [170, 0, 409, 0] [20, 400, 9, 150]\n",
      "technology\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 322s 232ms/step - loss: 0.9305 - acc: 0.5624 - val_loss: 0.9278 - val_acc: 0.6254\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 335s 241ms/step - loss: 0.9081 - acc: 0.5061 - val_loss: 0.9185 - val_acc: 0.5735\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 336s 242ms/step - loss: 0.9036 - acc: 0.5732 - val_loss: 0.9158 - val_acc: 0.3718\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 139s 100ms/step - loss: 0.8834 - acc: 0.6316 - val_loss: 0.9163 - val_acc: 0.6571\n",
      "0.5102 [0.33] 0.3765 [0.33] 0.6753 [0.49, 0.52] [188, 30, 356, 5] [47, 344, 42, 146]\n",
      "design\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 136s 98ms/step - loss: 1.0930 - acc: 0.5407 - val_loss: 1.1235 - val_acc: 0.4611\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 141s 101ms/step - loss: 1.0520 - acc: 0.4261 - val_loss: 1.1508 - val_acc: 0.2536\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 137s 99ms/step - loss: 1.0501 - acc: 0.4996 - val_loss: 1.1128 - val_acc: 0.7666\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 145s 105ms/step - loss: 1.0289 - acc: 0.6121 - val_loss: 1.0667 - val_acc: 0.3919\n",
      "0.3819 [0.43] 0.3627 [0.43] 0.7927 [0.7, 0.71, 0.72, 0.73, 0.74] [114, 96, 362, 7] [1, 458, 0, 120]\n",
      "business\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 135s 97ms/step - loss: 1.1535 - acc: 0.5912 - val_loss: 1.1444 - val_acc: 0.6369\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 201s 145ms/step - loss: 1.1229 - acc: 0.6842 - val_loss: 1.1851 - val_acc: 0.7435\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 232s 167ms/step - loss: 1.0928 - acc: 0.6229 - val_loss: 1.2016 - val_acc: 0.6916\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 233s 168ms/step - loss: 1.0684 - acc: 0.7678 - val_loss: 1.1408 - val_acc: 0.7896\n",
      "0.37 [0.52] 0.7824 [0.52] 0.8687 [0.74, 0.75, 0.76] [37, 416, 83, 43] [10, 493, 6, 70]\n",
      "biomechanics\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 237s 171ms/step - loss: 1.3331 - acc: 0.6431 - val_loss: 1.2723 - val_acc: 0.7896\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 238s 172ms/step - loss: 1.2400 - acc: 0.6280 - val_loss: 1.2394 - val_acc: 0.3055\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 240s 173ms/step - loss: 1.1864 - acc: 0.7448 - val_loss: 1.1717 - val_acc: 0.8069\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 250s 180ms/step - loss: 1.1494 - acc: 0.4895 - val_loss: 1.1940 - val_acc: 0.6628\n",
      "0.3822 [0.69] 0.8325 [0.69] 0.8998 [0.78, 0.8, 0.81, 0.82, 0.85] [30, 452, 68, 29] [12, 509, 11, 47]\n",
      "biodiversity\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 244s 176ms/step - loss: 1.2217 - acc: 0.6121 - val_loss: 1.1855 - val_acc: 0.8934\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 243s 175ms/step - loss: 1.1066 - acc: 0.5191 - val_loss: 1.1358 - val_acc: 0.4006\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 246s 177ms/step - loss: 1.0527 - acc: 0.5523 - val_loss: 1.1362 - val_acc: 0.5014\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 250s 180ms/step - loss: 0.8822 - acc: 0.7924 - val_loss: 1.0061 - val_acc: 0.8818\n",
      "0.5263 [0.56] 0.8912 [0.56] 0.9119 [0.88] [35, 481, 46, 17] [1, 527, 0, 51]\n",
      "media\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 251s 181ms/step - loss: 1.3972 - acc: 0.4441 - val_loss: 1.1077 - val_acc: 0.9049\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 253s 182ms/step - loss: 1.2499 - acc: 0.6647 - val_loss: 1.1738 - val_acc: 0.1066\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 260s 187ms/step - loss: 1.2529 - acc: 0.3050 - val_loss: 1.1432 - val_acc: 0.1124\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 266s 192ms/step - loss: 1.2364 - acc: 0.4939 - val_loss: 1.1171 - val_acc: 0.3573\n",
      "0.2327 [0.49] 0.3282 [0.49] 0.8756 [0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99] [59, 131, 376, 13] [0, 507, 0, 72]\n",
      "entertainment\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 319s 230ms/step - loss: 1.2301 - acc: 0.3872 - val_loss: 1.1866 - val_acc: 0.4409\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 628s 453ms/step - loss: 1.2087 - acc: 0.4434 - val_loss: 1.1911 - val_acc: 0.4294\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 866s 624ms/step - loss: 1.2055 - acc: 0.4427 - val_loss: 1.1802 - val_acc: 0.4409\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 689s 497ms/step - loss: 1.2036 - acc: 0.4463 - val_loss: 1.1796 - val_acc: 0.4467\n",
      "0.2525 [0.37] 0.3558 [0.37] 0.8636 [0.59, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99] [63, 143, 357, 16] [1, 499, 1, 78]\n",
      "history\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 749s 540ms/step - loss: 1.1699 - acc: 0.4441 - val_loss: 1.1324 - val_acc: 0.1758\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 766s 553ms/step - loss: 1.1568 - acc: 0.3929 - val_loss: 1.1272 - val_acc: 0.6916\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 589s 425ms/step - loss: 1.1546 - acc: 0.5097 - val_loss: 1.1273 - val_acc: 0.6167\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 384s 277ms/step - loss: 1.1528 - acc: 0.6424 - val_loss: 1.1339 - val_acc: 0.1758\n",
      "0.2784 [0.44] 0.1762 [0.44] 0.8411 [0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99] [92, 10, 477, 0] [0, 487, 0, 92]\n",
      "future\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 334s 241ms/step - loss: 1.2728 - acc: 0.2502 - val_loss: 1.2152 - val_acc: 0.3631\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 343s 247ms/step - loss: 1.2669 - acc: 0.4708 - val_loss: 1.2153 - val_acc: 0.3372\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 329s 238ms/step - loss: 1.2630 - acc: 0.2603 - val_loss: 1.2201 - val_acc: 0.3199\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 204s 147ms/step - loss: 1.2546 - acc: 0.6611 - val_loss: 1.2376 - val_acc: 0.1988\n",
      "0.1677 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38] 0.0915 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38] 0.9102 [0.63, 0.64] [53, 0, 526, 0] [1, 526, 0, 52]\n",
      "communication\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 196s 141ms/step - loss: 1.2815 - acc: 0.3547 - val_loss: 1.2475 - val_acc: 0.3862\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 204s 147ms/step - loss: 1.2710 - acc: 0.3381 - val_loss: 1.2450 - val_acc: 0.4150\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 214s 154ms/step - loss: 1.2650 - acc: 0.5133 - val_loss: 1.2424 - val_acc: 0.3919\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 300s 217ms/step - loss: 1.2563 - acc: 0.4211 - val_loss: 1.2355 - val_acc: 0.2450\n",
      "0.1455 [0.52] 0.3506 [0.52] 0.924 [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99] [32, 171, 364, 12] [0, 535, 0, 44]\n",
      "humanity\n",
      "Train on 1387 samples, validate on 347 samples\n",
      "Epoch 1/4\n",
      "1387/1387 [==============================] - 357s 257ms/step - loss: 1.2264 - acc: 0.5508 - val_loss: 1.4224 - val_acc: 0.8329\n",
      "Epoch 2/4\n",
      "1387/1387 [==============================] - 360s 260ms/step - loss: 1.2141 - acc: 0.7239 - val_loss: 1.4079 - val_acc: 0.8674\n",
      "Epoch 3/4\n",
      "1387/1387 [==============================] - 347s 250ms/step - loss: 1.1985 - acc: 0.5386 - val_loss: 1.4065 - val_acc: 0.8473\n",
      "Epoch 4/4\n",
      "1387/1387 [==============================] - 356s 257ms/step - loss: 1.1789 - acc: 0.7051 - val_loss: 1.3931 - val_acc: 0.7522\n",
      "0.2095 [0.48] 0.3092 [0.48] 0.8929 [0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99] [53, 126, 391, 9] [1, 516, 1, 61]\n"
     ]
    }
   ],
   "source": [
    "# tag_results = {}\n",
    "# for i in range(len(all_tags)):\n",
    "#     tag = all_tags[i]\n",
    "#     print(tag)\n",
    "#     single_class = get_target_column(tag, df)\n",
    "#     train_y_tag = train_y[tag]\n",
    "#     valid_y_tag = valid_y[tag]\n",
    "#     history = model.fit(X_train, train_y_tag, batch_size=32, epochs=4, verbose=1, validation_split=0.2)\n",
    "#     predictions = model.predict(X_test)\n",
    "#     highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i = get_threshold(tag,valid_y_tag,predictions)\n",
    "#     print(highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i)\n",
    "#     tag_results[tag] = [highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i]\n",
    "    \n",
    "tag_results = {}\n",
    "for i in range(len(all_tags)):\n",
    "    tag = all_tags[i]\n",
    "    print(tag)\n",
    "    train_y_tag = train_y[tag]\n",
    "    valid_y_tag = valid_y[tag]\n",
    "    class_weight = compute_class_weight(tag)\n",
    "    history = model.fit(X_train, train_y_tag, batch_size=32, epochs=4, verbose=1, validation_split=0.2, class_weight=class_weight)\n",
    "    predictions = model.predict(X_test)\n",
    "    model.save('{}_transcript_only.h5'.format(tag))\n",
    "    highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i, f1_metrics, accuracy_metrics = get_threshold(tag,valid_y_tag,predictions)\n",
    "    print(highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i, f1_metrics, accuracy_metrics)\n",
    "    tag_results[tag] = [highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i, f1_metrics, accuracy_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6826 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35] 0.5181 [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35] 0.5354 [0.43] [300, 0, 279, 0] [275, 35, 244, 25]\n"
     ]
    }
   ],
   "source": [
    "highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i, f1_metrics, accuracy_metrics = get_threshold(tag,valid_y_tag,predictions)\n",
    "print(highest_f1,f1_i,highest_accuracy_f1,accuracy_f1_i,highest_accuracy,accuracy_i, f1_metrics, accuracy_metrics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'culture': [0.6826, [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17], 0.5181, [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17], 0.5389, [0.52, 0.54], [300, 0, 279, 0], [98, 214, 65, 202]], 'politics': [0.2609, [0.4], 0.8826, [0.4], 0.9188, [0.51, 0.54, 0.55, 0.7], [12, 499, 27, 41], [8, 524, 2, 45]], 'science': [0.7767, [0.4], 0.6356, [0.4], 0.6373, [0.44], [367, 1, 211, 0], [363, 6, 206, 4]], 'global issues': [0.4539, [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19], 0.2936, [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19], 0.7254, [0.53, 0.54, 0.55], [170, 0, 409, 0], [20, 400, 9, 150]], 'technology': [0.5102, [0.33], 0.3765, [0.33], 0.6753, [0.49, 0.52], [188, 30, 356, 5], [47, 344, 42, 146]], 'design': [0.3819, [0.43], 0.3627, [0.43], 0.7927, [0.7, 0.71, 0.72, 0.73, 0.74], [114, 96, 362, 7], [1, 458, 0, 120]], 'business': [0.37, [0.52], 0.7824, [0.52], 0.8687, [0.74, 0.75, 0.76], [37, 416, 83, 43], [10, 493, 6, 70]], 'biomechanics': [0.3822, [0.69], 0.8325, [0.69], 0.8998, [0.78, 0.8, 0.81, 0.82, 0.85], [30, 452, 68, 29], [12, 509, 11, 47]], 'biodiversity': [0.5263, [0.56], 0.8912, [0.56], 0.9119, [0.88], [35, 481, 46, 17], [1, 527, 0, 51]], 'media': [0.2327, [0.49], 0.3282, [0.49], 0.8756, [0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99], [59, 131, 376, 13], [0, 507, 0, 72]], 'entertainment': [0.2525, [0.37], 0.3558, [0.37], 0.8636, [0.59, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99], [63, 143, 357, 16], [1, 499, 1, 78]], 'history': [0.2784, [0.44], 0.1762, [0.44], 0.8411, [0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99], [92, 10, 477, 0], [0, 487, 0, 92]], 'future': [0.1677, [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38], 0.0915, [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38], 0.9102, [0.63, 0.64], [53, 0, 526, 0], [1, 526, 0, 52]], 'communication': [0.1455, [0.52], 0.3506, [0.52], 0.924, [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99], [32, 171, 364, 12], [0, 535, 0, 44]], 'humanity': [0.2095, [0.48], 0.3092, [0.48], 0.8929, [0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99], [53, 126, 391, 9], [1, 516, 1, 61]]}\n"
     ]
    }
   ],
   "source": [
    "print(tag_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highest_f1</th>\n",
       "      <th>thresholds_for_highest_f1</th>\n",
       "      <th>highest_accuracy_at_highest_f1</th>\n",
       "      <th>thresholds_for_highest_accuracy_f1</th>\n",
       "      <th>highest_accuracy</th>\n",
       "      <th>threshold_for_highest_accuracy_i</th>\n",
       "      <th>highest_f1_confusion_metrics</th>\n",
       "      <th>highest_accuracy_confusion_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>0.6826</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>0.5389</td>\n",
       "      <td>[0.52, 0.54]</td>\n",
       "      <td>[300, 0, 279, 0]</td>\n",
       "      <td>[98, 214, 65, 202]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>0.2609</td>\n",
       "      <td>[0.4]</td>\n",
       "      <td>0.8826</td>\n",
       "      <td>[0.4]</td>\n",
       "      <td>0.9188</td>\n",
       "      <td>[0.51, 0.54, 0.55, 0.7]</td>\n",
       "      <td>[12, 499, 27, 41]</td>\n",
       "      <td>[8, 524, 2, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.7767</td>\n",
       "      <td>[0.4]</td>\n",
       "      <td>0.6356</td>\n",
       "      <td>[0.4]</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>[0.44]</td>\n",
       "      <td>[367, 1, 211, 0]</td>\n",
       "      <td>[363, 6, 206, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global issues</th>\n",
       "      <td>0.4539</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>0.2936</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>[0.53, 0.54, 0.55]</td>\n",
       "      <td>[170, 0, 409, 0]</td>\n",
       "      <td>[20, 400, 9, 150]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>0.5102</td>\n",
       "      <td>[0.33]</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>[0.33]</td>\n",
       "      <td>0.6753</td>\n",
       "      <td>[0.49, 0.52]</td>\n",
       "      <td>[188, 30, 356, 5]</td>\n",
       "      <td>[47, 344, 42, 146]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>design</th>\n",
       "      <td>0.3819</td>\n",
       "      <td>[0.43]</td>\n",
       "      <td>0.3627</td>\n",
       "      <td>[0.43]</td>\n",
       "      <td>0.7927</td>\n",
       "      <td>[0.7, 0.71, 0.72, 0.73, 0.74]</td>\n",
       "      <td>[114, 96, 362, 7]</td>\n",
       "      <td>[1, 458, 0, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>0.3700</td>\n",
       "      <td>[0.52]</td>\n",
       "      <td>0.7824</td>\n",
       "      <td>[0.52]</td>\n",
       "      <td>0.8687</td>\n",
       "      <td>[0.74, 0.75, 0.76]</td>\n",
       "      <td>[37, 416, 83, 43]</td>\n",
       "      <td>[10, 493, 6, 70]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biomechanics</th>\n",
       "      <td>0.3822</td>\n",
       "      <td>[0.69]</td>\n",
       "      <td>0.8325</td>\n",
       "      <td>[0.69]</td>\n",
       "      <td>0.8998</td>\n",
       "      <td>[0.78, 0.8, 0.81, 0.82, 0.85]</td>\n",
       "      <td>[30, 452, 68, 29]</td>\n",
       "      <td>[12, 509, 11, 47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biodiversity</th>\n",
       "      <td>0.5263</td>\n",
       "      <td>[0.56]</td>\n",
       "      <td>0.8912</td>\n",
       "      <td>[0.56]</td>\n",
       "      <td>0.9119</td>\n",
       "      <td>[0.88]</td>\n",
       "      <td>[35, 481, 46, 17]</td>\n",
       "      <td>[1, 527, 0, 51]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media</th>\n",
       "      <td>0.2327</td>\n",
       "      <td>[0.49]</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>[0.49]</td>\n",
       "      <td>0.8756</td>\n",
       "      <td>[0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73...</td>\n",
       "      <td>[59, 131, 376, 13]</td>\n",
       "      <td>[0, 507, 0, 72]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>0.2525</td>\n",
       "      <td>[0.37]</td>\n",
       "      <td>0.3558</td>\n",
       "      <td>[0.37]</td>\n",
       "      <td>0.8636</td>\n",
       "      <td>[0.59, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6...</td>\n",
       "      <td>[63, 143, 357, 16]</td>\n",
       "      <td>[1, 499, 1, 78]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>history</th>\n",
       "      <td>0.2784</td>\n",
       "      <td>[0.44]</td>\n",
       "      <td>0.1762</td>\n",
       "      <td>[0.44]</td>\n",
       "      <td>0.8411</td>\n",
       "      <td>[0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75...</td>\n",
       "      <td>[92, 10, 477, 0]</td>\n",
       "      <td>[0, 487, 0, 92]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>future</th>\n",
       "      <td>0.1677</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...</td>\n",
       "      <td>0.9102</td>\n",
       "      <td>[0.63, 0.64]</td>\n",
       "      <td>[53, 0, 526, 0]</td>\n",
       "      <td>[1, 526, 0, 52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communication</th>\n",
       "      <td>0.1455</td>\n",
       "      <td>[0.52]</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>[0.52]</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>[0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.6...</td>\n",
       "      <td>[32, 171, 364, 12]</td>\n",
       "      <td>[0, 535, 0, 44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humanity</th>\n",
       "      <td>0.2095</td>\n",
       "      <td>[0.48]</td>\n",
       "      <td>0.3092</td>\n",
       "      <td>[0.48]</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>[0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8...</td>\n",
       "      <td>[53, 126, 391, 9]</td>\n",
       "      <td>[1, 516, 1, 61]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               highest_f1                          thresholds_for_highest_f1  \\\n",
       "culture            0.6826  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "politics           0.2609                                              [0.4]   \n",
       "science            0.7767                                              [0.4]   \n",
       "global issues      0.4539  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "technology         0.5102                                             [0.33]   \n",
       "design             0.3819                                             [0.43]   \n",
       "business           0.3700                                             [0.52]   \n",
       "biomechanics       0.3822                                             [0.69]   \n",
       "biodiversity       0.5263                                             [0.56]   \n",
       "media              0.2327                                             [0.49]   \n",
       "entertainment      0.2525                                             [0.37]   \n",
       "history            0.2784                                             [0.44]   \n",
       "future             0.1677  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "communication      0.1455                                             [0.52]   \n",
       "humanity           0.2095                                             [0.48]   \n",
       "\n",
       "               highest_accuracy_at_highest_f1  \\\n",
       "culture                                0.5181   \n",
       "politics                               0.8826   \n",
       "science                                0.6356   \n",
       "global issues                          0.2936   \n",
       "technology                             0.3765   \n",
       "design                                 0.3627   \n",
       "business                               0.7824   \n",
       "biomechanics                           0.8325   \n",
       "biodiversity                           0.8912   \n",
       "media                                  0.3282   \n",
       "entertainment                          0.3558   \n",
       "history                                0.1762   \n",
       "future                                 0.0915   \n",
       "communication                          0.3506   \n",
       "humanity                               0.3092   \n",
       "\n",
       "                              thresholds_for_highest_accuracy_f1  \\\n",
       "culture        [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "politics                                                   [0.4]   \n",
       "science                                                    [0.4]   \n",
       "global issues  [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "technology                                                [0.33]   \n",
       "design                                                    [0.43]   \n",
       "business                                                  [0.52]   \n",
       "biomechanics                                              [0.69]   \n",
       "biodiversity                                              [0.56]   \n",
       "media                                                     [0.49]   \n",
       "entertainment                                             [0.37]   \n",
       "history                                                   [0.44]   \n",
       "future         [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07...   \n",
       "communication                                             [0.52]   \n",
       "humanity                                                  [0.48]   \n",
       "\n",
       "               highest_accuracy  \\\n",
       "culture                  0.5389   \n",
       "politics                 0.9188   \n",
       "science                  0.6373   \n",
       "global issues            0.7254   \n",
       "technology               0.6753   \n",
       "design                   0.7927   \n",
       "business                 0.8687   \n",
       "biomechanics             0.8998   \n",
       "biodiversity             0.9119   \n",
       "media                    0.8756   \n",
       "entertainment            0.8636   \n",
       "history                  0.8411   \n",
       "future                   0.9102   \n",
       "communication            0.9240   \n",
       "humanity                 0.8929   \n",
       "\n",
       "                                threshold_for_highest_accuracy_i  \\\n",
       "culture                                             [0.52, 0.54]   \n",
       "politics                                 [0.51, 0.54, 0.55, 0.7]   \n",
       "science                                                   [0.44]   \n",
       "global issues                                 [0.53, 0.54, 0.55]   \n",
       "technology                                          [0.49, 0.52]   \n",
       "design                             [0.7, 0.71, 0.72, 0.73, 0.74]   \n",
       "business                                      [0.74, 0.75, 0.76]   \n",
       "biomechanics                       [0.78, 0.8, 0.81, 0.82, 0.85]   \n",
       "biodiversity                                              [0.88]   \n",
       "media          [0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73...   \n",
       "entertainment  [0.59, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6...   \n",
       "history        [0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75...   \n",
       "future                                              [0.63, 0.64]   \n",
       "communication  [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.6...   \n",
       "humanity       [0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8...   \n",
       "\n",
       "              highest_f1_confusion_metrics highest_accuracy_confusion_metrics  \n",
       "culture                   [300, 0, 279, 0]                 [98, 214, 65, 202]  \n",
       "politics                 [12, 499, 27, 41]                    [8, 524, 2, 45]  \n",
       "science                   [367, 1, 211, 0]                   [363, 6, 206, 4]  \n",
       "global issues             [170, 0, 409, 0]                  [20, 400, 9, 150]  \n",
       "technology               [188, 30, 356, 5]                 [47, 344, 42, 146]  \n",
       "design                   [114, 96, 362, 7]                   [1, 458, 0, 120]  \n",
       "business                 [37, 416, 83, 43]                   [10, 493, 6, 70]  \n",
       "biomechanics             [30, 452, 68, 29]                  [12, 509, 11, 47]  \n",
       "biodiversity             [35, 481, 46, 17]                    [1, 527, 0, 51]  \n",
       "media                   [59, 131, 376, 13]                    [0, 507, 0, 72]  \n",
       "entertainment           [63, 143, 357, 16]                    [1, 499, 1, 78]  \n",
       "history                   [92, 10, 477, 0]                    [0, 487, 0, 92]  \n",
       "future                     [53, 0, 526, 0]                    [1, 526, 0, 52]  \n",
       "communication           [32, 171, 364, 12]                    [0, 535, 0, 44]  \n",
       "humanity                 [53, 126, 391, 9]                    [1, 516, 1, 61]  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(tag_results, orient='index', columns=['highest_f1', 'thresholds_for_highest_f1', 'highest_accuracy_at_highest_f1', 'thresholds_for_highest_accuracy_f1','highest_accuracy','threshold_for_highest_accuracy_i', 'highest_f1_confusion_metrics', 'highest_accuracy_confusion_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
